{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Common Settings\n",
    "Define configuration variables including model settings, number of results, temperature, and other shared parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Common Settings\n",
    "\n",
    "# Define configuration variables\n",
    "model = \"gpt-3.5-turbo\"  # Model to be used for both Knowledge Base and Agent\n",
    "num_results = 5  # Number of results to retrieve\n",
    "temperature = 0.7  # Temperature setting for the model\n",
    "max_tokens = 150  # Maximum number of tokens for the response\n",
    "\n",
    "# Print the configuration to verify\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"Number of Results: {num_results}\")\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Max Tokens: {max_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Knowledge Base\n",
    "Initialize and configure the knowledge base with document loading, chunking, and embedding setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.document_loaders import LocalDocumentLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize document loader\n",
    "document_loader = LocalDocumentLoader(directory_path=\"path/to/documents\")\n",
    "\n",
    "# Load documents\n",
    "documents = document_loader.load()\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=model)\n",
    "\n",
    "# Create vector store\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Print the number of chunks created\n",
    "print(f\"Number of chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Agent with Tools\n",
    "Create and configure an agent with necessary tools and action capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for agent setup\n",
    "from langchain.agents import Agent\n",
    "from langchain.tools import Tool\n",
    "\n",
    "# Define tools for the agent\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Document Search\",\n",
    "        description=\"Searches documents for relevant information.\",\n",
    "        action=lambda query: vector_store.similarity_search(query, k=num_results)\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Summarization\",\n",
    "        description=\"Summarizes the given text.\",\n",
    "        action=lambda text: embeddings.summarize(text, max_tokens=max_tokens, temperature=temperature)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize the agent with the defined tools\n",
    "agent = Agent(\n",
    "    tools=tools,\n",
    "    model=model,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens\n",
    ")\n",
    "\n",
    "# Print agent configuration to verify\n",
    "print(\"Agent configured with the following tools:\")\n",
    "for tool in tools:\n",
    "    print(f\"- {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Knowledge Base\n",
    "Perform multiple queries against the knowledge base and collect responses and citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Knowledge Base\n",
    "\n",
    "# Define a list of queries to perform against the knowledge base\n",
    "queries = [\n",
    "    \"What is the impact of climate change on polar bears?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"What are the benefits of a healthy diet?\",\n",
    "    \"How does quantum computing work?\",\n",
    "    \"What are the causes of the French Revolution?\"\n",
    "]\n",
    "\n",
    "# Perform queries and collect responses and citations\n",
    "responses = []\n",
    "citations = []\n",
    "\n",
    "for query in queries:\n",
    "    # Perform similarity search using the vector store\n",
    "    search_results = vector_store.similarity_search(query, k=num_results)\n",
    "    \n",
    "    # Collect responses and citations\n",
    "    response_texts = [result['text'] for result in search_results]\n",
    "    response_citations = [result['source'] for result in search_results]\n",
    "    \n",
    "    responses.append(response_texts)\n",
    "    citations.append(response_citations)\n",
    "\n",
    "# Print the collected responses and citations\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"Responses:\")\n",
    "    for response in responses[i]:\n",
    "        print(f\"- {response}\")\n",
    "    print(\"Citations:\")\n",
    "    for citation in citations[i]:\n",
    "        print(f\"- {citation}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Agent\n",
    "Run the same queries through the agent and collect its responses and supporting evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Agent\n",
    "\n",
    "# Perform queries through the agent and collect responses and supporting evidence\n",
    "agent_responses = []\n",
    "agent_evidence = []\n",
    "\n",
    "for query in queries:\n",
    "    # Use the agent to perform the query\n",
    "    agent_result = agent.run(query)\n",
    "    \n",
    "    # Collect the response and supporting evidence\n",
    "    agent_responses.append(agent_result['response'])\n",
    "    agent_evidence.append(agent_result['evidence'])\n",
    "\n",
    "# Print the collected agent responses and supporting evidence\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"Agent Responses:\")\n",
    "    print(f\"- {agent_responses[i]}\")\n",
    "    print(\"Supporting Evidence:\")\n",
    "    for evidence in agent_evidence[i]:\n",
    "        print(f\"- {evidence}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results and Citations\n",
    "Create comparison metrics and visualizations for both approaches' responses and citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Results and Citations\n",
    "\n",
    "# Import necessary libraries for comparison and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Query': queries,\n",
    "    'Knowledge Base Responses': responses,\n",
    "    'Knowledge Base Citations': citations,\n",
    "    'Agent Responses': agent_responses,\n",
    "    'Agent Evidence': agent_evidence\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "comparison_df.head()\n",
    "\n",
    "# Define a function to calculate precision, recall, and F1 score\n",
    "def calculate_metrics(true_citations, predicted_citations):\n",
    "    precision = precision_score(true_citations, predicted_citations, average='micro')\n",
    "    recall = recall_score(true_citations, predicted_citations, average='micro')\n",
    "    f1 = f1_score(true_citations, predicted_citations, average='micro')\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Calculate metrics for each query\n",
    "metrics = []\n",
    "for i in range(len(queries)):\n",
    "    true_citations = citations[i]\n",
    "    predicted_citations = agent_evidence[i]\n",
    "    precision, recall, f1 = calculate_metrics(true_citations, predicted_citations)\n",
    "    metrics.append((precision, recall, f1))\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Precision', 'Recall', 'F1 Score'], index=queries)\n",
    "\n",
    "# Display the metrics DataFrame\n",
    "metrics_df.head()\n",
    "\n",
    "# Plot the metrics\n",
    "metrics_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Comparison of Knowledge Base and Agent Approaches')\n",
    "plt.xlabel('Queries')\n",
    "plt.ylabel('Scores')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Performance Metrics\n",
    "Calculate and compare performance metrics including response time, accuracy, and relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Performance Metrics\n",
    "\n",
    "# Import necessary libraries for performance analysis\n",
    "import time\n",
    "\n",
    "# Define a function to measure response time\n",
    "def measure_response_time(query, method):\n",
    "    start_time = time.time()\n",
    "    if method == \"knowledge_base\":\n",
    "        vector_store.similarity_search(query, k=num_results)\n",
    "    elif method == \"agent\":\n",
    "        agent.run(query)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Measure response time for each query using both methods\n",
    "kb_response_times = []\n",
    "agent_response_times = []\n",
    "\n",
    "for query in queries:\n",
    "    kb_response_time = measure_response_time(query, \"knowledge_base\")\n",
    "    agent_response_time = measure_response_time(query, \"agent\")\n",
    "    kb_response_times.append(kb_response_time)\n",
    "    agent_response_times.append(agent_response_time)\n",
    "\n",
    "# Calculate average response time\n",
    "avg_kb_response_time = sum(kb_response_times) / len(kb_response_times)\n",
    "avg_agent_response_time = sum(agent_response_times) / len(agent_response_times)\n",
    "\n",
    "# Print average response times\n",
    "print(f\"Average Knowledge Base Response Time: {avg_kb_response_time:.2f} seconds\")\n",
    "print(f\"Average Agent Response Time: {avg_agent_response_time:.2f} seconds\")\n",
    "\n",
    "# Calculate relevance scores (dummy implementation for illustration)\n",
    "def calculate_relevance_score(responses, ground_truth):\n",
    "    # Placeholder for actual relevance score calculation\n",
    "    return [1.0] * len(responses)  # Assume perfect relevance for illustration\n",
    "\n",
    "# Calculate relevance scores for each query\n",
    "kb_relevance_scores = []\n",
    "agent_relevance_scores = []\n",
    "\n",
    "for i in range(len(queries)):\n",
    "    kb_relevance_score = calculate_relevance_score(responses[i], citations[i])\n",
    "    agent_relevance_score = calculate_relevance_score(agent_responses[i], agent_evidence[i])\n",
    "    kb_relevance_scores.append(kb_relevance_score)\n",
    "    agent_relevance_scores.append(agent_relevance_score)\n",
    "\n",
    "# Create a DataFrame to store the performance metrics\n",
    "performance_df = pd.DataFrame({\n",
    "    'Query': queries,\n",
    "    'KB Response Time (s)': kb_response_times,\n",
    "    'Agent Response Time (s)': agent_response_times,\n",
    "    'KB Relevance Score': kb_relevance_scores,\n",
    "    'Agent Relevance Score': agent_relevance_scores\n",
    "})\n",
    "\n",
    "# Display the performance DataFrame\n",
    "performance_df.head()\n",
    "\n",
    "# Plot the response times\n",
    "performance_df[['KB Response Time (s)', 'Agent Response Time (s)']].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Response Time Comparison')\n",
    "plt.xlabel('Queries')\n",
    "plt.ylabel('Response Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot the relevance scores\n",
    "performance_df[['KB Relevance Score', 'Agent Relevance Score']].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Relevance Score Comparison')\n",
    "plt.xlabel('Queries')\n",
    "plt.ylabel('Relevance Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
