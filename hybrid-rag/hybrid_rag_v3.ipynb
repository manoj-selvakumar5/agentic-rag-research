{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Collaboration - DSL query use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import subprocess\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configure Logging\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Bedrock Agent utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:11:33,261 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 version: 1.36.9\n"
     ]
    }
   ],
   "source": [
    "from src.utils.bedrock_agent import Agent, SupervisorAgent, agents_helper, region, account_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AWS clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:11:34,698 - __main__ - INFO - Region: us-west-2\n",
      "2025-02-09 16:11:34,699 - __main__ - INFO - Account ID: 533267284022\n",
      "2025-02-09 16:11:34,699 - __main__ - INFO - Agent Suffix: us-west-2-533\n"
     ]
    }
   ],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = session.region_name\n",
    "account_id_suffix = account_id[:3]\n",
    "agent_suffix = f\"{region}-{account_id_suffix}\"\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n",
    "iam_client = boto3.client('iam', region_name=region)\n",
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "\n",
    "logger.info(f\"Region: {region}\")\n",
    "logger.info(f\"Account ID: {account_id}\")\n",
    "logger.info(f\"Agent Suffix: {agent_suffix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iam_role(role_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates or retrieves an IAM Role with the necessary trust policy for Lambda.\n",
    "    Attaches AWSLambdaBasicExecutionRole, and adds inline policies for OpenSearch \n",
    "    and AOSS access.\n",
    "\n",
    "    :param role_name: Name of the IAM Role to create or retrieve.\n",
    "    :return: ARN of the created or retrieved IAM Role.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating or retrieving IAM Role: {role_name}\")\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        role = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    "        )\n",
    "        logger.info(f\"IAM Role {role_name} created.\")\n",
    "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "        logger.info(f\"IAM Role {role_name} already exists. Retrieving existing role.\")\n",
    "        role = iam_client.get_role(RoleName=role_name)\n",
    "\n",
    "    # Attach AWS Lambda execution policy\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "    )\n",
    "    logger.info(f\"Attached AWSLambdaBasicExecutionRole to {role_name}.\")\n",
    "\n",
    "    # Attach additional policies for OpenSearch access\n",
    "    opensearch_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"es:Describe*\",\n",
    "                    \"es:List*\",\n",
    "                    \"es:Get*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    opensearch_policy_name = f\"{role_name}-OpenSearchPolicy\"\n",
    "    try:\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=opensearch_policy_name,\n",
    "            PolicyDocument=json.dumps(opensearch_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Attached OpenSearch policy to IAM Role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to attach OpenSearch policy to IAM Role {role_name}: {str(e)}\")\n",
    "\n",
    "    # Attach the new policy for aoss:APICall\n",
    "    aoss_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"aoss:*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    aoss_policy_name = f\"{role_name}-AOSSPolicy\"\n",
    "    try:\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=aoss_policy_name,\n",
    "            PolicyDocument=json.dumps(aoss_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Attached AOSS policy to IAM Role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to attach AOSS policy to IAM Role {role_name}: {str(e)}\")\n",
    "\n",
    "    role_arn = role['Role']['Arn']\n",
    "\n",
    "    # Wait for IAM role to propagate\n",
    "    logger.info(\"Waiting 10 seconds for IAM role to propagate...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    return role_arn\n",
    "\n",
    "\n",
    "def create_lambda_package(source_file: str, zip_file_path: str, dependencies: list = None):\n",
    "    \"\"\"\n",
    "    Packages a Lambda function and its dependencies into a single ZIP file.\n",
    "\n",
    "    Args:\n",
    "        source_file (str): Path to the Lambda function source code\n",
    "        zip_file_path (str): Path to the ZIP file that will be created\n",
    "        dependencies (list, optional): List of Python packages required by the Lambda\n",
    "    \"\"\"\n",
    "    logger.info(f\"Packaging Lambda function from {source_file}\")\n",
    "    package_dir = \"package\"\n",
    "\n",
    "    # Create package directory if it doesn't exist\n",
    "    if not os.path.exists(package_dir):\n",
    "        os.makedirs(package_dir)\n",
    "\n",
    "    # Install dependencies only if there are any\n",
    "    if dependencies:\n",
    "        logger.info(f\"Installing dependencies: {dependencies}\")\n",
    "        subprocess.run(\n",
    "            f\"pip install {' '.join(dependencies)} -t {package_dir}\",\n",
    "            shell=True,\n",
    "            check=True\n",
    "        )\n",
    "    \n",
    "    # Create ZIP file with function (and dependencies if any)\n",
    "    logger.info(f\"Creating Lambda deployment package: {zip_file_path}\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
    "        # Add dependencies if they exist\n",
    "        if os.path.exists(package_dir) and os.listdir(package_dir):\n",
    "            for root, _, files in os.walk(package_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, package_dir)\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add the Lambda function code\n",
    "        if not os.path.exists(source_file):\n",
    "            raise FileNotFoundError(f\"Lambda source file not found: {source_file}\")\n",
    "        zipf.write(source_file, os.path.basename(source_file))\n",
    "\n",
    "    # Cleanup temporary package directory\n",
    "    if os.path.exists(package_dir):\n",
    "        logger.info(\"Cleaning up temporary package directory...\")\n",
    "        subprocess.run(f\"rm -rf {package_dir}\", shell=True)\n",
    "    \n",
    "    logger.info(\"Lambda package created successfully.\")\n",
    "\n",
    "\n",
    "def create_lambda_function(function_name: str,\n",
    "                         role_arn: str,\n",
    "                         handler: str,\n",
    "                         runtime: str,\n",
    "                         zip_file_path: str,\n",
    "                         region_name: str = region,\n",
    "                         layers: list = None,\n",
    "                         memory_size: int = 128,\n",
    "                         timeout: int = 15) -> dict:\n",
    "    \"\"\"\n",
    "    Creates or updates an AWS Lambda function with retry logic for updates.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating/updating Lambda function: {function_name}\")\n",
    "    lambda_client = boto3.client('lambda', region_name=region_name)\n",
    "\n",
    "    with open(zip_file_path, 'rb') as f:\n",
    "        zip_content = f.read()\n",
    "\n",
    "    # Prepare create function parameters\n",
    "    create_params = {\n",
    "        'FunctionName': function_name,\n",
    "        'Runtime': runtime,\n",
    "        'Role': role_arn,\n",
    "        'Handler': handler,\n",
    "        'Code': {'ZipFile': zip_content},\n",
    "        'Description': 'Lambda function to execute DSL queries',\n",
    "        'Timeout': timeout,\n",
    "        'MemorySize': memory_size,\n",
    "        'Publish': True\n",
    "    }\n",
    "\n",
    "    # Add layers if specified\n",
    "    if layers:\n",
    "        create_params['Layers'] = layers\n",
    "\n",
    "    try:\n",
    "        # Try to create new function\n",
    "        response = lambda_client.create_function(**create_params)\n",
    "        logger.info(f\"Lambda function {function_name} created successfully.\")\n",
    "    except lambda_client.exceptions.ResourceConflictException:\n",
    "        # Function exists, update it\n",
    "        logger.info(f\"Lambda function {function_name} already exists. Updating...\")\n",
    "        \n",
    "        def wait_for_function_update_completion():\n",
    "            max_attempts = 10\n",
    "            delay_seconds = 5\n",
    "            \n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    # Get current function state\n",
    "                    function_info = lambda_client.get_function(FunctionName=function_name)\n",
    "                    state = function_info['Configuration']['State']\n",
    "                    last_update = function_info['Configuration'].get('LastUpdateStatus', 'Successful')\n",
    "                    \n",
    "                    if state == 'Active' and last_update == 'Successful':\n",
    "                        return True\n",
    "                    \n",
    "                    logger.info(f\"Function state: {state}, LastUpdateStatus: {last_update}. Waiting...\")\n",
    "                    time.sleep(delay_seconds)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error checking function status: {str(e)}\")\n",
    "                    time.sleep(delay_seconds)\n",
    "            \n",
    "            return False\n",
    "\n",
    "        # Wait for any ongoing updates to complete\n",
    "        if not wait_for_function_update_completion():\n",
    "            raise Exception(\"Timeout waiting for function update to complete\")\n",
    "\n",
    "        # Update function code\n",
    "        response = lambda_client.update_function_code(\n",
    "            FunctionName=function_name,\n",
    "            ZipFile=zip_content\n",
    "        )\n",
    "        \n",
    "        # Wait for code update to complete\n",
    "        if not wait_for_function_update_completion():\n",
    "            raise Exception(\"Timeout waiting for function code update to complete\")\n",
    "        \n",
    "        # Update configuration if layers are specified\n",
    "        if layers:\n",
    "            config_update = {\n",
    "                'FunctionName': function_name,\n",
    "                'Timeout': timeout,\n",
    "                'MemorySize': memory_size,\n",
    "                'Layers': layers\n",
    "            }\n",
    "            lambda_client.update_function_configuration(**config_update)\n",
    "            \n",
    "            # Wait for configuration update to complete\n",
    "            if not wait_for_function_update_completion():\n",
    "                raise Exception(\"Timeout waiting for function configuration update to complete\")\n",
    "            \n",
    "        logger.info(f\"Lambda function {function_name} updated successfully.\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def add_resource_based_policy(function_name: str,\n",
    "                              agent_ids: list,\n",
    "                              region_name: str,\n",
    "                              account_id: str):\n",
    "    \"\"\"\n",
    "    Adds a resource-based policy to the specified Lambda function to allow invocation\n",
    "    from one or more Bedrock agents.\n",
    "\n",
    "    :param function_name: Name of the Lambda function.\n",
    "    :param agent_ids: List of agent IDs permitted to invoke this Lambda.\n",
    "    :param region_name: AWS region.\n",
    "    :param account_id: AWS account ID.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Adding resource-based policy to Lambda function {function_name} for agents: {agent_ids}\")\n",
    "    statement_id_prefix = \"AllowExecutionFromBedrockAgent\"\n",
    "    policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": []\n",
    "    }\n",
    "\n",
    "    for agent_id in agent_ids:\n",
    "        sid = f\"{statement_id_prefix}_{agent_id}\"\n",
    "        policy_doc['Statement'].append({\n",
    "            \"Sid\": sid,\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"lambda:InvokeFunction\",\n",
    "            \"Resource\": f\"arn:aws:lambda:{region_name}:{account_id}:function:{function_name}\",\n",
    "            \"Condition\": {\n",
    "                \"ArnLike\": {\n",
    "                    \"AWS:SourceArn\": f\"arn:aws:bedrock:{region_name}:{account_id}:agent/{agent_id}\"\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Retrieve existing policy and remove any existing statements with the same prefix\n",
    "    try:\n",
    "        existing_policy = lambda_client.get_policy(FunctionName=function_name)\n",
    "        existing_policy_doc = json.loads(existing_policy['Policy'])\n",
    "        for stmt in existing_policy_doc['Statement']:\n",
    "            if stmt['Sid'].startswith(statement_id_prefix):\n",
    "                sid_to_remove = stmt['Sid']\n",
    "                logger.info(f\"Removing existing statement: {sid_to_remove}\")\n",
    "                lambda_client.remove_permission(\n",
    "                    FunctionName=function_name,\n",
    "                    StatementId=sid_to_remove\n",
    "                )\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"No existing policy found for Lambda function {function_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving/removing existing policy for {function_name}: {str(e)}\")\n",
    "\n",
    "    # Add new permissions\n",
    "    for stmt in policy_doc['Statement']:\n",
    "        sid_val = stmt['Sid']\n",
    "        try:\n",
    "            lambda_client.add_permission(\n",
    "                FunctionName=function_name,\n",
    "                StatementId=sid_val,\n",
    "                Action=stmt['Action'],\n",
    "                Principal=stmt['Principal']['Service'],\n",
    "                SourceArn=stmt['Condition']['ArnLike']['AWS:SourceArn']\n",
    "            )\n",
    "            logger.info(f\"Added permission for statement: {sid_val}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to add resource-based policy for {function_name}, statement {sid_val}: {str(e)}\")\n",
    "\n",
    "\n",
    "def create_lambda_layer(layer_name: str, requirements_file: str):\n",
    "    \"\"\"\n",
    "    Create a Lambda Layer containing the specified dependencies with improved error handling.\n",
    "    \n",
    "    Args:\n",
    "        layer_name (str): Name of the Lambda Layer\n",
    "        requirements_file (str): Path to requirements.txt file\n",
    "    \"\"\"\n",
    "    # Create a temporary directory for layer contents\n",
    "    layer_dir = \"layer\"\n",
    "    if not os.path.exists(layer_dir):\n",
    "        os.makedirs(f\"{layer_dir}/python\")\n",
    "    \n",
    "    try:\n",
    "        # Install dependencies to the layer directory\n",
    "        subprocess.run(\n",
    "            f\"pip install -r {requirements_file} -t {layer_dir}/python\",\n",
    "            shell=True,\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        # Create ZIP file\n",
    "        layer_zip = f\"{layer_name}.zip\"\n",
    "        with zipfile.ZipFile(layer_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(layer_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, layer_dir)\n",
    "                    zipf.write(file_path, arcname)\n",
    "        \n",
    "        # Create Lambda Layer with retries\n",
    "        max_retries = 3\n",
    "        retry_delay = 5  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                with open(layer_zip, 'rb') as zip_file:\n",
    "                    response = lambda_client.publish_layer_version(\n",
    "                        LayerName=layer_name,\n",
    "                        Description='Dependencies for reasoning query function',\n",
    "                        Content={'ZipFile': zip_file.read()},\n",
    "                        CompatibleRuntimes=['python3.12']\n",
    "                    )\n",
    "                logger.info(f\"Successfully created Lambda layer {layer_name} on attempt {attempt + 1}\")\n",
    "                break\n",
    "            except lambda_client.exceptions.ClientError as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    logger.error(f\"Failed to create Lambda layer after {max_retries} attempts: {str(e)}\")\n",
    "                    raise\n",
    "                logger.warning(f\"Attempt {attempt + 1} failed, retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error creating Lambda layer: {str(e)}\")\n",
    "                raise\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        if os.path.exists(layer_dir):\n",
    "            subprocess.run(f\"rm -rf {layer_dir}\", shell=True)\n",
    "        if os.path.exists(layer_zip):\n",
    "            os.remove(layer_zip)\n",
    "    \n",
    "    return response['LayerVersionArn']\n",
    "\n",
    "\n",
    "def create_lambda_layer_via_s3(layer_name: str, requirements: list, bucket_name: str):\n",
    "    \"\"\"\n",
    "    Create a Lambda Layer using S3 for large dependencies.\n",
    "    \n",
    "    Args:\n",
    "        layer_name (str): Name of the Lambda Layer\n",
    "        requirements (list): List of requirements to install\n",
    "        bucket_name (str): S3 bucket to use for layer upload\n",
    "    \"\"\"\n",
    "    layer_dir = \"layer\"\n",
    "    if not os.path.exists(layer_dir):\n",
    "        os.makedirs(f\"{layer_dir}/python\")\n",
    "    \n",
    "    try:\n",
    "        # Split requirements into smaller groups\n",
    "        max_group_size = 3\n",
    "        requirement_groups = [\n",
    "            requirements[i:i + max_group_size] \n",
    "            for i in range(0, len(requirements), max_group_size)\n",
    "        ]\n",
    "        \n",
    "        layer_arns = []\n",
    "        for idx, group in enumerate(requirement_groups):\n",
    "            group_layer_name = f\"{layer_name}-part-{idx+1}\"\n",
    "            layer_zip = f\"{group_layer_name}.zip\"\n",
    "            \n",
    "            # Install dependencies\n",
    "            for req in group:\n",
    "                subprocess.run(\n",
    "                    f\"pip install {req} -t {layer_dir}/python\",\n",
    "                    shell=True,\n",
    "                    check=True\n",
    "                )\n",
    "            \n",
    "            # Create ZIP file\n",
    "            with zipfile.ZipFile(layer_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "                for root, _, files in os.walk(layer_dir):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        arcname = os.path.relpath(file_path, layer_dir)\n",
    "                        zipf.write(file_path, arcname)\n",
    "            \n",
    "            # Upload to S3\n",
    "            s3_key = f\"lambda-layers/{layer_zip}\"\n",
    "            s3_client.upload_file(layer_zip, bucket_name, s3_key)\n",
    "            \n",
    "            # Create layer from S3\n",
    "            response = lambda_client.publish_layer_version(\n",
    "                LayerName=group_layer_name,\n",
    "                Description=f'Dependencies group {idx+1} for reasoning query function',\n",
    "                Content={\n",
    "                    'S3Bucket': bucket_name,\n",
    "                    'S3Key': s3_key\n",
    "                },\n",
    "                CompatibleRuntimes=['python3.12']\n",
    "            )\n",
    "            \n",
    "            layer_arns.append(response['LayerVersionArn'])\n",
    "            \n",
    "            # Cleanup group files\n",
    "            subprocess.run(f\"rm -rf {layer_dir}/python/*\", shell=True)\n",
    "            os.remove(layer_zip)\n",
    "            \n",
    "            logger.info(f\"Created layer part {idx+1}: {group_layer_name}\")\n",
    "            \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        if os.path.exists(layer_dir):\n",
    "            subprocess.run(f\"rm -rf {layer_dir}\", shell=True)\n",
    "    \n",
    "    return layer_arns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shipping Schema \n",
    "with open('schemas/ecom_shipping_schema.json', 'r') as file:\n",
    "    ecom_shipping_schema = json.load(file)\n",
    "ecom_shipping_schema_string = json.dumps(ecom_shipping_schema, indent=2)\n",
    "\n",
    "# Agent foundation model \n",
    "agent_foundation_model = [\n",
    "    # \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "    \"us.amazon.nova-lite-v1:0\"\n",
    "]\n",
    "\n",
    "# Force re-create default setting for Agent objects, but for now set to False\n",
    "Agent.set_force_recreate_default(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:11:34,749 - __main__ - INFO - Creating or retrieving IAM Role: LambdaExecutionRole-us-west-2-533\n",
      "2025-02-09 16:11:35,098 - __main__ - INFO - IAM Role LambdaExecutionRole-us-west-2-533 already exists. Retrieving existing role.\n",
      "2025-02-09 16:11:35,341 - __main__ - INFO - Attached AWSLambdaBasicExecutionRole to LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-09 16:11:35,469 - __main__ - INFO - Attached OpenSearch policy to IAM Role LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-09 16:11:35,587 - __main__ - INFO - Attached AOSS policy to IAM Role LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-09 16:11:35,589 - __main__ - INFO - Waiting 10 seconds for IAM role to propagate...\n",
      "2025-02-09 16:11:45,592 - __main__ - INFO - Packaging Lambda function from src/lambda/execute_dsl_query.py\n",
      "2025-02-09 16:11:45,593 - __main__ - INFO - Installing dependencies: ['opensearch-py', 'requests', 'urllib3']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py\n",
      "  Using cached opensearch_py-2.8.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-dateutil (from opensearch-py)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2024.07.04 (from opensearch-py)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Events (from opensearch-py)\n",
      "  Using cached Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting six>=1.5 (from python-dateutil->opensearch-py)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached opensearch_py-2.8.0-py3-none-any.whl (353 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: Events, urllib3, six, idna, charset-normalizer, certifi, requests, python-dateutil, opensearch-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.16.1 requires botocore<1.35.89,>=1.35.74, but you have botocore 1.36.9 which is incompatible.\n",
      "datasets 2.21.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "awscli 1.34.8 requires botocore==1.35.8, but you have botocore 1.36.9 which is incompatible.\n",
      "awscli 1.34.8 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-02-09 16:11:46,666 - __main__ - INFO - Creating Lambda deployment package: dsl_query_function.zip\n",
      "2025-02-09 16:11:46,707 - __main__ - INFO - Cleaning up temporary package directory...\n",
      "2025-02-09 16:11:46,744 - __main__ - INFO - Lambda package created successfully.\n",
      "2025-02-09 16:11:46,745 - __main__ - INFO - Creating/updating Lambda function: execute-dsl-query-us-west-2-533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Events-0.5 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 opensearch-py-2.8.0 python-dateutil-2.9.0.post0 requests-2.32.3 six-1.17.0 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:11:52,037 - __main__ - INFO - Lambda function execute-dsl-query-us-west-2-533 already exists. Updating...\n",
      "2025-02-09 16:11:59,478 - __main__ - INFO - Function state: Active, LastUpdateStatus: InProgress. Waiting...\n",
      "2025-02-09 16:12:04,577 - __main__ - INFO - Lambda function execute-dsl-query-us-west-2-533 updated successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main execution flow:\n",
    "    1. Create an IAM Role for Lambda.\n",
    "    2. Create/Update two Lambda functions (execute-dsl-query, execute-modified-dsl-query).\n",
    "    3. Create DSL Query Agent & Query Fixer Agent referencing those Lambda functions.\n",
    "    4. Retrieve the newly created agent IDs.\n",
    "    5. Add resource-based policies to each Lambda function for those agent IDs.\n",
    "    6. Create the Supervisor Agent to orchestrate both DSL Query and Query Fixer agents.\n",
    "    7. Invoke the Supervisor Agent with a sample query.\n",
    "    8. Delete the agents (cleanup).\n",
    "\"\"\"\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Create (or retrieve) IAM Role for Lambda\n",
    "# -------------------------------------------------------------------------\n",
    "IAM_ROLE_NAME = f\"LambdaExecutionRole-{agent_suffix}\"\n",
    "role_arn = create_iam_role(IAM_ROLE_NAME)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Create the first Lambda (execute-dsl-query)\n",
    "# -------------------------------------------------------------------------\n",
    "DSL_QUERY_LAMBDA_NAME = f\"execute-dsl-query-{agent_suffix}\"\n",
    "DSL_QUERY_LAMBDA_PATH = \"src/lambda/execute_dsl_query.py\"\n",
    "DSL_QUERY_ZIP_PATH = \"dsl_query_function.zip\"\n",
    "\n",
    "if not os.path.exists(DSL_QUERY_LAMBDA_PATH):\n",
    "    logger.error(f\"Error: {DSL_QUERY_LAMBDA_PATH} does not exist.\")\n",
    "\n",
    "DEPENDENCIES = [\"opensearch-py\", \"requests\", \"urllib3\"]\n",
    "\n",
    "# Package & create the Lambda\n",
    "create_lambda_package(DSL_QUERY_LAMBDA_PATH, DSL_QUERY_ZIP_PATH, DEPENDENCIES)\n",
    "create_lambda_function(\n",
    "    function_name=DSL_QUERY_LAMBDA_NAME,\n",
    "    role_arn=role_arn,\n",
    "    handler=\"execute_dsl_query.lambda_handler\",\n",
    "    runtime=\"python3.12\",\n",
    "    zip_file_path=DSL_QUERY_ZIP_PATH\n",
    ")\n",
    "os.remove(DSL_QUERY_ZIP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:12:04,595 - __main__ - INFO - Packaging Lambda function from src/lambda/execute_modified_dsl_query.py\n",
      "2025-02-09 16:12:04,596 - __main__ - INFO - Installing dependencies: ['opensearch-py', 'requests', 'urllib3']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py\n",
      "  Using cached opensearch_py-2.8.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-dateutil (from opensearch-py)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2024.07.04 (from opensearch-py)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Events (from opensearch-py)\n",
      "  Using cached Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting six>=1.5 (from python-dateutil->opensearch-py)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached opensearch_py-2.8.0-py3-none-any.whl (353 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: Events, urllib3, six, idna, charset-normalizer, certifi, requests, python-dateutil, opensearch-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.16.1 requires botocore<1.35.89,>=1.35.74, but you have botocore 1.36.9 which is incompatible.\n",
      "datasets 2.21.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "awscli 1.34.8 requires botocore==1.35.8, but you have botocore 1.36.9 which is incompatible.\n",
      "awscli 1.34.8 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-02-09 16:12:05,714 - __main__ - INFO - Creating Lambda deployment package: modified_query_function.zip\n",
      "2025-02-09 16:12:05,754 - __main__ - INFO - Cleaning up temporary package directory...\n",
      "2025-02-09 16:12:05,794 - __main__ - INFO - Lambda package created successfully.\n",
      "2025-02-09 16:12:05,795 - __main__ - INFO - Creating/updating Lambda function: execute-modified-dsl-query-us-west-2-533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Events-0.5 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 opensearch-py-2.8.0 python-dateutil-2.9.0.post0 requests-2.32.3 six-1.17.0 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:12:12,253 - __main__ - INFO - Lambda function execute-modified-dsl-query-us-west-2-533 already exists. Updating...\n",
      "2025-02-09 16:12:18,531 - __main__ - INFO - Function state: Active, LastUpdateStatus: InProgress. Waiting...\n",
      "2025-02-09 16:12:23,642 - __main__ - INFO - Lambda function execute-modified-dsl-query-us-west-2-533 updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2(b). Create the second Lambda (execute-modified-dsl-query)\n",
    "# -------------------------------------------------------------------------\n",
    "MODIFIED_QUERY_LAMBDA_NAME = f\"execute-modified-dsl-query-{agent_suffix}\"\n",
    "MODIFIED_QUERY_LAMBDA_PATH = \"src/lambda/execute_modified_dsl_query.py\"\n",
    "MODIFIED_QUERY_ZIP_PATH = \"modified_query_function.zip\"\n",
    "\n",
    "if not os.path.exists(MODIFIED_QUERY_LAMBDA_PATH):\n",
    "    logger.error(f\"Error: {MODIFIED_QUERY_LAMBDA_PATH} does not exist.\")\n",
    "\n",
    "create_lambda_package(MODIFIED_QUERY_LAMBDA_PATH, MODIFIED_QUERY_ZIP_PATH, DEPENDENCIES)\n",
    "create_lambda_function(\n",
    "    function_name=MODIFIED_QUERY_LAMBDA_NAME,\n",
    "    role_arn=role_arn,\n",
    "    handler=\"execute_modified_dsl_query.lambda_handler\",\n",
    "    runtime=\"python3.12\",\n",
    "    zip_file_path=MODIFIED_QUERY_ZIP_PATH\n",
    ")\n",
    "os.remove(MODIFIED_QUERY_ZIP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up existing resources\n",
    "# try:\n",
    "#     # Delete the Lambda function\n",
    "#     lambda_client.delete_function(FunctionName=REASONING_QUERY_LAMBDA_NAME)\n",
    "#     logger.info(f\"Deleted Lambda function: {REASONING_QUERY_LAMBDA_NAME}\")\n",
    "# except lambda_client.exceptions.ResourceNotFoundException:\n",
    "#     pass\n",
    "\n",
    "# # Delete the layers\n",
    "# for group in requirements_groups:\n",
    "#     layer_name = f\"reasoning-deps-{group['name']}-{agent_suffix}\"\n",
    "#     try:\n",
    "#         # Get all layer versions\n",
    "#         versions = lambda_client.list_layer_versions(LayerName=layer_name)\n",
    "#         # Delete each version\n",
    "#         for version in versions['LayerVersions']:\n",
    "#             lambda_client.delete_layer_version(\n",
    "#                 LayerName=layer_name,\n",
    "#                 VersionNumber=version['Version']\n",
    "#             )\n",
    "#         logger.info(f\"Deleted layer: {layer_name}\")\n",
    "#     except lambda_client.exceptions.ResourceNotFoundException:\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "# REASONING_QUERY_LAMBDA_NAME = f\"execute-reasoning-query-{agent_suffix}\"\n",
    "# REASONING_QUERY_LAMBDA_PATH = \"src/lambda/execute_reasoning_query.py\"\n",
    "# REASONING_QUERY_ZIP_PATH = \"reasoning_query_function.zip\"\n",
    "\n",
    "# # Create minimal Lambda package with just the handler code\n",
    "# create_lambda_package(\n",
    "#     source_file=REASONING_QUERY_LAMBDA_PATH,\n",
    "#     zip_file_path=REASONING_QUERY_ZIP_PATH,\n",
    "#     dependencies=[]  # No dependencies needed in package since using layers\n",
    "# )\n",
    "\n",
    "\n",
    "# # Define S3 bucket for layer deployment (ensure agent_suffix is defined)\n",
    "# LAYER_BUCKET = f\"lambda-layers-{agent_suffix.lower()}\"\n",
    "\n",
    "# # Create the S3 bucket if it doesn't already exist\n",
    "# try:\n",
    "#     s3_client.create_bucket(\n",
    "#         Bucket=LAYER_BUCKET,\n",
    "#         CreateBucketConfiguration={'LocationConstraint': region}\n",
    "#     )\n",
    "# except s3_client.exceptions.BucketAlreadyExists:\n",
    "#     pass\n",
    "# except s3_client.exceptions.BucketAlreadyOwnedByYou:\n",
    "#     pass\n",
    "\n",
    "# # Define optimized requirements with only necessary components (torch removed)\n",
    "# requirements_groups = [\n",
    "#     # Group 1: Core dependencies for tokenization and model prompt formatting\n",
    "#     {\n",
    "#         'name': 'core',\n",
    "#         'packages': [\n",
    "#             'transformers',   # Use transformers without the [torch] extra\n",
    "#             'tokenizers',\n",
    "#             'numpy',\n",
    "#             'typing-extensions'\n",
    "#         ]\n",
    "#     },\n",
    "#     # Group 2: Utility dependencies\n",
    "#     {\n",
    "#         'name': 'utils',\n",
    "#         'packages': [\n",
    "#             'regex',\n",
    "#             'requests',\n",
    "#             'tqdm'\n",
    "#         ]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# def create_optimized_layer(group, bucket_name):\n",
    "#     \"\"\"Create an optimized Lambda layer for a group of requirements.\"\"\"\n",
    "#     layer_name = f\"reasoning-deps-{group['name']}-{agent_suffix}\"\n",
    "#     layer_dir = f\"layer_{group['name']}\"\n",
    "#     zip_path = f\"{layer_name}.zip\"\n",
    "    \n",
    "#     try:\n",
    "#         # Create the directory structure for the layer\n",
    "#         os.makedirs(f\"{layer_dir}/python\", exist_ok=True)\n",
    "        \n",
    "#         # Special handling for numpy to prevent source directory issues\n",
    "#         if 'numpy' in group['packages']:\n",
    "#             group['packages'].remove('numpy')\n",
    "#             logger.info(\"Installing numpy separately with special handling\")\n",
    "#             subprocess.run(\n",
    "#                 \"pip install numpy --no-cache-dir --platform manylinux2014_x86_64 \"\n",
    "#                 f\"--target {layer_dir}/python --only-binary=:all:\",\n",
    "#                 shell=True,\n",
    "#                 check=True\n",
    "#             )\n",
    "        \n",
    "#         # Install remaining packages into the layer directory\n",
    "#         for package in group['packages']:\n",
    "#             logger.info(f\"Installing package {package} into {layer_dir}/python\")\n",
    "#             subprocess.run(\n",
    "#                 f\"pip install {package} --no-cache-dir --target {layer_dir}/python\",\n",
    "#                 shell=True,\n",
    "#                 check=True\n",
    "#             )\n",
    "        \n",
    "#         # Remove unnecessary files to reduce the layer size\n",
    "#         cleanup_patterns = [\n",
    "#             # \"*.dist-info\",\n",
    "#             \"*.egg-info\",\n",
    "#             \"__pycache__\",\n",
    "#             \"*.pyc\",\n",
    "#             \"*.pyo\",\n",
    "#             \"tests\",\n",
    "#             # \"*.so\"  # Remove shared objects as they might cause compatibility issues\n",
    "#         ]\n",
    "#         for pattern in cleanup_patterns:\n",
    "#             logger.info(f\"Cleaning up files matching {pattern} in {layer_dir}/python\")\n",
    "#             subprocess.run(\n",
    "#                 f\"find {layer_dir}/python -name '{pattern}' -exec rm -rf {{}} +\",\n",
    "#                 shell=True\n",
    "#             )\n",
    "        \n",
    "#         # Create a ZIP file from the layer directory\n",
    "#         logger.info(f\"Creating zip file {zip_path} from {layer_dir}\")\n",
    "#         with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "#             for root, _, files in os.walk(layer_dir):\n",
    "#                 for file in files:\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     arcname = os.path.relpath(file_path, layer_dir)\n",
    "#                     zipf.write(file_path, arcname)\n",
    "        \n",
    "#         # Upload the ZIP file to S3\n",
    "#         s3_key = f\"lambda-layers/{zip_path}\"\n",
    "#         logger.info(f\"Uploading {zip_path} to S3 bucket {bucket_name} with key {s3_key}\")\n",
    "#         s3_client.upload_file(zip_path, bucket_name, s3_key)\n",
    "        \n",
    "#         # Publish the Lambda layer version\n",
    "#         logger.info(f\"Publishing Lambda layer version for {layer_name}\")\n",
    "#         response = lambda_client.publish_layer_version(\n",
    "#             LayerName=layer_name,\n",
    "#             Description=f'Optimized {group[\"name\"]} dependencies',\n",
    "#             Content={\n",
    "#                 'S3Bucket': bucket_name,\n",
    "#                 'S3Key': s3_key\n",
    "#             },\n",
    "#             CompatibleRuntimes=['python3.12']\n",
    "#         )\n",
    "        \n",
    "#         return response['LayerVersionArn']\n",
    "        \n",
    "#     finally:\n",
    "#         # Cleanup local files\n",
    "#         if os.path.exists(layer_dir):\n",
    "#             subprocess.run(f\"rm -rf {layer_dir}\", shell=True)\n",
    "#         if os.path.exists(zip_path):\n",
    "#             os.remove(zip_path)\n",
    "\n",
    "# # Create layers and then the Lambda function with the published layers\n",
    "# try:\n",
    "#     layer_arns = []\n",
    "#     for group in requirements_groups:\n",
    "#         logger.info(f\"Creating layer for group: {group['name']}...\")\n",
    "#         layer_arn = create_optimized_layer(group, LAYER_BUCKET)\n",
    "#         layer_arns.append(layer_arn)\n",
    "#         logger.info(f\"Created layer {group['name']}: {layer_arn}\")\n",
    "    \n",
    "#     # Create the Lambda function using the optimized layers\n",
    "#     create_lambda_function(\n",
    "#         function_name=REASONING_QUERY_LAMBDA_NAME,\n",
    "#         role_arn=role_arn,\n",
    "#         handler=\"execute_reasoning_query.lambda_handler\",\n",
    "#         runtime=\"python3.12\",\n",
    "#         zip_file_path=REASONING_QUERY_ZIP_PATH,\n",
    "#         layers=layer_arns,\n",
    "#         timeout=900,\n",
    "#         memory_size=2048\n",
    "#     )\n",
    "    \n",
    "#     logger.info(f\"Lambda function '{REASONING_QUERY_LAMBDA_NAME}' created successfully\")\n",
    "#     logger.info(f\"Attached layers: {json.dumps(layer_arns, indent=2)}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Failed to create Lambda layers: {str(e)}\")\n",
    "#     raise\n",
    "# finally:\n",
    "#     # Cleanup the Lambda function zip file if it exists\n",
    "#     if os.path.exists(REASONING_QUERY_ZIP_PATH):\n",
    "#         os.remove(REASONING_QUERY_ZIP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_query_lambda_arn = \"arn:aws:lambda:us-west-2:533267284022:function:HuggingFaceLambdaFunction\"\n",
    "\n",
    "REASONING_QUERY_LAMBDA_NAME =\"HuggingFaceLambdaFunction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:12:23,673 - __main__ - INFO - Creating DSL Query Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting existing agent and corresponding lambda for: dsl-query-agent-us-west-2-533...\n",
      "Agent dsl-query-agent-us-west-2-533 not found\n",
      "Creating agent dsl-query-agent-us-west-2-533...\n",
      "Created agent, id: HAPHJFTMAB, alias id: TSTALIASID\n",
      "\n",
      "Adding action group with Lambda: arn:aws:lambda:us-west-2:533267284022:function:execute-dsl-query-us-west-2-533...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id HAPHJFTMAB current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id HAPHJFTMAB current status: PREPARED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:12:51,695 - __main__ - INFO - Creating Query Fixer Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: Agent: dsl-query-agent-us-west-2-533, id: HAPHJFTMAB, alias id: YB1XUC0ZRR\n",
      "\n",
      "\n",
      "Deleting existing agent and corresponding lambda for: query-fixer-agent-us-west-2-533...\n",
      "Agent query-fixer-agent-us-west-2-533 not found\n",
      "Creating agent query-fixer-agent-us-west-2-533...\n",
      "Created agent, id: Y4W1ICA55I, alias id: TSTALIASID\n",
      "\n",
      "Adding action group with Lambda: arn:aws:lambda:us-west-2:533267284022:function:execute-modified-dsl-query-us-west-2-533...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id Y4W1ICA55I current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id Y4W1ICA55I current status: PREPARED\n",
      "DONE: Agent: query-fixer-agent-us-west-2-533, id: Y4W1ICA55I, alias id: XIQPAGMFD5\n",
      "\n",
      "\n",
      "Deleting existing agent and corresponding lambda for: kb-response-agent-us-west-2-533...\n",
      "Agent kb-response-agent-us-west-2-533 not found\n",
      "Creating agent kb-response-agent-us-west-2-533...\n",
      "Creating agent: kb-response-agent-us-west-2-533...\n",
      "Created agent IAM role: arn:aws:iam::533267284022:role/DEFAULT_AgentExecutionRole...\n",
      "Creating agent: kb-response-agent-us-west-2-533 with model: us.anthropic.claude-3-5-sonnet-20240620-v1:0...\n",
      "kwargs: {}\n",
      "Created agent, resulting id: NMYVAUXBYQ\n",
      "Created agent, id: NMYVAUXBYQ, alias id: TSTALIASID\n",
      "\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id NMYVAUXBYQ current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id NMYVAUXBYQ current status: PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id NMYVAUXBYQ current status: PREPARED\n",
      "DONE: Agent: kb-response-agent-us-west-2-533, id: NMYVAUXBYQ, alias id: BTMK53A5UM\n",
      "\n",
      "\n",
      "Deleting existing agent and corresponding lambda for: reasoning-agent-us-west-2-533...\n",
      "Agent reasoning-agent-us-west-2-533 not found\n",
      "Creating agent reasoning-agent-us-west-2-533...\n",
      "Created agent, id: QEZS20IZCK, alias id: TSTALIASID\n",
      "\n",
      "Adding action group with Lambda: arn:aws:lambda:us-west-2:533267284022:function:HuggingFaceLambdaFunction...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id QEZS20IZCK current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id QEZS20IZCK current status: PREPARED\n",
      "DONE: Agent: reasoning-agent-us-west-2-533, id: QEZS20IZCK, alias id: KISEPBZICH\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Create the DSL Query Agent & Query Fixer Agent\n",
    "#\n",
    "#    Important: reference the just-created Lambda ARNs in \"tool_code\"\n",
    "#    The actual ARN is \"arn:aws:lambda:<REGION>:<ACCOUNT>:function:<FUNCTION_NAME>\"\n",
    "# -------------------------------------------------------------------------\n",
    "dsl_query_lambda_arn = f\"arn:aws:lambda:{region}:{account_id}:function:{DSL_QUERY_LAMBDA_NAME}\"\n",
    "modified_query_lambda_arn = f\"arn:aws:lambda:{region}:{account_id}:function:{MODIFIED_QUERY_LAMBDA_NAME}\"\n",
    "# reasoning_query_lambda_arn = f\"arn:aws:lambda:{region}:{account_id}:function:{REASONING_QUERY_LAMBDA_NAME}\"\n",
    "\n",
    "logger.info(\"Creating DSL Query Agent...\")\n",
    "dsl_query_agent = Agent.direct_create(\n",
    "    name=f\"dsl-query-agent-{agent_suffix}\",\n",
    "    role=\"DSL Query Creator\",\n",
    "    goal=\"Create DSL queries for a given user query\",\n",
    "    instructions=f\"\"\"\n",
    "    You are an expert in generating Query DSL for Elasticsearch-style queries. Your task is to convert a \n",
    "    given natural language user question into a well-structured Query DSL.\n",
    "    \n",
    "    ## Instructions:\n",
    "    - Use the provided e-commerce shipping schema to construct the query.\n",
    "    - Encapsulate the output in <json>...</json> tags.\n",
    "    - Follow the syntax of the Query DSL strictly; do not introduce elements outside the provided schema.\n",
    "    \n",
    "    ## Query Construction Rules:\n",
    "    - **Keyword fields** (carrier, status, country): Use `term` for exact matches or `prefix`/`wildcard` for partial matches.\n",
    "    - **Text fields** (description, address): Use `match` queries to account for analyzed terms.\n",
    "    - **Nested fields** (tracking): Always use `nested` queries.\n",
    "    - **Date fields**: Use `range` queries with date math for filtering by date ranges.\n",
    "    - **Aggregations**: When counting occurrences, use a 'terms' aggregation on the relevant keyword field to capture the exact values present (e.g., 'delivery.carrier').\n",
    "    - Break down complex queries into smaller parts for accuracy.\n",
    "    - Think step-by-step before constructing the query.\n",
    "\n",
    "\n",
    "    ## Schema:\n",
    "    {ecom_shipping_schema_string}\n",
    "\n",
    "    ## Output Format:\n",
    "    - Return only the generated Query DSL within <json>...</json> tags.\n",
    "    - Do not include explanations, comments, or additional text.\n",
    "    \"\"\",\n",
    "    tool_code=dsl_query_lambda_arn,\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"execute_dsl_query\",\n",
    "            \"description\": \"Executes a given DSL query and returns the results\",\n",
    "            \"parameters\": {\n",
    "                \"dsl_query\": {\n",
    "                    \"description\": \"The DSL query to execute\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(\"Creating Query Fixer Agent...\")\n",
    "query_fixer_agent = Agent.direct_create(\n",
    "    name=f\"query-fixer-agent-{agent_suffix}\",\n",
    "    role=\"Query Repair Specialist\",\n",
    "    goal=\"Fix and optimize failed DSL queries\",\n",
    "    instructions=f\"\"\"\n",
    "    You are an expert query debugger and optimizer. Your tasks are:\n",
    "    1. Analyze failed DSL queries from the query generator\n",
    "    2. Diagnose errors using OpenSearch error messages\n",
    "    3. Apply targeted fixes while maintaining original intent\n",
    "    4. Optimize queries for better recall when results are empty\n",
    "    5. Extract exact terms from 'terms' aggregations for accurate reporting.\n",
    "    6. Identify alternative ways to answer queries when direct fields are missing.\n",
    "    7. Recognize schema gaps and propose workarounds or schema modifications.\n",
    "\n",
    "    ## Repair Strategies:\n",
    "    - SYNTAX ERRORS: Fix formatting issues in nested queries/aggregations\n",
    "    - FIELD ERRORS: Map invalid fields to valid schema equivalents\n",
    "    - ZERO HITS: Apply query relaxation techniques:\n",
    "        * Add wildcards to keyword searches\n",
    "        * Expand date ranges\n",
    "        * Reduce strictness of term matches\n",
    "        * Add synonym expansion\n",
    "    - Ensure queries include 'terms' aggregations to capture exact values\n",
    "\n",
    "    ## Optimization Rules:\n",
    "    - Maintain original query structure where possible\n",
    "    - Prefer query-time fixes over rearchitecting\n",
    "    - Document all modifications in revision notes\n",
    "    - Limit query relaxation to 3 iterations\n",
    "    - When results are found, check the 'terms' aggregation for the exact field values.\n",
    "    - Report the exact terms from the data (e.g., use \"DHL Express\" if that's the stored value).\n",
    "\n",
    "    ## Schema Gap Analysis & Alternative Solutions:\n",
    "    - Creative Field Mapping: If direct fields are missing, use existing fields to infer answers.\n",
    "      * Example: If \"delivery duration\" is not available, compute it using `delivered_time - out_for_delivery_time`.\n",
    "    - Schema Enhancement: Identify missing fields required for full query support.\n",
    "    - Derived Data Solutions:\n",
    "      * If exact data isn’t available but can be computed, create scripted fields using painless scripting.\n",
    "      * If no alternative exists, clearly state the required data and suggest schema modifications.\n",
    "\n",
    "\n",
    "\n",
    "    ## Schema:\n",
    "    {ecom_shipping_schema_string}\n",
    "\n",
    "    ## Output Format:\n",
    "    - Return modified query in <json> tags\n",
    "    - Include revision notes and exact terms from aggregations in <notes> tags \n",
    "    \"\"\",\n",
    "    tool_code=modified_query_lambda_arn,\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"retry_query\",\n",
    "            \"description\": \"Retries a modified version of the failed query\",\n",
    "            \"parameters\": {\n",
    "                \"modified_dsl_query\": {\n",
    "                    \"description\": \"The corrected DSL query\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True\n",
    "                },\n",
    "                \"revision_notes\": {\n",
    "                    \"description\": \"Description of modifications made\",\n",
    "                    \"type\": \"string\",\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "kb_rag_agent = Agent.direct_create(\n",
    "    name=f\"kb-response-agent-{agent_suffix}\",\n",
    "    role=\"Knowledge Base Content Analyzer\",\n",
    "    goal=\"Analyze retrieved document content and generate well-structured responses\",\n",
    "    instructions=\"\"\"\n",
    "    You analyze pre-retrieved document content and generate clear, accurate responses.\n",
    "\n",
    "    ## Response Rules:\n",
    "    - Synthesize information from provided passages\n",
    "    - Include relevant quotes with proper citations\n",
    "    - Use consistent citation format [doc_id:para_num]\n",
    "    - Maintain factual accuracy\n",
    "    - Flag any inconsistencies between sources\n",
    "\n",
    "    ## Output Format:\n",
    "    Response should be structured as:\n",
    "    1. Direct answer\n",
    "    2. Supporting evidence\n",
    "    3. Source citations\n",
    "    4. Confidence level (High/Medium/Low)\n",
    "\n",
    "    ## Quality Guidelines:\n",
    "    - Prefer direct quotes for key information\n",
    "    - Summarize when appropriate\n",
    "    - Note any information gaps\n",
    "    - Maintain neutral tone\n",
    "    \"\"\",\n",
    "    kb_descr=\"Use knowledge base to extract relevant information, analyze content across multiple documents, and generate accurate responses with proper citations. Focus on maintaining context and factual accuracy.\",\n",
    "    kb_id=\"5GADU65GNF\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "reasoning_agent = Agent.direct_create(\n",
    "    name=f\"reasoning-agent-{agent_suffix}\",\n",
    "    role=\"Reasoning Specialist\",\n",
    "    goal=\"Analyze complex scenarios and provide well-reasoned answers\",\n",
    "    instructions=\"\"\"\n",
    "    You are a reasoning specialist tasked with analyzing complex scenarios and providing well-reasoned answers.\n",
    "\n",
    "    ## Task Description:\n",
    "    - Analyze the provided scenario and identify key elements\n",
    "    - Break down the problem into smaller parts for easier analysis\n",
    "    - Use logical reasoning to derive conclusions\n",
    "    - Clearly explain your thought process and reasoning\n",
    "\n",
    "    ## Output Format:\n",
    "    - Explain your reasoning and thought process in a <reasoning>...</reasoning> block\n",
    "    - Include any assumptions made in your analysis in a <assumptions>...</assumptions> block\n",
    "    \"\"\",\n",
    "    tool_code=reasoning_query_lambda_arn,\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"execute_reasoning\",\n",
    "            \"description\": \"Executes a reasoning model to analyze complex scenarios\",\n",
    "            \"parameters\": {\n",
    "                \"scenario\": {\n",
    "                    \"description\": \"The scenario to analyze\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:14:20,444 - __main__ - INFO - Retrieving DSL Query Agent ID...\n",
      "2025-02-09 16:14:20,533 - __main__ - INFO - DSL Query Agent ID: HAPHJFTMAB\n",
      "2025-02-09 16:14:20,533 - __main__ - INFO - Retrieving Query Fixer Agent ID...\n",
      "2025-02-09 16:14:20,611 - __main__ - INFO - Query Fixer Agent ID: Y4W1ICA55I\n",
      "2025-02-09 16:14:20,611 - __main__ - INFO - Retrieving KB Response Agent ID...\n",
      "2025-02-09 16:14:20,687 - __main__ - INFO - KB Response Agent ID: NMYVAUXBYQ\n",
      "2025-02-09 16:14:20,688 - __main__ - INFO - Retrieving Reasoning Agent ID...\n",
      "2025-02-09 16:14:20,774 - __main__ - INFO - Reasoning Agent ID: QEZS20IZCK\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Retrieve the newly created Agent IDs\n",
    "# -------------------------------------------------------------------------\n",
    "logger.info(\"Retrieving DSL Query Agent ID...\")\n",
    "dsl_query_agent_id = agents_helper.get_agent_id_by_name(dsl_query_agent.name)\n",
    "logger.info(f\"DSL Query Agent ID: {dsl_query_agent_id}\")\n",
    "\n",
    "logger.info(\"Retrieving Query Fixer Agent ID...\")\n",
    "query_fixer_agent_id = agents_helper.get_agent_id_by_name(query_fixer_agent.name)\n",
    "logger.info(f\"Query Fixer Agent ID: {query_fixer_agent_id}\")\n",
    "\n",
    "logger.info(\"Retrieving KB Response Agent ID...\")\n",
    "kb_rag_agent_id = agents_helper.get_agent_id_by_name(kb_rag_agent.name)\n",
    "logger.info(f\"KB Response Agent ID: {kb_rag_agent_id}\")\n",
    "\n",
    "logger.info(\"Retrieving Reasoning Agent ID...\")\n",
    "reasoning_agent_id = agents_helper.get_agent_id_by_name(reasoning_agent.name)\n",
    "logger.info(f\"Reasoning Agent ID: {reasoning_agent_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:14:20,778 - __main__ - INFO - Adding resource-based policy to Lambda function execute-dsl-query-us-west-2-533 for agents: ['HAPHJFTMAB']\n",
      "2025-02-09 16:14:20,963 - __main__ - INFO - Removing existing statement: AllowExecutionFromBedrockAgent_RCUWCRBCR4\n",
      "2025-02-09 16:14:21,129 - __main__ - INFO - Added permission for statement: AllowExecutionFromBedrockAgent_HAPHJFTMAB\n",
      "2025-02-09 16:14:21,131 - __main__ - INFO - Adding resource-based policy to Lambda function execute-modified-dsl-query-us-west-2-533 for agents: ['Y4W1ICA55I']\n",
      "2025-02-09 16:14:21,175 - __main__ - INFO - Removing existing statement: AllowExecutionFromBedrockAgent_ZJBRMHWDBY\n",
      "2025-02-09 16:14:21,338 - __main__ - INFO - Added permission for statement: AllowExecutionFromBedrockAgent_Y4W1ICA55I\n",
      "2025-02-09 16:14:21,339 - __main__ - INFO - Adding resource-based policy to Lambda function HuggingFaceLambdaFunction for agents: ['QEZS20IZCK']\n",
      "2025-02-09 16:14:21,396 - __main__ - INFO - Removing existing statement: AllowExecutionFromBedrockAgent_FOMMXMUTYV\n",
      "2025-02-09 16:14:21,570 - __main__ - INFO - Added permission for statement: AllowExecutionFromBedrockAgent_QEZS20IZCK\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 5. Add resource-based policy to each Lambda so the Agents can invoke them\n",
    "# -------------------------------------------------------------------------\n",
    "add_resource_based_policy(DSL_QUERY_LAMBDA_NAME, [dsl_query_agent_id], region, account_id)\n",
    "add_resource_based_policy(MODIFIED_QUERY_LAMBDA_NAME, [query_fixer_agent_id], region, account_id)\n",
    "add_resource_based_policy(REASONING_QUERY_LAMBDA_NAME, [reasoning_agent_id], region, account_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervisor_agent = SupervisorAgent.direct_create(\n",
    "#     name=f\"supervisor-agent-{agent_suffix}\",\n",
    "#     role=\"Query Pipeline Orchestrator\",\n",
    "#     collaboration_type=\"SUPERVISOR\",\n",
    "#     collaborator_objects=[dsl_query_agent, query_fixer_agent, kb_rag_agent, reasoning_agent],\n",
    "#     collaborator_agents=[\n",
    "#         {\n",
    "#             \"agent\": dsl_query_agent.name,\n",
    "#             \"instructions\": dedent(f\"\"\"\n",
    "#                 {dsl_query_agent.name} is responsible for generating the primary DSL query based on \n",
    "#                 the provided e-commerce shipping schema. Your task is to produce a precise Query DSL \n",
    "#                 encapsulated in <json>...</json> tags. Ensure the query strictly follows the schema \n",
    "#                 and DSL syntax without any additional commentary or explanations.\n",
    "#             \"\"\").strip(),\n",
    "#             \"relay_conversation_history\": \"DISABLED\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"agent\": query_fixer_agent.name,\n",
    "#             \"instructions\": dedent(f\"\"\"\n",
    "#                 Engage {query_fixer_agent.name} when any of the following conditions occur:\n",
    "#                 1. The DSL query returns syntax or validation errors.\n",
    "#                 2. The DSL query execution returns zero hits.\n",
    "#                 3. The query requires optimization for improved recall.\n",
    "#                 4. Alternative query solutions are needed due to schema limitations.\n",
    "                \n",
    "#                 Responsibilities:\n",
    "#                 - Analyze error messages and the current query structure.\n",
    "#                 - Apply targeted fixes that preserve the original query intent.\n",
    "#                 - Implement query relaxation techniques (for example, adding wildcards, extending date ranges, or expanding term matches).\n",
    "#                 - Identify and map alternative fields if direct schema fields are missing.\n",
    "#                 - Suggest schema enhancements when appropriate.\n",
    "#                 - Document all modifications with clear revision notes and output exact terms from aggregations.\n",
    "                \n",
    "#                 Return the corrected DSL query within <json>...</json> tags and include any revision notes within <notes>...</notes> tags.\n",
    "#             \"\"\"),\n",
    "#             \"relay_conversation_history\": \"DISABLED\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"agent\": kb_rag_agent.name,\n",
    "#             \"instructions\": dedent(f\"\"\"\n",
    "#                 Engage {kb_rag_agent.name} to answer user questions that require analyzing the document content retrieved from executed queries.\n",
    "#                 When search results are available, your task is to:\n",
    "                \n",
    "#                 1. Synthesize and validate the information from the provided passages.\n",
    "#                 2. Generate a final response that includes a direct answer and supporting evidence with relevant quotes and citations.\n",
    "                \n",
    "#                 Your output must be clear, well-structured, and factually accurate to support decision-making.\n",
    "#             \"\"\"),\n",
    "#             \"relay_conversation_history\": \"DISABLED\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"agent\": reasoning_agent.name,\n",
    "#             \"instructions\": dedent(f\"\"\"\n",
    "#                 Engage {reasoning_agent.name} when the scenario is complex and the supervisor agent cannot determine the appropriate call.\n",
    "#                 This includes situations with multiple ambiguous factors or when further logical analysis is required.\n",
    "                \n",
    "#                 Responsibilities:\n",
    "#                 1. Analyze the provided scenario and identify its key elements.\n",
    "#                 2. Break down the problem into manageable parts.\n",
    "#                 3. Use logical reasoning to derive well-reasoned conclusions.\n",
    "#                 4. Explain your thought process in a <reasoning>...</reasoning> block and list any assumptions in an <assumptions>...</assumptions> block.\n",
    "#             \"\"\"),\n",
    "#             \"relay_conversation_history\": \"DISABLED\"\n",
    "#         }\n",
    "#     ],\n",
    "#     instructions=dedent(f\"\"\"\n",
    "#         High-Level Overview:\n",
    "#         Route user queries to the appropriate agent based on the type of answer required:\n",
    "#           - Structured Data Retrieval: If the query requires retrieving structured information from the e-commerce shipping data, route it to {dsl_query_agent.name}. \n",
    "#             If the DSL query returns errors or zero hits, immediately route the query to {query_fixer_agent.name} for reattempts.\n",
    "#           - Document Content Analysis: If the query requires synthesizing and analyzing document content from executed queries, route it to {kb_rag_agent.name}.\n",
    "#           - Complex Scenarios: When the supervisor agent determines that the scenario is too complex or ambiguous to decide the next step, route it to {reasoning_agent.name}.\n",
    "        \n",
    "#         Detailed Instructions:\n",
    "        \n",
    "#         Route A: Structured Data Retrieval (DSL Query Agent + Query Fixer Agent)\n",
    "#         1. Initial Query Analysis:\n",
    "#            - Receive the user's natural language query.\n",
    "#            - Determine if the query requires structured data retrieval from the e-commerce shipping data.\n",
    "#            - Validate the query against the provided schema:\n",
    "#              {ecom_shipping_schema_string}\n",
    "#            - If the query qualifies, route it to {dsl_query_agent.name}.\n",
    "        \n",
    "#         2. DSL Query Execution:\n",
    "#            - {dsl_query_agent.name} generates a Query DSL encapsulated in <json>...</json> tags that follows the provided schema.\n",
    "        \n",
    "#         3. Error Handling & Retry:\n",
    "#            - Monitor the query execution results:\n",
    "#              a. If the DSL query returns syntax or validation errors, or if the result is zero hits, capture the error context.\n",
    "#              b. Immediately route the query, along with diagnostic details, to {query_fixer_agent.name}.\n",
    "#              c. {query_fixer_agent.name} applies targeted fixes and query relaxation techniques, then returns a modified DSL query.\n",
    "#              d. Validate the modified query; allow up to 3 retry attempts if necessary.\n",
    "        \n",
    "#         4. Evaluation & Final Approval (for structured data queries):\n",
    "#            - Confirm that the final DSL query adheres to best practices (e.g., proper nested queries, correct field types and mappings).\n",
    "#            - Maintain an audit trail of all query versions and modifications.\n",
    "#            - Generate an execution summary including:\n",
    "#              - Query versions attempted.\n",
    "#              - Reasons for modifications.\n",
    "#              - Performance metrics.\n",
    "        \n",
    "#         General Aggregation Guidance:\n",
    "#            - If an aggregation returns an unexpectedly inflated count, verify whether it is counting nested or repeated values.\n",
    "#            - To accurately count unique items, use a cardinality aggregation on a unique identifier rather than aggregating on fields that may contain duplicate entries.\n",
    "        \n",
    "#         Route B: Document Content Analysis (KB Response Agent)\n",
    "#         1. Initial Query Analysis:\n",
    "#            - Receive the user's natural language query.\n",
    "#            - Determine if the query requires synthesizing and analyzing document content from executed queries.\n",
    "#            - If so, route the query to {kb_rag_agent.name}.\n",
    "        \n",
    "#         2. KB Response Generation:\n",
    "#            - {kb_rag_agent.name} synthesizes and validates the information from the provided passages.\n",
    "#            - Generate a final response that includes:\n",
    "#              a. A direct answer.\n",
    "#              b. Supporting evidence with relevant quotes and citations.\n",
    "#              c. A confidence level.\n",
    "        \n",
    "#         3. Final Response:\n",
    "#            - Deliver the final, well-structured answer to the user by addressing the user by name.\n",
    "        \n",
    "#         Route C: Complex Scenarios (Reasoning Agent)\n",
    "#         1. When a scenario is identified as complex—i.e., the supervisor agent cannot determine the appropriate call due to multiple ambiguous factors—route the query to {reasoning_agent.name}.\n",
    "#         2. {reasoning_agent.name} will analyze the scenario, break it down into manageable parts, and provide a well-reasoned explanation.\n",
    "#         3. The output must include your thought process in a <reasoning>...</reasoning> block and any assumptions in an <assumptions>...</assumptions> block.\n",
    "#     \"\"\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent supervisor-agent-us-west-2-533 not found\n",
      "\n",
      "Created supervisor, id: MZAK8ELKGV, alias id: TSTALIASID\n",
      "\n",
      "  associating sub-agents / collaborators to supervisor...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id MZAK8ELKGV current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id MZAK8ELKGV current status: PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id MZAK8ELKGV current status: PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id MZAK8ELKGV current status: PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id MZAK8ELKGV current status: PREPARED\n",
      "DONE: Agent: supervisor-agent-us-west-2-533, id: MZAK8ELKGV, alias id: WECCGNIT8G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supervisor_agent = SupervisorAgent.direct_create(\n",
    "    name=f\"supervisor-agent-{agent_suffix}\",\n",
    "    role=\"Query Pipeline Orchestrator\",\n",
    "    collaboration_type=\"SUPERVISOR\",\n",
    "    collaborator_objects=[dsl_query_agent, query_fixer_agent, kb_rag_agent, reasoning_agent],\n",
    "    collaborator_agents=[\n",
    "        {\n",
    "            \"agent\": dsl_query_agent.name,\n",
    "            \"instructions\": dedent(f\"\"\"\n",
    "                {dsl_query_agent.name} is responsible for generating the primary DSL query based on \n",
    "                the provided e-commerce shipping schema. Your task is to produce a precise Query DSL \n",
    "                encapsulated in <json>...</json> tags. Ensure the query strictly follows the schema \n",
    "                and DSL syntax without any additional commentary or explanations.\n",
    "                \n",
    "                For testing purposes, if you encounter any uncertainty or complexity, always seek advice from the reasoning agent.\n",
    "            \"\"\").strip(),\n",
    "            \"relay_conversation_history\": \"DISABLED\"\n",
    "        },\n",
    "        {\n",
    "            \"agent\": query_fixer_agent.name,\n",
    "            \"instructions\": dedent(f\"\"\"\n",
    "                Engage {query_fixer_agent.name} when any of the following conditions occur:\n",
    "                1. The DSL query returns syntax or validation errors.\n",
    "                2. The DSL query execution returns zero hits.\n",
    "                3. The query requires optimization for improved recall.\n",
    "                4. Alternative query solutions are needed due to schema limitations.\n",
    "                \n",
    "                Responsibilities:\n",
    "                - Analyze error messages and the current query structure.\n",
    "                - Apply targeted fixes that preserve the original query intent.\n",
    "                - Implement query relaxation techniques (for example, adding wildcards, extending date ranges, or expanding term matches).\n",
    "                - Identify and map alternative fields if direct schema fields are missing.\n",
    "                - Suggest schema enhancements when appropriate.\n",
    "                - Document all modifications with clear revision notes and output exact terms from aggregations.\n",
    "                \n",
    "                For testing purposes, if any ambiguity arises during error handling, always consult the reasoning agent for advice.\n",
    "                \n",
    "                Return the corrected DSL query within <json>...</json> tags and include any revision notes within <notes>...</notes> tags.\n",
    "            \"\"\"),\n",
    "            \"relay_conversation_history\": \"DISABLED\"\n",
    "        },\n",
    "        {\n",
    "            \"agent\": kb_rag_agent.name,\n",
    "            \"instructions\": dedent(f\"\"\"\n",
    "                Engage {kb_rag_agent.name} to answer user questions that require analyzing the document content retrieved from executed queries.\n",
    "                When search results are available, your task is to:\n",
    "                \n",
    "                1. Synthesize and validate the information from the provided passages.\n",
    "                2. Generate a final response that includes a direct answer and supporting evidence with relevant quotes and citations.\n",
    "                \n",
    "                Your output must be clear, well-structured, and factually accurate to support decision-making.\n",
    "                \n",
    "                For testing purposes, if you face any complex interpretation issues, always seek advice from the reasoning agent.\n",
    "            \"\"\"),\n",
    "            \"relay_conversation_history\": \"DISABLED\"\n",
    "        },\n",
    "        {\n",
    "            \"agent\": reasoning_agent.name,\n",
    "            \"instructions\": dedent(f\"\"\"\n",
    "                Engage {reasoning_agent.name} when the scenario is complex or ambiguous and the supervisor agent cannot determine the appropriate call.\n",
    "                For testing purposes, always consult the reasoning agent in uncertain situations.\n",
    "                \n",
    "                Responsibilities:\n",
    "                1. Analyze the provided scenario and identify its key elements.\n",
    "                2. Break down the problem into manageable parts.\n",
    "                3. Use logical reasoning to derive well-reasoned conclusions.\n",
    "                4. Explain your thought process in a <reasoning>...</reasoning> block and list any assumptions in an <assumptions>...</assumptions> block.\n",
    "            \"\"\"),\n",
    "            \"relay_conversation_history\": \"DISABLED\"\n",
    "        }\n",
    "    ],\n",
    "    instructions=dedent(f\"\"\"\n",
    "        High-Level Overview:\n",
    "        Route user queries to the appropriate agent based on the type of answer required:\n",
    "          - Structured Data Retrieval: If the query requires retrieving structured information from the e-commerce shipping data, route it to {dsl_query_agent.name}. \n",
    "            If the DSL query returns errors or zero hits, immediately route the query to {query_fixer_agent.name} for reattempts.\n",
    "          - Document Content Analysis: If the query requires synthesizing and analyzing document content from executed queries, route it to {kb_rag_agent.name}.\n",
    "          - Complex or Ambiguous Scenarios: For any scenario that is complex, ambiguous, or when the supervisor agent is uncertain, always consult {reasoning_agent.name} for advice.\n",
    "        \n",
    "        Detailed Instructions:\n",
    "        \n",
    "        Route A: Structured Data Retrieval (DSL Query Agent + Query Fixer Agent)\n",
    "        1. Initial Query Analysis:\n",
    "           - Receive the user's natural language query.\n",
    "           - Determine if the query requires structured data retrieval from the e-commerce shipping data.\n",
    "           - Validate the query against the provided schema:\n",
    "             {ecom_shipping_schema_string}\n",
    "           - If the query qualifies, route it to {dsl_query_agent.name}.\n",
    "           - For testing purposes, if there is any uncertainty in the query's intent or complexity, consult {reasoning_agent.name} for advice.\n",
    "        \n",
    "        2. DSL Query Execution:\n",
    "           - {dsl_query_agent.name} generates a Query DSL encapsulated in <json>...</json> tags that follows the provided schema.\n",
    "           - For testing purposes, if the query appears complex, seek advice from {reasoning_agent.name}.\n",
    "        \n",
    "        3. Error Handling & Retry:\n",
    "           - Monitor the query execution results:\n",
    "             a. If the DSL query returns syntax or validation errors, or if the result is zero hits, capture the error context.\n",
    "             b. Immediately route the query, along with diagnostic details, to {query_fixer_agent.name}.\n",
    "             c. {query_fixer_agent.name} applies targeted fixes and query relaxation techniques, then returns a modified DSL query.\n",
    "             d. Validate the modified query; allow up to 3 retry attempts if necessary.\n",
    "             e. At any point, if the error context is complex, consult {reasoning_agent.name} for further advice.\n",
    "        \n",
    "        4. Evaluation & Final Approval (for structured data queries):\n",
    "           - Confirm that the final DSL query adheres to best practices (e.g., proper nested queries, correct field types and mappings).\n",
    "           - Maintain an audit trail of all query versions and modifications.\n",
    "           - Generate an execution summary including:\n",
    "             - Query versions attempted.\n",
    "             - Reasons for modifications.\n",
    "             - Performance metrics.\n",
    "           - For testing purposes, if any ambiguity arises during evaluation, consult {reasoning_agent.name}.\n",
    "        \n",
    "        General Aggregation Guidance:\n",
    "           - If an aggregation returns an unexpectedly inflated count, verify whether it is counting nested or repeated values.\n",
    "           - To accurately count unique items, use a cardinality aggregation on a unique identifier rather than aggregating on fields that may contain duplicate entries.\n",
    "           - For testing purposes, if aggregation results are ambiguous, always seek advice from {reasoning_agent.name}.\n",
    "        \n",
    "        Route B: Document Content Analysis (KB Response Agent)\n",
    "        1. Initial Query Analysis:\n",
    "           - Receive the user's natural language query.\n",
    "           - Determine if the query requires synthesizing and analyzing document content from executed queries.\n",
    "           - If so, route the query to {kb_rag_agent.name}.\n",
    "           - For testing purposes, if the scenario appears complex, consult {reasoning_agent.name} for advice.\n",
    "        \n",
    "        2. KB Response Generation:\n",
    "           - {kb_rag_agent.name} synthesizes and validates the information from the provided passages.\n",
    "           - Generate a final response that includes:\n",
    "             a. A direct answer.\n",
    "             b. Supporting evidence with relevant quotes and citations.\n",
    "             c. A confidence level.\n",
    "           - For testing purposes, if the response strategy is unclear, consult {reasoning_agent.name}.\n",
    "        \n",
    "        3. Final Response:\n",
    "           - Deliver the final, well-structured answer to the user by addressing the user by name.\n",
    "           - For testing purposes, if any final ambiguity remains, seek additional advice from {reasoning_agent.name} before responding.\n",
    "        \n",
    "        Route C: Complex or Ambiguous Scenarios (Reasoning Agent)\n",
    "        1. When a scenario is identified as complex or ambiguous—i.e., the supervisor agent cannot determine the appropriate call—route the query to {reasoning_agent.name}.\n",
    "        2. {reasoning_agent.name} will analyze the scenario, break it down into manageable parts, and provide a well-reasoned explanation.\n",
    "        3. The output must include the reasoning in a <reasoning>...</reasoning> block and any assumptions in an <assumptions>...</assumptions> block.\n",
    "    \"\"\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect attributes of the SupervisorAgent object\n",
    "# print(dir(supervisor_agent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:15:05,739 - __main__ - INFO - Retrieving Supervisor Agent ID...\n",
      "2025-02-09 16:15:05,821 - __main__ - INFO - Supervisor Agent ID: MZAK8ELKGV\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the Supervisor Agent ID\n",
    "logger.info(\"Retrieving Supervisor Agent ID...\")\n",
    "supervisor_agent_id = agents_helper.get_agent_id_by_name(supervisor_agent.name)\n",
    "logger.info(f\"Supervisor Agent ID: {supervisor_agent_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the agent based on the ID\n",
    "# supervisor_agentV2 = agents_helper.get_agent_by_id(\"KGTOVCVLKI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query = \"What are the priority orders with shipping cost greater than $30?\"\n",
    "# user_query = \"What is the average delivery time for DHL Express shipments?\"\n",
    "user_query = \"temperature-controlled packages delivered within last week, what is the average cost and the economic impact of late deliveries?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invokeAgent API request ID: ab212f98-44e7-4f80-b417-9f8591467f38\n",
      "invokeAgent API session ID: 09b8bbd402b74a6b962550b63bc8a78f\n",
      "  agent id: MZAK8ELKGV, agent alias id: WECCGNIT8G\n",
      "\u001b[32m---- Step 1 ----\u001b[0m\n",
      "\u001b[33mTook 8.8s, using 3703 tokens (in: 3401, out: 302) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mThis query requires multiple aspects to be analyzed:\n",
      "1. First, we need to get data about temperature-controlled packages from the last week\n",
      "2. We need to calculate average shipping costs\n",
      "3. We need to identify late deliveries and their economic impact\n",
      "\n",
      "Let me break this down into steps and consult multiple agents simultaneously to handle this complex query efficiently.\n",
      "\n",
      "1. I'll ask the DSL query agent to create a query for temperature-controlled packages from the last week\n",
      "2. I'll also ask the reasoning agent about how to best approach the economic impact analysis\n",
      "3. Once we have the data, we'll need the KB response agent to synthesize the findings\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'dsl-query-agent-us-west-2-533 [HAPHJFTMAB/YB1XUC0ZRR]' passing input text:\u001b[0m\n",
      "\u001b[35mCreate a DSL query to find temperature-controlled packages delivered within the last week. We need:\n",
      "1. Filter for packages that mention temperature control in the description\n",
      "2. Time range: last 7 days\n",
      "3. Include delivery status, shipping cost, package value\n",
      "4. Include estimated and actual delivery \n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.1 [using sub-agent name:dsl-query-agent-us-west-2-533, id:HAPHJFTMAB/YB1XUC0ZRR] ----\u001b[0m\n",
      "\u001b[33mTook 15.1s, using 2327 tokens (in: 1538, out: 789) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mTo create a DSL query for this request, I need to consider several aspects of the given schema and requirements. Let's break it down:\n",
      "\n",
      "1. We need to search for \"temperature control\" in the package description.\n",
      "2. We need to filter for shipments created within the last 7 days.\n",
      "3. We need to include delivery status, shipping cost, and package value in the results.\n",
      "4. We need to include estimated and actual delivery dates for late delivery analysis.\n",
      "\n",
      "I'll construct a query that addresses all these points using the appropriate fields from the schema.\n",
      "</thinking>\n",
      "\n",
      "<json>\n",
      "{\n",
      "  \"query\": {\n",
      "    \"bool\": {\n",
      "      \"must\": [\n",
      "        {\n",
      "          \"match\": {\n",
      "            \"package.description\": \"temperature control\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"range\": {\n",
      "            \"created_at\": {\n",
      "              \"gte\": \"now-7d/d\",\n",
      "              \"lte\": \"now/d\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"_source\": [\n",
      "    \"status\",\n",
      "    \"delivery.shipping_cost\",\n",
      "    \"package.value\",\n",
      "    \"delivery.estimated_date\",\n",
      "    \"tracking\"\n",
      "  ],\n",
      "  \"sort\": [\n",
      "    {\n",
      "      \"created_at\": {\n",
      "        \"order\": \"desc\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "</json>\n",
      "\n",
      "<thinking>\n",
      "Now that I have created the DSL query, I'll explain the key components:\n",
      "\n",
      "1. The \"must\" clause in the bool query ensures both conditions (description match and date range) are met.\n",
      "2. We use a \"match\" query for the package description to find \"temperature control\".\n",
      "3. The date range query filters for the last 7 days using date math.\n",
      "4. The \"_source\" field specifies which fields to return in the results, including status, shipping cost, package value, estimated delivery date, and the entire tracking object (which contains actual delivery information).\n",
      "5. We sort the results by creation date in descending order to see the most recent shipments first.\n",
      "\n",
      "This query should provide all the necessary information for analyzing temperature-controlled packages delivered within the last week, including their delivery status, costs, and timing information for late delivery analysis.\n",
      "\n",
      "Let's execute this query to get the results.\u001b[0m\n",
      "\u001b[35mUsing tool: execute_dsl_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'dsl_query', 'type': 'string', 'value': '{\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"match\": {\\n            \"package.description\": \"temperature control\"\\n          }\\n        },\\n        {\\n          \"range\": {\\n            \"created_at\": {\\n              \"gte\": \"now-7d/d\",\\n              \"lte\": \"now/d\"\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  },\\n  \"_source\": [\\n    \"status\",\\n    \"delivery.shipping_cost\",\\n    \"package.value\",\\n    \"delivery.estimated_date\",\\n    \"tracking\"\\n  ],\\n  \"sort\": [\\n    {\\n      \"created_at\": {\\n        \"order\": \"desc\"\\n      }\\n    }\\n  ]\\n}'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': {'took': 15, 'timed_out': False, '_shards': {'total': 0, 'successful': 0, 'skipped': 1, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}}}}...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.2 [using sub-agent name:dsl-query-agent-us-west-2-533, id:HAPHJFTMAB/YB1XUC0ZRR] ----\u001b[0m\n",
      "\u001b[33mTook 8.5s, using 2693 tokens (in: 2422, out: 271) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI have executed the DSL query, and now I'll analyze the results to provide an answer to the user's request.\n",
      "\n",
      "The query execution was successful, but it returned no results. This could mean that:\n",
      "1. There are no packages mentioning \"temperature control\" in their description within the last 7 days, or\n",
      "2. The data might not be present in the index for the specified time range.\n",
      "\n",
      "Given this information, I'll provide a response to the user.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "Based on the query results, there are no temperature-controlled packages delivered within the last week that match your criteria. This could mean that:\n",
      "\n",
      "1. No temperature-controlled packages were shipped in the last 7 days.\n",
      "2. If any were shipped, they might not have \"temperature control\" mentioned ...\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent dsl-query-agent-us-west-2-533 output text:\n",
      "Based on the query results, there are no temperature-controlled packages delivered within the last week that match your criteria. This could mean that:\n",
      "\n",
      "1. No temperature-controlled packages were shipped in the last 7 days.\n",
      "2. If any were shipped, they might not have \"temperature control\" mentioned ...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.3 [using sub-agent name:dsl-query-agent-us-west-2-533, id:HAPHJFTMAB/YB1XUC0ZRR] ----\u001b[0m\n",
      "\u001b[33mTook 5.7s, using 4085 tokens (in: 3867, out: 218) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mSince we got no results with the initial query, I should:\n",
      "1. Consult the query-fixer agent to try alternative approaches\n",
      "2. Ask the reasoning agent about the best way to modify our search strategy\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'query-fixer-agent-us-west-2-533 [Y4W1ICA55I/XIQPAGMFD5]' passing input text:\u001b[0m\n",
      "\u001b[35mThe initial query for temperature-controlled packages in the last week returned no results. Please suggest modifications to:\n",
      "1. Include alternative terms for temperature-controlled packages (e.g., \"cold chain\", \"refrigerated\", \"frozen\", \"climate controlled\")\n",
      "2. Extend the time range to last 30 days\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.4 [using sub-agent name:query-fixer-agent-us-west-2-533, id:Y4W1ICA55I/XIQPAGMFD5] ----\u001b[0m\n",
      "\u001b[33mTook 12.2s, using 2363 tokens (in: 1738, out: 625) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI understand that we need to modify a query that initially returned no results for temperature-controlled packages in the last week. We need to make several adjustments to improve the recall of the query. Let's analyze the requirements and create a modified query accordingly.\n",
      "\n",
      "1. We need to include alternative terms for temperature-controlled packages.\n",
      "2. We should extend the time range from 1 week to 30 days.\n",
      "3. We need to implement fuzzy matching for the package description field.\n",
      "\n",
      "Based on these requirements, I'll modify the query to address all these points. I'll use the \"actions_query-fixer-agent-us-west-2-533__retry_query\" function to submit the modified query.\u001b[0m\n",
      "\u001b[35mUsing tool: retry_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'modified_dsl_query', 'type': 'string', 'value': '{\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"range\": {\\n            \"created_at\": {\\n              \"gte\": \"now-30d/d\",\\n              \"lte\": \"now/d\"\\n            }\\n          }\\n        },\\n        {\\n          \"multi_match\": {\\n            \"query\": \"temperature-controlled cold chain refrigerated frozen climate controlled\",\\n            \"fields\": [\"package.description^2\", \"delivery.carrier\"],\\n            \"type\": \"best_fields\",\\n            \"fuzziness\": \"AUTO\"\\n          }\\n        }\\n      ]\\n    }\\n  },\\n  \"aggs\": {\\n    \"package_types\": {\\n      \"terms\": {\\n        \"field\": \"package.description\",\\n        \"size\": 10\\n      }\\n    },\\n    \"carriers\": {\\n      \"terms\": {\\n        \"field\": \"delivery.carrier\",\\n        \"size\": 10\\n      }\\n    }\\n  }\\n}'}, {'name': 'revision_notes', 'type': 'string', 'value': '1. Extended date range from 1 week to 30 days using \"now-30d/d\" in the range query.\\n2. Added alternative terms for temperature-controlled packages in the multi_match query.\\n3. Implemented fuzzy matching by setting \"fuzziness\": \"AUTO\" in the multi_match query.\\n4. Added a multi_match query to search in both package.description and delivery.carrier fields.\\n5. Boosted the package.description field (^2) to give it more weight in the search.\\n6. Added aggregations to get the most common package descriptions and carriers.'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': '{\\n  \"error_type\": \"RequestError\",\\n  \"error_message\": \"RequestError(400, \\'search_phase_execution_exception\\', \\'Text fields are not optimised for operations that require per-document field data like aggregations and sorting, so these operations are disabled by default. Please us...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.5 [using sub-agent name:query-fixer-agent-us-west-2-533, id:Y4W1ICA55I/XIQPAGMFD5] ----\u001b[0m\n",
      "\u001b[33mTook 12.0s, using 3455 tokens (in: 2992, out: 463) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mIt seems that our modified query encountered an error. The error message indicates that we cannot perform aggregations on the \"package.description\" field because it is a text field, which is not optimized for such operations. To resolve this issue, we need to make some adjustments to our query. Let's modify it again to address this problem and improve our search.\u001b[0m\n",
      "\u001b[35mUsing tool: retry_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'modified_dsl_query', 'type': 'string', 'value': '{\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"range\": {\\n            \"created_at\": {\\n              \"gte\": \"now-30d/d\",\\n              \"lte\": \"now/d\"\\n            }\\n          }\\n        },\\n        {\\n          \"multi_match\": {\\n            \"query\": \"temperature-controlled cold chain refrigerated frozen climate controlled\",\\n            \"fields\": [\"package.description^2\", \"delivery.carrier\"],\\n            \"type\": \"best_fields\",\\n            \"fuzziness\": \"AUTO\"\\n          }\\n        }\\n      ]\\n    }\\n  },\\n  \"aggs\": {\\n    \"carriers\": {\\n      \"terms\": {\\n        \"field\": \"delivery.carrier\",\\n        \"size\": 10\\n      }\\n    }\\n  }\\n}'}, {'name': 'revision_notes', 'type': 'string', 'value': \"1. Removed the aggregation on package.description field due to it being a text field not optimized for aggregations.\\n2. Kept the aggregation on delivery.carrier as it's a keyword field.\\n3. Maintained the extended date range, alternative terms, and fuzzy matching from the previous query.\\n4. The multi_match query still searches in both package.description and delivery.carrier fields with boosting on package.description.\"}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': '{\\n  \"hits\": 0,\\n  \"aggregations\": {\\n    \"carriers\": {\\n      \"doc_count_error_upper_bound\": 0,\\n      \"sum_other_doc_count\": 0,\\n      \"buckets\": []\\n    }\\n  },\\n  \"revision_notes\": \"1. Removed the aggregation on package.description field due to it being a text field not optimi...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.6 [using sub-agent name:query-fixer-agent-us-west-2-533, id:Y4W1ICA55I/XIQPAGMFD5] ----\u001b[0m\n",
      "\u001b[33mTook 12.3s, using 4238 tokens (in: 3664, out: 574) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mThe query has been successfully executed, but it still returned no results. This suggests that there might not be any documents in the index that match our criteria, or our search terms might be too specific. Let's try to further relax our query to see if we can find any relevant results.\u001b[0m\n",
      "\u001b[35mUsing tool: retry_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'modified_dsl_query', 'type': 'string', 'value': '{\\n  \"query\": {\\n    \"bool\": {\\n      \"should\": [\\n        {\\n          \"range\": {\\n            \"created_at\": {\\n              \"gte\": \"now-90d/d\",\\n              \"lte\": \"now/d\"\\n            }\\n          }\\n        },\\n        {\\n          \"multi_match\": {\\n            \"query\": \"temperature controlled cold chain refrigerated frozen climate controlled cool\",\\n            \"fields\": [\"package.description^2\", \"delivery.carrier\", \"package.weight\"],\\n            \"type\": \"best_fields\",\\n            \"fuzziness\": \"AUTO\",\\n            \"minimum_should_match\": \"30%\"\\n          }\\n        }\\n      ],\\n      \"minimum_should_match\": 1\\n    }\\n  },\\n  \"aggs\": {\\n    \"carriers\": {\\n      \"terms\": {\\n        \"field\": \"delivery.carrier\",\\n        \"size\": 10\\n      }\\n    },\\n    \"date_histogram\": {\\n      \"date_histogram\": {\\n        \"field\": \"created_at\",\\n        \"calendar_interval\": \"week\"\\n      }\\n    }\\n  }\\n}'}, {'name': 'revision_notes', 'type': 'string', 'value': '1. Changed the bool query to use \"should\" instead of \"must\" to make it more lenient.\\n2. Extended the date range to 90 days to capture a wider timeframe.\\n3. Added \"cool\" to the search terms and included \"package.weight\" in the fields to search, as temperature-controlled packages might have specific weight requirements.\\n4. Set \"minimum_should_match\": \"30%\" in the multi_match query to allow for partial matches.\\n5. Added a date histogram aggregation to see the distribution of matching documents over time.\\n6. Set \"minimum_should_match\": 1 for the main bool query to ensure at least one condition is met.'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': '{\\n  \"error_type\": \"RequestError\",\\n  \"error_message\": \"RequestError(400, \\'search_phase_execution_exception\\', \\'failed to create query: Can only use fuzzy queries on keyword and text fields - not on [package.weight] which is of type [float]\\')\",\\n  \"modified_query\": \"{\\\\n  \\\\\"qu...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.7 [using sub-agent name:query-fixer-agent-us-west-2-533, id:Y4W1ICA55I/XIQPAGMFD5] ----\u001b[0m\n",
      "\u001b[33mTook 14.1s, using 5594 tokens (in: 4905, out: 689) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI apologize for the oversight. The error message indicates that we can't use fuzzy matching on the \"package.weight\" field because it's a float type. Let's remove this field from the multi_match query and make a few more adjustments to improve our search.\u001b[0m\n",
      "\u001b[35mUsing tool: retry_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'modified_dsl_query', 'type': 'string', 'value': '{\\n  \"query\": {\\n    \"bool\": {\\n      \"should\": [\\n        {\\n          \"range\": {\\n            \"created_at\": {\\n              \"gte\": \"now-90d/d\",\\n              \"lte\": \"now/d\"\\n            }\\n          }\\n        },\\n        {\\n          \"multi_match\": {\\n            \"query\": \"temperature controlled cold chain refrigerated frozen climate controlled cool\",\\n            \"fields\": [\"package.description^2\", \"delivery.carrier\"],\\n            \"type\": \"best_fields\",\\n            \"fuzziness\": \"AUTO\",\\n            \"minimum_should_match\": \"30%\"\\n          }\\n        },\\n        {\\n          \"range\": {\\n            \"package.weight\": {\\n              \"gte\": 0,\\n              \"lte\": 100\\n            }\\n          }\\n        }\\n      ],\\n      \"minimum_should_match\": 1\\n    }\\n  },\\n  \"aggs\": {\\n    \"carriers\": {\\n      \"terms\": {\\n        \"field\": \"delivery.carrier\",\\n        \"size\": 10\\n      }\\n    },\\n    \"date_histogram\": {\\n      \"date_histogram\": {\\n        \"field\": \"created_at\",\\n        \"calendar_interval\": \"week\"\\n      }\\n    },\\n    \"weight_ranges\": {\\n      \"range\": {\\n        \"field\": \"package.weight\",\\n        \"ranges\": [\\n          { \"to\": 1 },\\n          { \"from\": 1, \"to\": 5 },\\n          { \"from\": 5, \"to\": 10 },\\n          { \"from\": 10 }\\n        ]\\n      }\\n    }\\n  }\\n}'}, {'name': 'revision_notes', 'type': 'string', 'value': '1. Removed \"package.weight\" from the multi_match query fields.\\n2. Added a separate range query for \"package.weight\" to include packages between 0 and 100 units (assuming kg or lbs).\\n3. Kept the extended date range of 90 days and the relaxed bool query with \"should\" clauses.\\n4. Maintained the multi_match query with fuzzy matching and partial matching for temperature-controlled related terms.\\n5. Added a \"weight_ranges\" aggregation to see the distribution of package weights.\\n6. Kept the date histogram and carriers aggregations from the previous query.'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': '{\\n  \"hits\": 5,\\n  \"aggregations\": {\\n    \"date_histogram\": {\\n      \"buckets\": [\\n        {\\n          \"key_as_string\": \"2024-01-08T00:00:00.000Z\",\\n          \"key\": 1704672000000,\\n          \"doc_count\": 1\\n        },\\n        {\\n          \"key_as_string\": \"2024-01-15T00:00:00.0...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.8 [using sub-agent name:query-fixer-agent-us-west-2-533, id:Y4W1ICA55I/XIQPAGMFD5] ----\u001b[0m\n",
      "\u001b[33mTook 11.4s, using 6757 tokens (in: 6362, out: 395) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mGreat! We've successfully retrieved some results with our modified query. Let's analyze the results and provide a summary to the user.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "The modified query for temperature-controlled packages has returned results. Here's a summary of the findings:\n",
      "\n",
      "1. We found 5 shipments that potentially match the criteria for temperature-controlled packages.\n",
      "\n",
      "2. Time range: The shipments were created in the past 90 days, specifically in January 202...\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent query-fixer-agent-us-west-2-533 output text:\n",
      "The modified query for temperature-controlled packages has returned results. Here's a summary of the findings:\n",
      "\n",
      "1. We found 5 shipments that potentially match the criteria for temperature-controlled packages.\n",
      "\n",
      "2. Time range: The shipments were created in the past 90 days, specifically in January 202...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.9 [using sub-agent name:query-fixer-agent-us-west-2-533, id:Y4W1ICA55I/XIQPAGMFD5] ----\u001b[0m\n",
      "\u001b[33mTook 4.8s, using 4627 tokens (in: 4450, out: 177) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'reasoning-agent-us-west-2-533 [QEZS20IZCK/KISEPBZICH]' passing input text:\u001b[0m\n",
      "\u001b[35mWe've identified 5 potential temperature-controlled shipments. Please help analyze:\n",
      "1. How should we calculate the economic impact of late deliveries? Consider:\n",
      "   - Package value\n",
      "   - Shipping costs\n",
      "   - Potential spoilage for temperature-sensitive items\n",
      "   - Impact on customer satisfaction\n",
      "2. What\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.10 [using sub-agent name:reasoning-agent-us-west-2-533, id:QEZS20IZCK/KISEPBZICH] ----\u001b[0m\n",
      "\u001b[33mTook 4.6s, using 1017 tokens (in: 818, out: 199) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mTo analyze this complex scenario involving temperature-controlled shipments and their potential economic impact, I'll need to break down the problem into smaller parts and use logical reasoning to provide a comprehensive answer. I'll use the reasoning agent to help structure our analysis and provide insights.\u001b[0m\n",
      "\u001b[35mUsing tool: execute_reasoning with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'scenario', 'type': 'string', 'value': 'We need to analyze the economic impact of late deliveries for 5 potential temperature-controlled shipments. We need to consider factors such as package value, shipping costs, potential spoilage for temperature-sensitive items, and impact on customer satisfaction. We also need to determine appropriate metrics for analysis and how to present this information in an actionable manner.'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': {'generation': \"We need to analyze the economic impact of late deliveries for 5 potential temperature-controlled shipments. We need to consider factors such as package value, shipping costs, potential spoilage for temperature-sensitive items, and impact on customer satisfaction. We...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.11 [using sub-agent name:reasoning-agent-us-west-2-533, id:QEZS20IZCK/KISEPBZICH] ----\u001b[0m\n",
      "\u001b[33mTook 24.5s, using 5270 tokens (in: 4951, out: 319) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mThe reasoning agent has provided a comprehensive analysis of the scenario. I'll now synthesize this information to answer the user's specific questions.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "1. To calculate the economic impact of late deliveries, consider the following approach:\n",
      "   - Define a threshold for late delivery (e.g., 24 hours past scheduled time)\n",
      "   - Calculate total costs: (Shipping costs + Additional costs due to delay) / Package value\n",
      "   - Estimate spoilage cost: (Spoilage ...\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent reasoning-agent-us-west-2-533 output text:\n",
      "1. To calculate the economic impact of late deliveries, consider the following approach:\n",
      "   - Define a threshold for late delivery (e.g., 24 hours past scheduled time)\n",
      "   - Calculate total costs: (Shipping costs + Additional costs due to delay) / Package value\n",
      "   - Estimate spoilage cost: (Spoilage ...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.12 [using sub-agent name:reasoning-agent-us-west-2-533, id:QEZS20IZCK/KISEPBZICH] ----\u001b[0m\n",
      "\u001b[33mTook 5.0s, using 5147 tokens (in: 4915, out: 232) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mNow that we have both the data and the analytical framework, let's get a comprehensive analysis from the KB response agent.\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'kb-response-agent-us-west-2-533 [NMYVAUXBYQ/BTMK53A5UM]' passing input text:\u001b[0m\n",
      "\u001b[35mPlease analyze the following data about temperature-controlled shipments:\n",
      "- 5 shipments identified in January 2024\n",
      "- Distributed across DHL Express, FedEx Priority, Local Express, UPS, and USPS\n",
      "- Package weights: 2 under 1 unit, 3 between 1-5 units\n",
      "- Need analysis of:\n",
      "  1. Average shipping costs\n",
      "  2\n",
      "\u001b[0m\n",
      "\u001b[35mUsing knowledge base id: 5GADU65GNF to search for:\u001b[0m\n",
      "\u001b[35m  Please analyze the following data about temperature-controlled shipments:\n",
      "- 5 shipments identified in January 2024\n",
      "- Distributed across DHL Express, FedEx Priority, Local Express, UPS, and USPS\n",
      "- Package weights: 2 under 1 unit, 3 between 1-5 units\n",
      "- Need analysis of:\n",
      "  1. Average shipping costs\n",
      "  2. Economic impact of late deliveries\n",
      "  3. Patterns or trends in delivery performance\n",
      "  4. Recommendations for improvement\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.13 [using sub-agent name:kb-response-agent-us-west-2-533, id:NMYVAUXBYQ/BTMK53A5UM] ----\u001b[0m\n",
      "\u001b[33mTook 6.8s, using 4589 tokens (in: 4368, out: 221) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "The search results do not contain information about the specific temperature-controlled shipments you described from January 2024. The report focuses on broader e-commerce and shipping trends from 2021-2022, but does not include data on temperature-controlled shipments or the carriers you mentioned ...\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent kb-response-agent-us-west-2-533 output text:\n",
      "The search results do not contain information about the specific temperature-controlled shipments you described from January 2024. The report focuses on broader e-commerce and shipping trends from 2021-2022, but does not include data on temperature-controlled shipments or the carriers you mentioned ...\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 16:36:56,562 - __main__ - INFO - Supervisor agent response: I apologize, but I'm unable to provide a complete analysis of temperature-controlled packages and their economic impact due to limited data availability. Here's what we found:\n",
      "\n",
      "1. We identified 5 potential temperature-sensitive shipments from January 2024, but we need additional data to confirm their nature and status.\n",
      "\n",
      "2. To provide you with accurate information about costs and economic impact, we would need:\n",
      "   - Confirmation of which packages are actually temperature-controlled\n",
      "   - Specific delivery status (on-time vs. delayed)\n",
      "   - Actual shipping costs and package values\n",
      "   - Temperature monitoring data if available\n",
      "\n",
      "Would you like me to:\n",
      "1. Modify the search criteria to look for specific temperature-control keywords?\n",
      "2. Extend the time range to gather more historical data?\n",
      "3. Focus on specific carriers or shipping methods?\n",
      "\n",
      "Please let me know how you'd like to proceed with the analysis.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m---- Step 1.14 [using sub-agent name:kb-response-agent-us-west-2-533, id:NMYVAUXBYQ/BTMK53A5UM] ----\u001b[0m\n",
      "\u001b[33mTook 8.3s, using 5542 tokens (in: 5249, out: 293) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI need to inform the user about the limitations in our data and provide a path forward.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "I apologize, but I'm unable to provide a complete analysis of temperature-controlled packages and their economic impact due to limited data availability. Here's what we found:\n",
      "\n",
      "1. We identified 5 potential temperature-sensitive shipments from January 2024, but we need additional data to confirm thei...\u001b[0m\n",
      "\u001b[33mAgent made a total of 15 LLM calls, using 61407 tokens (in: 55640, out: 5767), and took 154.4 total seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "response = supervisor_agent.invoke(\n",
    "    input_text=user_query,\n",
    "    session_id=uuid.uuid4().hex,\n",
    "    enable_trace=True,\n",
    "    trace_level=\"core\",\n",
    "    session_state={\n",
    "        \"sessionAttributes\": {\n",
    "            \"currentTimestamp\": \"2022-01-01T12:00:00Z\"\n",
    "        },\n",
    "        \"promptSessionAttributes\": {\n",
    "            \"userName\": \"Alice Mallory\",\n",
    "            \"jobTitle\": \"Data Analyst\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "logger.info(f\"Supervisor agent response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------------------------------------------------------\n",
    "# # 7. Invoke the Supervisor Agent with a sample query\n",
    "# # -------------------------------------------------------------------------\n",
    "# response = supervisor_agent.invoke(\n",
    "#     input_text=\"How many orders have been shipped by DHL?\",\n",
    "#     session_id=\"12345\",\n",
    "#     enable_trace=True,\n",
    "#     trace_level=\"core\"\n",
    "# )\n",
    "# logger.info(f\"Supervisor agent response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = supervisor_agent.invoke(\n",
    "#     input_text=\"What are the effects of Covid-19 on e-commerce?\",\n",
    "#     session_id=\"1245\",\n",
    "#     enable_trace=True,\n",
    "#     trace_level=\"core\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = supervisor_agent.invoke(\n",
    "#     input_text=\"How many orders have recipients in Spain and were last updated during customs clearance after January 16, 2024?\",\n",
    "#     session_id=\"1245\",\n",
    "#     enable_trace=True,\n",
    "#     trace_level=\"core\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------------------------------------------------------\n",
    "# # 8. Cleanup: Delete the created agents\n",
    "# # -------------------------------------------------------------------------\n",
    "# logger.info(\"Deleting Supervisor Agent...\")\n",
    "# agents_helper.delete_agent(supervisor_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting DSL Query Agent...\")\n",
    "# agents_helper.delete_agent(dsl_query_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting Query Fixer Agent...\")\n",
    "# agents_helper.delete_agent(query_fixer_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting KB Response Agent...\")\n",
    "# agents_helper.delete_agent(kb_rag_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting Reasoning Agent...\")\n",
    "# agents_helper.delete_agent(reasoning_agent.name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
