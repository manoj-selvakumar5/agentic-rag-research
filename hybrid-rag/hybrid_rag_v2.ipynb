{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Collaboration - DSL query use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import subprocess\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configure Logging\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Bedrock Agent utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.bedrock_agent import Agent, SupervisorAgent, agents_helper, region, account_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AWS clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:55:31,802 - __main__ - INFO - Region: us-west-2\n",
      "2025-02-07 18:55:31,802 - __main__ - INFO - Account ID: 533267284022\n",
      "2025-02-07 18:55:31,803 - __main__ - INFO - Agent Suffix: us-west-2-533\n"
     ]
    }
   ],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = session.region_name\n",
    "account_id_suffix = account_id[:3]\n",
    "agent_suffix = f\"{region}-{account_id_suffix}\"\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n",
    "iam_client = boto3.client('iam', region_name=region)\n",
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "\n",
    "logger.info(f\"Region: {region}\")\n",
    "logger.info(f\"Account ID: {account_id}\")\n",
    "logger.info(f\"Agent Suffix: {agent_suffix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iam_role(role_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates or retrieves an IAM Role with the necessary trust policy for Lambda.\n",
    "    Attaches AWSLambdaBasicExecutionRole, and adds inline policies for OpenSearch \n",
    "    and AOSS access.\n",
    "\n",
    "    :param role_name: Name of the IAM Role to create or retrieve.\n",
    "    :return: ARN of the created or retrieved IAM Role.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating or retrieving IAM Role: {role_name}\")\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        role = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    "        )\n",
    "        logger.info(f\"IAM Role {role_name} created.\")\n",
    "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "        logger.info(f\"IAM Role {role_name} already exists. Retrieving existing role.\")\n",
    "        role = iam_client.get_role(RoleName=role_name)\n",
    "\n",
    "    # Attach AWS Lambda execution policy\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "    )\n",
    "    logger.info(f\"Attached AWSLambdaBasicExecutionRole to {role_name}.\")\n",
    "\n",
    "    # Attach additional policies for OpenSearch access\n",
    "    opensearch_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"es:Describe*\",\n",
    "                    \"es:List*\",\n",
    "                    \"es:Get*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    opensearch_policy_name = f\"{role_name}-OpenSearchPolicy\"\n",
    "    try:\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=opensearch_policy_name,\n",
    "            PolicyDocument=json.dumps(opensearch_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Attached OpenSearch policy to IAM Role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to attach OpenSearch policy to IAM Role {role_name}: {str(e)}\")\n",
    "\n",
    "    # Attach the new policy for aoss:APICall\n",
    "    aoss_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"aoss:*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    aoss_policy_name = f\"{role_name}-AOSSPolicy\"\n",
    "    try:\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=aoss_policy_name,\n",
    "            PolicyDocument=json.dumps(aoss_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Attached AOSS policy to IAM Role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to attach AOSS policy to IAM Role {role_name}: {str(e)}\")\n",
    "\n",
    "    role_arn = role['Role']['Arn']\n",
    "\n",
    "    # Wait for IAM role to propagate\n",
    "    logger.info(\"Waiting 10 seconds for IAM role to propagate...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    return role_arn\n",
    "\n",
    "\n",
    "def create_lambda_package(source_file: str, zip_file_path: str, dependencies: list):\n",
    "    \"\"\"\n",
    "    Packages a Lambda function and its dependencies into a single ZIP file.\n",
    "\n",
    "    :param source_file: Path to the Lambda function source code.\n",
    "    :param zip_file_path: Path to the ZIP file that will be created.\n",
    "    :param dependencies: A list of Python packages required by the Lambda.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Packaging Lambda function from {source_file} with dependencies {dependencies}\")\n",
    "    package_dir = \"package\"\n",
    "\n",
    "    # Install dependencies to a local folder\n",
    "    if not os.path.exists(package_dir):\n",
    "        os.makedirs(package_dir)\n",
    "    logger.info(\"Installing dependencies locally...\")\n",
    "    subprocess.run(\n",
    "        f\"pip install {' '.join(dependencies)} -t {package_dir}\",\n",
    "        shell=True,\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    # Create ZIP file with dependencies and function\n",
    "    logger.info(f\"Creating Lambda deployment package: {zip_file_path}\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
    "        # Add dependencies\n",
    "        for root, _, files in os.walk(package_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, package_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add the Lambda function code\n",
    "        zipf.write(source_file, os.path.basename(source_file))\n",
    "\n",
    "    # Cleanup temporary package directory\n",
    "    logger.info(\"Cleaning up temporary package directory...\")\n",
    "    subprocess.run(f\"rm -rf {package_dir}\", shell=True)\n",
    "    logger.info(\"Lambda package created successfully.\")\n",
    "\n",
    "\n",
    "def create_lambda_function(function_name: str,\n",
    "                           role_arn: str,\n",
    "                           handler: str,\n",
    "                           runtime: str,\n",
    "                           zip_file_path: str,\n",
    "                           region_name: str = region) -> dict:\n",
    "    \"\"\"\n",
    "    Creates or updates an AWS Lambda function.\n",
    "\n",
    "    :param function_name: Name of the Lambda function to create or update.\n",
    "    :param role_arn: ARN of the IAM Role that Lambda will assume.\n",
    "    :param handler: The function handler (e.g., 'index.lambda_handler').\n",
    "    :param runtime: The Lambda runtime (e.g., 'python3.12').\n",
    "    :param zip_file_path: Path to the ZIP file containing the Lambda code.\n",
    "    :param region_name: AWS region where the Lambda will be created.\n",
    "    :return: The response from the create_function or update_function_code API call.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating/updating Lambda function: {function_name}\")\n",
    "    lambda_client = boto3.client('lambda', region_name=region_name)\n",
    "\n",
    "    with open(zip_file_path, 'rb') as f:\n",
    "        zip_content = f.read()\n",
    "\n",
    "    try:\n",
    "        response = lambda_client.create_function(\n",
    "            FunctionName=function_name,\n",
    "            Runtime=runtime,\n",
    "            Role=role_arn,\n",
    "            Handler=handler,\n",
    "            Code={'ZipFile': zip_content},\n",
    "            Description='Lambda function to execute DSL queries',\n",
    "            Timeout=15,\n",
    "            MemorySize=128,\n",
    "            Publish=True\n",
    "        )\n",
    "        logger.info(f\"Lambda function {function_name} created successfully.\")\n",
    "    except lambda_client.exceptions.ResourceConflictException:\n",
    "        logger.info(f\"Lambda function {function_name} already exists. Updating its code...\")\n",
    "        response = lambda_client.update_function_code(\n",
    "            FunctionName=function_name,\n",
    "            ZipFile=zip_content\n",
    "        )\n",
    "        logger.info(f\"Lambda function {function_name} updated successfully.\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def add_resource_based_policy(function_name: str,\n",
    "                              agent_ids: list,\n",
    "                              region_name: str,\n",
    "                              account_id: str):\n",
    "    \"\"\"\n",
    "    Adds a resource-based policy to the specified Lambda function to allow invocation\n",
    "    from one or more Bedrock agents.\n",
    "\n",
    "    :param function_name: Name of the Lambda function.\n",
    "    :param agent_ids: List of agent IDs permitted to invoke this Lambda.\n",
    "    :param region_name: AWS region.\n",
    "    :param account_id: AWS account ID.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Adding resource-based policy to Lambda function {function_name} for agents: {agent_ids}\")\n",
    "    statement_id_prefix = \"AllowExecutionFromBedrockAgent\"\n",
    "    policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": []\n",
    "    }\n",
    "\n",
    "    for agent_id in agent_ids:\n",
    "        sid = f\"{statement_id_prefix}_{agent_id}\"\n",
    "        policy_doc['Statement'].append({\n",
    "            \"Sid\": sid,\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"lambda:InvokeFunction\",\n",
    "            \"Resource\": f\"arn:aws:lambda:{region_name}:{account_id}:function:{function_name}\",\n",
    "            \"Condition\": {\n",
    "                \"ArnLike\": {\n",
    "                    \"AWS:SourceArn\": f\"arn:aws:bedrock:{region_name}:{account_id}:agent/{agent_id}\"\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Retrieve existing policy and remove any existing statements with the same prefix\n",
    "    try:\n",
    "        existing_policy = lambda_client.get_policy(FunctionName=function_name)\n",
    "        existing_policy_doc = json.loads(existing_policy['Policy'])\n",
    "        for stmt in existing_policy_doc['Statement']:\n",
    "            if stmt['Sid'].startswith(statement_id_prefix):\n",
    "                sid_to_remove = stmt['Sid']\n",
    "                logger.info(f\"Removing existing statement: {sid_to_remove}\")\n",
    "                lambda_client.remove_permission(\n",
    "                    FunctionName=function_name,\n",
    "                    StatementId=sid_to_remove\n",
    "                )\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"No existing policy found for Lambda function {function_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving/removing existing policy for {function_name}: {str(e)}\")\n",
    "\n",
    "    # Add new permissions\n",
    "    for stmt in policy_doc['Statement']:\n",
    "        sid_val = stmt['Sid']\n",
    "        try:\n",
    "            lambda_client.add_permission(\n",
    "                FunctionName=function_name,\n",
    "                StatementId=sid_val,\n",
    "                Action=stmt['Action'],\n",
    "                Principal=stmt['Principal']['Service'],\n",
    "                SourceArn=stmt['Condition']['ArnLike']['AWS:SourceArn']\n",
    "            )\n",
    "            logger.info(f\"Added permission for statement: {sid_val}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to add resource-based policy for {function_name}, statement {sid_val}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shipping Schema \n",
    "with open('schemas/ecom_shipping_schema.json', 'r') as file:\n",
    "    ecom_shipping_schema = json.load(file)\n",
    "ecom_shipping_schema_string = json.dumps(ecom_shipping_schema, indent=2)\n",
    "\n",
    "# Agent foundation model \n",
    "agent_foundation_model = [\n",
    "    \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "]\n",
    "\n",
    "# Force re-create default setting for Agent objects, but for now set to False\n",
    "Agent.set_force_recreate_default(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:55:31,829 - __main__ - INFO - Creating or retrieving IAM Role: LambdaExecutionRole-us-west-2-533\n",
      "2025-02-07 18:55:32,206 - __main__ - INFO - IAM Role LambdaExecutionRole-us-west-2-533 already exists. Retrieving existing role.\n",
      "2025-02-07 18:55:32,501 - __main__ - INFO - Attached AWSLambdaBasicExecutionRole to LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-07 18:55:32,656 - __main__ - INFO - Attached OpenSearch policy to IAM Role LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-07 18:55:32,810 - __main__ - INFO - Attached AOSS policy to IAM Role LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-07 18:55:32,811 - __main__ - INFO - Waiting 10 seconds for IAM role to propagate...\n",
      "2025-02-07 18:55:42,819 - __main__ - INFO - Packaging Lambda function from src/lambda/execute_dsl_query.py with dependencies ['opensearch-py', 'requests', 'urllib3']\n",
      "2025-02-07 18:55:42,821 - __main__ - INFO - Installing dependencies locally...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py\n",
      "  Using cached opensearch_py-2.8.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-dateutil (from opensearch-py)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2024.07.04 (from opensearch-py)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Events (from opensearch-py)\n",
      "  Using cached Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting six>=1.5 (from python-dateutil->opensearch-py)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached opensearch_py-2.8.0-py3-none-any.whl (353 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: Events, urllib3, six, idna, charset-normalizer, certifi, requests, python-dateutil, opensearch-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.16.1 requires botocore<1.35.89,>=1.35.74, but you have botocore 1.36.9 which is incompatible.\n",
      "datasets 2.21.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "awscli 1.34.8 requires botocore==1.35.8, but you have botocore 1.36.9 which is incompatible.\n",
      "awscli 1.34.8 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-02-07 18:55:43,919 - __main__ - INFO - Creating Lambda deployment package: dsl_query_function.zip\n",
      "2025-02-07 18:55:43,958 - __main__ - INFO - Cleaning up temporary package directory...\n",
      "2025-02-07 18:55:43,995 - __main__ - INFO - Lambda package created successfully.\n",
      "2025-02-07 18:55:43,996 - __main__ - INFO - Creating/updating Lambda function: execute-dsl-query-us-west-2-533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Events-0.5 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 opensearch-py-2.8.0 python-dateutil-2.9.0.post0 requests-2.32.3 six-1.17.0 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:55:46,299 - __main__ - INFO - Lambda function execute-dsl-query-us-west-2-533 already exists. Updating its code...\n",
      "2025-02-07 18:55:47,891 - __main__ - INFO - Lambda function execute-dsl-query-us-west-2-533 updated successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main execution flow:\n",
    "    1. Create an IAM Role for Lambda.\n",
    "    2. Create/Update two Lambda functions (execute-dsl-query, execute-modified-dsl-query).\n",
    "    3. Create DSL Query Agent & Query Fixer Agent referencing those Lambda functions.\n",
    "    4. Retrieve the newly created agent IDs.\n",
    "    5. Add resource-based policies to each Lambda function for those agent IDs.\n",
    "    6. Create the Supervisor Agent to orchestrate both DSL Query and Query Fixer agents.\n",
    "    7. Invoke the Supervisor Agent with a sample query.\n",
    "    8. Delete the agents (cleanup).\n",
    "\"\"\"\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Create (or retrieve) IAM Role for Lambda\n",
    "# -------------------------------------------------------------------------\n",
    "IAM_ROLE_NAME = f\"LambdaExecutionRole-{agent_suffix}\"\n",
    "role_arn = create_iam_role(IAM_ROLE_NAME)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Create the first Lambda (execute-dsl-query)\n",
    "# -------------------------------------------------------------------------\n",
    "DSL_QUERY_LAMBDA_NAME = f\"execute-dsl-query-{agent_suffix}\"\n",
    "DSL_QUERY_LAMBDA_PATH = \"src/lambda/execute_dsl_query.py\"\n",
    "DSL_QUERY_ZIP_PATH = \"dsl_query_function.zip\"\n",
    "\n",
    "if not os.path.exists(DSL_QUERY_LAMBDA_PATH):\n",
    "    logger.error(f\"Error: {DSL_QUERY_LAMBDA_PATH} does not exist.\")\n",
    "\n",
    "DEPENDENCIES = [\"opensearch-py\", \"requests\", \"urllib3\"]\n",
    "\n",
    "# Package & create the Lambda\n",
    "create_lambda_package(DSL_QUERY_LAMBDA_PATH, DSL_QUERY_ZIP_PATH, DEPENDENCIES)\n",
    "create_lambda_function(\n",
    "    function_name=DSL_QUERY_LAMBDA_NAME,\n",
    "    role_arn=role_arn,\n",
    "    handler=\"execute_dsl_query.lambda_handler\",\n",
    "    runtime=\"python3.12\",\n",
    "    zip_file_path=DSL_QUERY_ZIP_PATH\n",
    ")\n",
    "os.remove(DSL_QUERY_ZIP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:55:47,907 - __main__ - INFO - Packaging Lambda function from src/lambda/execute_modified_dsl_query.py with dependencies ['opensearch-py', 'requests', 'urllib3']\n",
      "2025-02-07 18:55:47,908 - __main__ - INFO - Installing dependencies locally...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py\n",
      "  Using cached opensearch_py-2.8.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-dateutil (from opensearch-py)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2024.07.04 (from opensearch-py)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Events (from opensearch-py)\n",
      "  Using cached Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting six>=1.5 (from python-dateutil->opensearch-py)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached opensearch_py-2.8.0-py3-none-any.whl (353 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: Events, urllib3, six, idna, charset-normalizer, certifi, requests, python-dateutil, opensearch-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.16.1 requires botocore<1.35.89,>=1.35.74, but you have botocore 1.36.9 which is incompatible.\n",
      "datasets 2.21.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "awscli 1.34.8 requires botocore==1.35.8, but you have botocore 1.36.9 which is incompatible.\n",
      "awscli 1.34.8 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-02-07 18:55:48,851 - __main__ - INFO - Creating Lambda deployment package: modified_query_function.zip\n",
      "2025-02-07 18:55:48,889 - __main__ - INFO - Cleaning up temporary package directory...\n",
      "2025-02-07 18:55:48,926 - __main__ - INFO - Lambda package created successfully.\n",
      "2025-02-07 18:55:48,927 - __main__ - INFO - Creating/updating Lambda function: execute-modified-dsl-query-us-west-2-533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Events-0.5 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 opensearch-py-2.8.0 python-dateutil-2.9.0.post0 requests-2.32.3 six-1.17.0 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:55:50,704 - __main__ - INFO - Lambda function execute-modified-dsl-query-us-west-2-533 already exists. Updating its code...\n",
      "2025-02-07 18:55:52,299 - __main__ - INFO - Lambda function execute-modified-dsl-query-us-west-2-533 updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2(b). Create the second Lambda (execute-modified-dsl-query)\n",
    "# -------------------------------------------------------------------------\n",
    "MODIFIED_QUERY_LAMBDA_NAME = f\"execute-modified-dsl-query-{agent_suffix}\"\n",
    "MODIFIED_QUERY_LAMBDA_PATH = \"src/lambda/execute_modified_dsl_query.py\"\n",
    "MODIFIED_QUERY_ZIP_PATH = \"modified_query_function.zip\"\n",
    "\n",
    "if not os.path.exists(MODIFIED_QUERY_LAMBDA_PATH):\n",
    "    logger.error(f\"Error: {MODIFIED_QUERY_LAMBDA_PATH} does not exist.\")\n",
    "\n",
    "create_lambda_package(MODIFIED_QUERY_LAMBDA_PATH, MODIFIED_QUERY_ZIP_PATH, DEPENDENCIES)\n",
    "create_lambda_function(\n",
    "    function_name=MODIFIED_QUERY_LAMBDA_NAME,\n",
    "    role_arn=role_arn,\n",
    "    handler=\"execute_modified_dsl_query.lambda_handler\",\n",
    "    runtime=\"python3.12\",\n",
    "    zip_file_path=MODIFIED_QUERY_ZIP_PATH\n",
    ")\n",
    "os.remove(MODIFIED_QUERY_ZIP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:55:52,321 - __main__ - INFO - Creating DSL Query Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting existing agent and corresponding lambda for: dsl-query-agent-us-west-2-533...\n",
      "Agent dsl-query-agent-us-west-2-533 not found\n",
      "Creating agent dsl-query-agent-us-west-2-533...\n",
      "Created agent, id: UZ0ZCHE67L, alias id: TSTALIASID\n",
      "\n",
      "Adding action group with Lambda: arn:aws:lambda:us-west-2:533267284022:function:execute-dsl-query-us-west-2-533...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id UZ0ZCHE67L current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id UZ0ZCHE67L current status: PREPARED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:56:20,595 - __main__ - INFO - Creating Query Fixer Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: Agent: dsl-query-agent-us-west-2-533, id: UZ0ZCHE67L, alias id: AGKDHUAHJJ\n",
      "\n",
      "\n",
      "Deleting existing agent and corresponding lambda for: query-fixer-agent-us-west-2-533...\n",
      "Agent query-fixer-agent-us-west-2-533 not found\n",
      "Creating agent query-fixer-agent-us-west-2-533...\n",
      "Created agent, id: BVZPDW8UGH, alias id: TSTALIASID\n",
      "\n",
      "Adding action group with Lambda: arn:aws:lambda:us-west-2:533267284022:function:execute-modified-dsl-query-us-west-2-533...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id BVZPDW8UGH current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id BVZPDW8UGH current status: PREPARED\n",
      "DONE: Agent: query-fixer-agent-us-west-2-533, id: BVZPDW8UGH, alias id: KCBBPUCEVM\n",
      "\n",
      "\n",
      "Deleting existing agent and corresponding lambda for: kb-response-agent-us-west-2-533...\n",
      "Agent kb-response-agent-us-west-2-533 not found\n",
      "Creating agent kb-response-agent-us-west-2-533...\n",
      "Creating agent: kb-response-agent-us-west-2-533...\n",
      "Created agent IAM role: arn:aws:iam::533267284022:role/DEFAULT_AgentExecutionRole...\n",
      "Creating agent: kb-response-agent-us-west-2-533 with model: us.anthropic.claude-3-5-sonnet-20240620-v1:0...\n",
      "kwargs: {}\n",
      "Created agent, resulting id: CRL3Q8ZW0K\n",
      "Created agent, id: CRL3Q8ZW0K, alias id: TSTALIASID\n",
      "\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id CRL3Q8ZW0K current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id CRL3Q8ZW0K current status: PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id CRL3Q8ZW0K current status: PREPARED\n",
      "DONE: Agent: kb-response-agent-us-west-2-533, id: CRL3Q8ZW0K, alias id: J6KX4IIFJQ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Create the DSL Query Agent & Query Fixer Agent\n",
    "#\n",
    "#    Important: reference the just-created Lambda ARNs in \"tool_code\"\n",
    "#    The actual ARN is \"arn:aws:lambda:<REGION>:<ACCOUNT>:function:<FUNCTION_NAME>\"\n",
    "# -------------------------------------------------------------------------\n",
    "dsl_query_lambda_arn = f\"arn:aws:lambda:{region}:{account_id}:function:{DSL_QUERY_LAMBDA_NAME}\"\n",
    "modified_query_lambda_arn = f\"arn:aws:lambda:{region}:{account_id}:function:{MODIFIED_QUERY_LAMBDA_NAME}\"\n",
    "\n",
    "logger.info(\"Creating DSL Query Agent...\")\n",
    "dsl_query_agent = Agent.direct_create(\n",
    "    name=f\"dsl-query-agent-{agent_suffix}\",\n",
    "    role=\"DSL Query Creator\",\n",
    "    goal=\"Create DSL queries for a given user query\",\n",
    "    instructions=f\"\"\"\n",
    "    You are an expert in generating Query DSL for Elasticsearch-style queries. Your task is to convert a \n",
    "    given natural language user question into a well-structured Query DSL.\n",
    "    \n",
    "    ## Instructions:\n",
    "    - Use the provided e-commerce shipping schema to construct the query.\n",
    "    - Encapsulate the output in <json>...</json> tags.\n",
    "    - Follow the syntax of the Query DSL strictly; do not introduce elements outside the provided schema.\n",
    "    \n",
    "    ## Query Construction Rules:\n",
    "    - **Keyword fields** (carrier, status, country): Use `term` for exact matches or `prefix`/`wildcard` for partial matches.\n",
    "    - **Text fields** (description, address): Use `match` queries to account for analyzed terms.\n",
    "    - **Nested fields** (tracking): Always use `nested` queries.\n",
    "    - **Date fields**: Use `range` queries with date math for filtering by date ranges.\n",
    "    - **Aggregations**: When counting occurrences, use a 'terms' aggregation on the relevant keyword field to capture the exact values present (e.g., 'delivery.carrier').\n",
    "    - Break down complex queries into smaller parts for accuracy.\n",
    "    - Think step-by-step before constructing the query.\n",
    "\n",
    "\n",
    "    ## Schema:\n",
    "    {ecom_shipping_schema_string}\n",
    "\n",
    "    ## Output Format:\n",
    "    - Return only the generated Query DSL within <json>...</json> tags.\n",
    "    - Do not include explanations, comments, or additional text.\n",
    "    \"\"\",\n",
    "    tool_code=dsl_query_lambda_arn,\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"execute_dsl_query\",\n",
    "            \"description\": \"Executes a given DSL query and returns the results\",\n",
    "            \"parameters\": {\n",
    "                \"dsl_query\": {\n",
    "                    \"description\": \"The DSL query to execute\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(\"Creating Query Fixer Agent...\")\n",
    "query_fixer_agent = Agent.direct_create(\n",
    "    name=f\"query-fixer-agent-{agent_suffix}\",\n",
    "    role=\"Query Repair Specialist\",\n",
    "    goal=\"Fix and optimize failed DSL queries\",\n",
    "    instructions=f\"\"\"\n",
    "    You are an expert query debugger and optimizer. Your tasks are:\n",
    "    1. Analyze failed DSL queries from the query generator\n",
    "    2. Diagnose errors using OpenSearch error messages\n",
    "    3. Apply targeted fixes while maintaining original intent\n",
    "    4. Optimize queries for better recall when results are empty\n",
    "    5. Extract exact terms from 'terms' aggregations for accurate reporting.\n",
    "    6. Identify alternative ways to answer queries when direct fields are missing.\n",
    "    7. Recognize schema gaps and propose workarounds or schema modifications.\n",
    "\n",
    "    ## Repair Strategies:\n",
    "    - SYNTAX ERRORS: Fix formatting issues in nested queries/aggregations\n",
    "    - FIELD ERRORS: Map invalid fields to valid schema equivalents\n",
    "    - ZERO HITS: Apply query relaxation techniques:\n",
    "        * Add wildcards to keyword searches\n",
    "        * Expand date ranges\n",
    "        * Reduce strictness of term matches\n",
    "        * Add synonym expansion\n",
    "    - Ensure queries include 'terms' aggregations to capture exact values\n",
    "\n",
    "    ## Optimization Rules:\n",
    "    - Maintain original query structure where possible\n",
    "    - Prefer query-time fixes over rearchitecting\n",
    "    - Document all modifications in revision notes\n",
    "    - Limit query relaxation to 3 iterations\n",
    "    - When results are found, check the 'terms' aggregation for the exact field values.\n",
    "    - Report the exact terms from the data (e.g., use \"DHL Express\" if that's the stored value).\n",
    "\n",
    "    ## Schema Gap Analysis & Alternative Solutions:\n",
    "    - Creative Field Mapping: If direct fields are missing, use existing fields to infer answers.\n",
    "      * Example: If \"delivery duration\" is not available, compute it using `delivered_time - out_for_delivery_time`.\n",
    "    - Schema Enhancement: Identify missing fields required for full query support.\n",
    "    - Derived Data Solutions:\n",
    "      * If exact data isnâ€™t available but can be computed, create scripted fields using painless scripting.\n",
    "      * If no alternative exists, clearly state the required data and suggest schema modifications.\n",
    "\n",
    "\n",
    "\n",
    "    ## Schema:\n",
    "    {ecom_shipping_schema_string}\n",
    "\n",
    "    ## Output Format:\n",
    "    - Return modified query in <json> tags\n",
    "    - Include revision notes and exact terms from aggregations in <notes> tags \n",
    "    \"\"\",\n",
    "    tool_code=modified_query_lambda_arn,\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"retry_query\",\n",
    "            \"description\": \"Retries a modified version of the failed query\",\n",
    "            \"parameters\": {\n",
    "                \"modified_dsl_query\": {\n",
    "                    \"description\": \"The corrected DSL query\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True\n",
    "                },\n",
    "                \"revision_notes\": {\n",
    "                    \"description\": \"Description of modifications made\",\n",
    "                    \"type\": \"string\",\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "kb_rag_agent = Agent.direct_create(\n",
    "    name=f\"kb-response-agent-{agent_suffix}\",\n",
    "    role=\"Knowledge Base Content Analyzer\",\n",
    "    goal=\"Analyze retrieved document content and generate well-structured responses\",\n",
    "    instructions=\"\"\"\n",
    "    You analyze pre-retrieved document content and generate clear, accurate responses.\n",
    "\n",
    "    ## Response Rules:\n",
    "    - Synthesize information from provided passages\n",
    "    - Include relevant quotes with proper citations\n",
    "    - Use consistent citation format [doc_id:para_num]\n",
    "    - Maintain factual accuracy\n",
    "    - Flag any inconsistencies between sources\n",
    "\n",
    "    ## Output Format:\n",
    "    Response should be structured as:\n",
    "    1. Direct answer\n",
    "    2. Supporting evidence\n",
    "    3. Source citations\n",
    "    4. Confidence level (High/Medium/Low)\n",
    "\n",
    "    ## Quality Guidelines:\n",
    "    - Prefer direct quotes for key information\n",
    "    - Summarize when appropriate\n",
    "    - Note any information gaps\n",
    "    - Maintain neutral tone\n",
    "    \"\"\",\n",
    "    kb_descr=\"Use knowledge base to extract relevant information, analyze content across multiple documents, and generate accurate responses with proper citations. Focus on maintaining context and factual accuracy.\",\n",
    "    kb_id=\"5GADU65GNF\",\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:57:21,855 - __main__ - INFO - Retrieving DSL Query Agent ID...\n",
      "2025-02-07 18:57:21,934 - __main__ - INFO - DSL Query Agent ID: UZ0ZCHE67L\n",
      "2025-02-07 18:57:21,935 - __main__ - INFO - Retrieving Query Fixer Agent ID...\n",
      "2025-02-07 18:57:22,026 - __main__ - INFO - Query Fixer Agent ID: BVZPDW8UGH\n",
      "2025-02-07 18:57:22,027 - __main__ - INFO - Retrieving KB Response Agent ID...\n",
      "2025-02-07 18:57:22,102 - __main__ - INFO - KB Response Agent ID: CRL3Q8ZW0K\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Retrieve the newly created Agent IDs\n",
    "# -------------------------------------------------------------------------\n",
    "logger.info(\"Retrieving DSL Query Agent ID...\")\n",
    "dsl_query_agent_id = agents_helper.get_agent_id_by_name(dsl_query_agent.name)\n",
    "logger.info(f\"DSL Query Agent ID: {dsl_query_agent_id}\")\n",
    "\n",
    "logger.info(\"Retrieving Query Fixer Agent ID...\")\n",
    "query_fixer_agent_id = agents_helper.get_agent_id_by_name(query_fixer_agent.name)\n",
    "logger.info(f\"Query Fixer Agent ID: {query_fixer_agent_id}\")\n",
    "\n",
    "logger.info(\"Retrieving KB Response Agent ID...\")\n",
    "kb_rag_agent_id = agents_helper.get_agent_id_by_name(kb_rag_agent.name)\n",
    "logger.info(f\"KB Response Agent ID: {kb_rag_agent_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:57:22,106 - __main__ - INFO - Adding resource-based policy to Lambda function execute-dsl-query-us-west-2-533 for agents: ['UZ0ZCHE67L']\n",
      "2025-02-07 18:57:22,245 - __main__ - INFO - Removing existing statement: AllowExecutionFromBedrockAgent_YNAHU2DUTR\n",
      "2025-02-07 18:57:22,429 - __main__ - INFO - Added permission for statement: AllowExecutionFromBedrockAgent_UZ0ZCHE67L\n",
      "2025-02-07 18:57:22,429 - __main__ - INFO - Adding resource-based policy to Lambda function execute-modified-dsl-query-us-west-2-533 for agents: ['BVZPDW8UGH']\n",
      "2025-02-07 18:57:22,478 - __main__ - INFO - Removing existing statement: AllowExecutionFromBedrockAgent_TN0JXYFRFN\n",
      "2025-02-07 18:57:22,641 - __main__ - INFO - Added permission for statement: AllowExecutionFromBedrockAgent_BVZPDW8UGH\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 5. Add resource-based policy to each Lambda so the Agents can invoke them\n",
    "# -------------------------------------------------------------------------\n",
    "add_resource_based_policy(DSL_QUERY_LAMBDA_NAME, [dsl_query_agent_id], region, account_id)\n",
    "add_resource_based_policy(MODIFIED_QUERY_LAMBDA_NAME, [query_fixer_agent_id], region, account_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent supervisor-agent-us-west-2-533 not found\n",
      "\n",
      "Created supervisor, id: PTF2OIBXNT, alias id: TSTALIASID\n",
      "\n",
      "  associating sub-agents / collaborators to supervisor...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id PTF2OIBXNT current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id PTF2OIBXNT current status: PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id PTF2OIBXNT current status: PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id PTF2OIBXNT current status: PREPARED\n",
      "DONE: Agent: supervisor-agent-us-west-2-533, id: PTF2OIBXNT, alias id: SM0GTIQCW6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supervisor_agent = SupervisorAgent.direct_create(\n",
    "    name=f\"supervisor-agent-{agent_suffix}\",\n",
    "    role=\"Query Pipeline Orchestrator\",\n",
    "    collaboration_type=\"SUPERVISOR\",\n",
    "    collaborator_objects=[dsl_query_agent, query_fixer_agent, kb_rag_agent],\n",
    "    collaborator_agents=[\n",
    "        {\n",
    "            \"agent\": dsl_query_agent.name,\n",
    "            \"instructions\": dedent(f\"\"\"\n",
    "                {dsl_query_agent.name} is responsible for generating the primary DSL query based on \n",
    "                the provided e-commerce shipping schema. Your task is to produce a precise Query DSL \n",
    "                encapsulated in <json>...</json> tags. Ensure the query strictly follows the schema \n",
    "                and DSL syntax without any additional commentary or explanations.\n",
    "            \"\"\").strip(),\n",
    "            \"relay_conversation_history\": \"DISABLED\"\n",
    "        },\n",
    "        {\n",
    "            \"agent\": query_fixer_agent.name,\n",
    "            \"instructions\": dedent(f\"\"\"\n",
    "                Engage {query_fixer_agent.name} when any of the following conditions occur:\n",
    "                1. The DSL query returns syntax or validation errors.\n",
    "                2. The DSL query execution returns zero hits.\n",
    "                3. The query requires optimization for improved recall.\n",
    "                4. Alternative query solutions are needed due to schema limitations.\n",
    "                \n",
    "                Responsibilities:\n",
    "                - Analyze error messages and the current query structure.\n",
    "                - Apply targeted fixes that preserve the original query intent.\n",
    "                - Implement query relaxation techniques (for example, adding wildcards, extending date ranges, or expanding term matches).\n",
    "                - Identify and map alternative fields if direct schema fields are missing.\n",
    "                - Suggest schema enhancements when appropriate.\n",
    "                - Document all modifications with clear revision notes and output exact terms from aggregations.\n",
    "                \n",
    "                Return the corrected DSL query within <json>...</json> tags and include any revision notes within <notes>...</notes> tags.\n",
    "            \"\"\"),\n",
    "            \"relay_conversation_history\": \"DISABLED\"\n",
    "        },\n",
    "        {\n",
    "            \"agent\": kb_rag_agent.name,\n",
    "            \"instructions\": dedent(f\"\"\"\n",
    "                Engage {kb_rag_agent.name} to answer user questions that require analyzing the document content retrieved from executed queries.\n",
    "                When search results are available, your task is to:\n",
    "                \n",
    "                1. Synthesize and validate the information from the provided passages.\n",
    "                2. Generate a final response that includes a direct answer and supporting evidence with relevant quotes and citations.\n",
    "                \n",
    "                Your output must be clear, well-structured, and factually accurate to support decision-making.\n",
    "            \"\"\"),\n",
    "            \"relay_conversation_history\": \"DISABLED\"\n",
    "        }\n",
    "    ],\n",
    "    instructions=dedent(f\"\"\"\n",
    "        High-Level Overview:\n",
    "        Route user queries to the appropriate agent based on the type of answer required:\n",
    "          - Structured Data Retrieval: If the query requires retrieving structured information from the e-commerce shipping data, it is routed to {dsl_query_agent.name}. If the DSL query returns errors or zero hits, the query is immediately routed to the Query Fixer Agent for reattempts.\n",
    "          - Document Content Analysis: If the query requires a synthesized, in-depth answer derived from analyzing executed query results, it is routed to {kb_rag_agent.name}.\n",
    "        \n",
    "        Detailed Instructions:\n",
    "        \n",
    "        Route A: Structured Data Retrieval (DSL Query Agent + Query Fixer Agent)\n",
    "        1. Initial Query Analysis:\n",
    "           - Receive the user's natural language query.\n",
    "           - Determine if the query requires structured data retrieval from the e-commerce shipping data.\n",
    "           - Validate the query against the provided schema:\n",
    "             {ecom_shipping_schema_string}\n",
    "           - If the query qualifies, route it to {dsl_query_agent.name}.\n",
    "        \n",
    "        2. DSL Query Execution:\n",
    "           - {dsl_query_agent.name} generates a Query DSL that is encapsulated in <json>...</json> tags and follows the provided schema.\n",
    "        \n",
    "        3. Error Handling & Retry:\n",
    "           - Monitor the query execution results:\n",
    "             a. If the DSL query returns syntax or validation errors, or if the result is zero hits, capture the error context.\n",
    "             b. Immediately route the query, along with diagnostic details, to {query_fixer_agent.name}.\n",
    "             c. {query_fixer_agent.name} applies targeted fixes and query relaxation techniques, then returns a modified DSL query.\n",
    "             d. Validate the modified query; allow up to 3 retry attempts if necessary.\n",
    "        \n",
    "        4. Evaluation & Final Approval (for structured data queries):\n",
    "           - Confirm that the final DSL query adheres to best practices (for example, proper nested queries, correct field types and mappings).\n",
    "           - Maintain an audit trail of all query versions and modifications.\n",
    "           - Generate an execution summary including:\n",
    "             - Query versions attempted.\n",
    "             - Reasons for modifications.\n",
    "             - Performance metrics.\n",
    "        \n",
    "        General Aggregation Guidance:\n",
    "           - If an aggregation returns an unexpectedly inflated count, verify whether the aggregation is counting nested or repeated values.\n",
    "           - To accurately count unique items, use a cardinality aggregation on a unique field identifier rather than aggregating on fields that might contain duplicate entries.\n",
    "        \n",
    "        Route B: Document Content Analysis (KB Response Agent)\n",
    "        1. Initial Query Analysis:\n",
    "           - Receive the user's natural language query.\n",
    "           - Determine if the query requires synthesizing and analyzing document content from executed queries.\n",
    "           - If so, route the query to {kb_rag_agent.name}.\n",
    "        \n",
    "        2. KB Response Generation:\n",
    "           - {kb_rag_agent.name} synthesizes and validates the information from the provided passages.\n",
    "           - Generate a final response that includes:\n",
    "             a. A direct answer.\n",
    "             b. Supporting evidence with relevant quotes and citations.\n",
    "             c. A confidence level.\n",
    "        \n",
    "        3. Final Response:\n",
    "           - Deliver the final, well-structured answer to the user by addressing the user by name. \n",
    "    \"\"\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect attributes of the SupervisorAgent object\n",
    "# print(dir(supervisor_agent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:57:58,514 - __main__ - INFO - Retrieving Supervisor Agent ID...\n",
      "2025-02-07 18:57:58,588 - __main__ - INFO - Supervisor Agent ID: PTF2OIBXNT\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the Supervisor Agent ID\n",
    "logger.info(\"Retrieving Supervisor Agent ID...\")\n",
    "supervisor_agent_id = agents_helper.get_agent_id_by_name(supervisor_agent.name)\n",
    "logger.info(f\"Supervisor Agent ID: {supervisor_agent_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the agent based on the ID\n",
    "# supervisor_agentV2 = agents_helper.get_agent_by_id(\"KGTOVCVLKI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invokeAgent API request ID: dde28364-d966-4c2a-a6f7-7b0dcc910abb\n",
      "invokeAgent API session ID: 12345\n",
      "  agent id: PTF2OIBXNT, agent alias id: SM0GTIQCW6\n",
      "\u001b[32m---- Step 1 ----\u001b[0m\n",
      "\u001b[33mTook 4.6s, using 2797 tokens (in: 2651, out: 146) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mThis query requires structured data retrieval to count orders shipped by DHL carrier. I'll route this to the DSL query agent to create an appropriate query.\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'dsl-query-agent-us-west-2-533 [UZ0ZCHE67L/AGKDHUAHJJ]' passing input text:\u001b[0m\n",
      "\u001b[35mCreate a DSL query to count the number of orders where the delivery carrier is DHL.\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.1 [using sub-agent name:dsl-query-agent-us-west-2-533, id:UZ0ZCHE67L/AGKDHUAHJJ] ----\u001b[0m\n",
      "\u001b[33mTook 8.5s, using 1880 tokens (in: 1488, out: 392) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mTo create a DSL query that counts the number of orders where the delivery carrier is DHL, we need to use a term query to match the exact carrier name and then use a count aggregation. Let's construct the query step by step:\n",
      "\n",
      "1. We'll use a term query to match the delivery carrier \"DHL\".\n",
      "2. We'll use a count aggregation to count the number of matching documents.\n",
      "\n",
      "Now, let's create the DSL query based on these requirements.\n",
      "</thinking>\n",
      "\n",
      "<json>\n",
      "{\n",
      "  \"size\": 0,\n",
      "  \"query\": {\n",
      "    \"term\": {\n",
      "      \"delivery.carrier\": \"DHL\"\n",
      "    }\n",
      "  },\n",
      "  \"aggs\": {\n",
      "    \"order_count\": {\n",
      "      \"value_count\": {\n",
      "        \"field\": \"order_id\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "</json>\n",
      "\n",
      "<thinking>\n",
      "Now that we have created the DSL query, let's execute it using the provided function to get the results.\u001b[0m\n",
      "\u001b[35mUsing tool: execute_dsl_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'dsl_query', 'type': 'string', 'value': '{\\n  \"size\": 0,\\n  \"query\": {\\n    \"term\": {\\n      \"delivery.carrier\": \"DHL\"\\n    }\\n  },\\n  \"aggs\": {\\n    \"order_count\": {\\n      \"value_count\": {\\n        \"field\": \"order_id\"\\n      }\\n    }\\n  }\\n}'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': {'took': 14, 'timed_out': False, '_shards': {'total': 0, 'successful': 0, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}, 'aggregations': {'order_count': {'value': 0}}}}}...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.2 [using sub-agent name:dsl-query-agent-us-west-2-533, id:UZ0ZCHE67L/AGKDHUAHJJ] ----\u001b[0m\n",
      "\u001b[33mTook 5.3s, using 2110 tokens (in: 1991, out: 119) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mNow that we have the results from executing the DSL query, let's interpret them and provide an answer to the user's question.\n",
      "\n",
      "The query execution was successful, and we can see the aggregation results in the response. The 'order_count' aggregation shows a value of 0, which means there are no orders where the delivery carrier is DHL in the current dataset.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "Based on the query results, there are currently 0 orders where the delivery carrier is DHL....\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent dsl-query-agent-us-west-2-533 output text:\n",
      "Based on the query results, there are currently 0 orders where the delivery carrier is DHL....\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.3 [using sub-agent name:dsl-query-agent-us-west-2-533, id:UZ0ZCHE67L/AGKDHUAHJJ] ----\u001b[0m\n",
      "\u001b[33mTook 4.8s, using 2988 tokens (in: 2825, out: 163) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mSince we got zero hits, I should engage the query fixer agent to verify and potentially modify the query to ensure we're not missing any results.\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'query-fixer-agent-us-west-2-533 [BVZPDW8UGH/KCBBPUCEVM]' passing input text:\u001b[0m\n",
      "\u001b[35mThe query for counting orders with DHL as the carrier returned zero hits. Please verify and suggest any modifications to ensure we're not missing any results. Consider case sensitivity and possible variations in carrier name.\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.4 [using sub-agent name:query-fixer-agent-us-west-2-533, id:BVZPDW8UGH/KCBBPUCEVM] ----\u001b[0m\n",
      "\u001b[33mTook 10.2s, using 2143 tokens (in: 1700, out: 443) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI understand that a query to count orders with DHL as the carrier returned zero hits. This is unexpected, and we need to investigate why this might be happening. Let's analyze the query and make some modifications to ensure we're not missing any results. We'll consider case sensitivity and possible variations in the carrier name.\n",
      "\n",
      "First, I'll create a modified query that addresses these potential issues:\n",
      "\n",
      "1. We'll use a match query instead of a term query to allow for some flexibility in matching.\n",
      "2. We'll add a terms aggregation to see what carrier values actually exist in the index.\n",
      "3. We'll make the search case-insensitive by using lowercase in the query and applying a lowercase filter.\n",
      "\n",
      "Here's the modified query:\u001b[0m\n",
      "\u001b[35mUsing tool: retry_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'modified_dsl_query', 'type': 'string', 'value': '{\\n  \"size\": 0,\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"match\": {\\n            \"delivery.carrier\": {\\n              \"query\": \"dhl\",\\n              \"analyzer\": \"standard\"\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  },\\n  \"aggs\": {\\n    \"carriers\": {\\n      \"terms\": {\\n        \"field\": \"delivery.carrier\",\\n        \"size\": 10\\n      }\\n    }\\n  }\\n}'}, {'name': 'revision_notes', 'type': 'string', 'value': 'Modified the query to use a match query instead of a term query for more flexible matching.\\nAdded a terms aggregation on the delivery.carrier field to see what values actually exist.\\nMade the search case-insensitive by using lowercase in the query and applying a standard analyzer.'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': '{\\n  \"hits\": 0,\\n  \"aggregations\": {\\n    \"carriers\": {\\n      \"doc_count_error_upper_bound\": 0,\\n      \"sum_other_doc_count\": 0,\\n      \"buckets\": []\\n    }\\n  },\\n  \"revision_notes\": \"Modified the query to use a match query instead of a term query for more flexible matching.\\\\nA...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.5 [using sub-agent name:query-fixer-agent-us-west-2-533, id:BVZPDW8UGH/KCBBPUCEVM] ----\u001b[0m\n",
      "\u001b[33mTook 8.3s, using 2590 tokens (in: 2306, out: 284) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mThe modified query still returned zero hits, which is concerning. However, the aggregation results provide some important information. The \"carriers\" aggregation returned an empty bucket, which suggests that there might not be any documents with the \"delivery.carrier\" field populated, or the field might not exist at all.\n",
      "\n",
      "Let's try another modification to the query to investigate this further. We'll remove the filter on the carrier and just look at the top carriers in the index:\u001b[0m\n",
      "\u001b[35mUsing tool: retry_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'modified_dsl_query', 'type': 'string', 'value': '{\\n  \"size\": 0,\\n  \"aggs\": {\\n    \"carriers\": {\\n      \"terms\": {\\n        \"field\": \"delivery.carrier\",\\n        \"size\": 20\\n      }\\n    }\\n  }\\n}'}, {'name': 'revision_notes', 'type': 'string', 'value': 'Removed the query filter to see all carriers in the index.\\nIncreased the size of the terms aggregation to 20 to get a broader view of the carriers.'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': '{\\n  \"hits\": 5,\\n  \"aggregations\": {\\n    \"carriers\": {\\n      \"doc_count_error_upper_bound\": 0,\\n      \"sum_other_doc_count\": 0,\\n      \"buckets\": [\\n        {\\n          \"key\": \"DHL Express\",\\n          \"doc_count\": 1\\n        },\\n        {\\n          \"key\": \"FedEx Priority\",\\n ...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.6 [using sub-agent name:query-fixer-agent-us-west-2-533, id:BVZPDW8UGH/KCBBPUCEVM] ----\u001b[0m\n",
      "\u001b[33mTook 7.8s, using 3189 tokens (in: 2886, out: 303) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mNow we have some valuable information. The query returned results, and we can see the actual carrier values in the index. The reason our original query didn't work is that the carrier name for DHL is actually stored as \"DHL Express\" instead of just \"DHL\".\n",
      "\n",
      "Let's modify our query one more time to correctly count the orders with DHL Express as the carrier:\u001b[0m\n",
      "\u001b[35mUsing tool: retry_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'modified_dsl_query', 'type': 'string', 'value': '{\\n  \"size\": 0,\\n  \"query\": {\\n    \"term\": {\\n      \"delivery.carrier.keyword\": \"DHL Express\"\\n    }\\n  },\\n  \"aggs\": {\\n    \"carriers\": {\\n      \"terms\": {\\n        \"field\": \"delivery.carrier.keyword\",\\n        \"size\": 10\\n      }\\n    }\\n  }\\n}'}, {'name': 'revision_notes', 'type': 'string', 'value': 'Updated the query to use the exact term \"DHL Express\".\\nUsed a term query on the keyword field for exact matching.\\nKept the terms aggregation to verify the results.'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': '{\\n  \"hits\": 0,\\n  \"aggregations\": {\\n    \"carriers\": {\\n      \"doc_count_error_upper_bound\": 0,\\n      \"sum_other_doc_count\": 0,\\n      \"buckets\": []\\n    }\\n  },\\n  \"revision_notes\": \"Updated the query to use the exact term \\\\\"DHL Express\\\\\".\\\\nUsed a term query on the keyword f...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.7 [using sub-agent name:query-fixer-agent-us-west-2-533, id:BVZPDW8UGH/KCBBPUCEVM] ----\u001b[0m\n",
      "\u001b[33mTook 6.2s, using 3622 tokens (in: 3342, out: 280) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI apologize for the confusion. It seems that even with the exact term \"DHL Express\", we're still not getting any results. This suggests that there might be an issue with how we're querying the field. Let's try one more modification to ensure we're targeting the correct field structure:\u001b[0m\n",
      "\u001b[35mUsing tool: retry_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'modified_dsl_query', 'type': 'string', 'value': '{\\n  \"size\": 0,\\n  \"query\": {\\n    \"match\": {\\n      \"delivery.carrier\": \"DHL Express\"\\n    }\\n  },\\n  \"aggs\": {\\n    \"carriers\": {\\n      \"terms\": {\\n        \"field\": \"delivery.carrier\",\\n        \"size\": 10\\n      }\\n    }\\n  }\\n}'}, {'name': 'revision_notes', 'type': 'string', 'value': 'Changed back to a match query to allow for some flexibility.\\nRemoved the .keyword suffix from the field name.\\nKept the terms aggregation to verify the results.'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': '{\\n  \"hits\": 1,\\n  \"aggregations\": {\\n    \"carriers\": {\\n      \"doc_count_error_upper_bound\": 0,\\n      \"sum_other_doc_count\": 0,\\n      \"buckets\": [\\n        {\\n          \"key\": \"DHL Express\",\\n          \"doc_count\": 1\\n        }\\n      ]\\n    }\\n  },\\n  \"revision_notes\": \"Change...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.8 [using sub-agent name:query-fixer-agent-us-west-2-533, id:BVZPDW8UGH/KCBBPUCEVM] ----\u001b[0m\n",
      "\u001b[33mTook 8.3s, using 4075 tokens (in: 3803, out: 272) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mGreat! We've finally got the correct results. Now I can provide an answer to the user's question.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "I've investigated the issue with counting orders that have DHL as the carrier, and I've found the reason for the zero hits in the original query. Here are the findings and modifications:\n",
      "\n",
      "1. The carrier name for DHL in the index is actually stored as \"DHL Express\", not just \"DHL\".\n",
      "2. The query neede...\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent query-fixer-agent-us-west-2-533 output text:\n",
      "I've investigated the issue with counting orders that have DHL as the carrier, and I've found the reason for the zero hits in the original query. Here are the findings and modifications:\n",
      "\n",
      "1. The carrier name for DHL in the index is actually stored as \"DHL Express\", not just \"DHL\".\n",
      "2. The query neede...\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:59:24,927 - __main__ - INFO - Supervisor agent response: There is 1 order that has been shipped by DHL (specifically DHL Express).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m---- Step 1.9 [using sub-agent name:query-fixer-agent-us-west-2-533, id:BVZPDW8UGH/KCBBPUCEVM] ----\u001b[0m\n",
      "\u001b[33mTook 3.3s, using 3344 tokens (in: 3233, out: 111) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mNow I can provide the accurate answer to the user.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "There is 1 order that has been shipped by DHL (specifically DHL Express)....\u001b[0m\n",
      "\u001b[33mAgent made a total of 10 LLM calls, using 28738 tokens (in: 26225, out: 2513), and took 67.5 total seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 7. Invoke the Supervisor Agent with a sample query\n",
    "# -------------------------------------------------------------------------\n",
    "response = supervisor_agent.invoke(\n",
    "    input_text=\"How many orders have been shipped by DHL?\",\n",
    "    session_id=\"12345\",\n",
    "    enable_trace=True,\n",
    "    trace_level=\"core\"\n",
    ")\n",
    "logger.info(f\"Supervisor agent response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invokeAgent API request ID: 350bf5c5-5bed-4b3f-81af-fc0798c1caf7\n",
      "invokeAgent API session ID: 12345\n",
      "  agent id: D059VOSRUS, agent alias id: XBA6NRSJJ4\n",
      "\u001b[32m---- Step 1 ----\u001b[0m\n",
      "\u001b[33mTook 4.3s, using 3505 tokens (in: 3341, out: 164) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI need to create a query to count orders where the delivery carrier is DHL. I'll send this request to the DSL query agent to generate the appropriate query.\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'dsl-query-agent-us-west-2-533 [YNAHU2DUTR/WJXPCPQSDM]' passing input text:\u001b[0m\n",
      "\u001b[35mCreate a query to count the number of orders where the delivery carrier contains 'DHL' (using a wildcard or match query to catch variations like 'DHL Express').\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.1 [using sub-agent name:dsl-query-agent-us-west-2-533, id:YNAHU2DUTR/WJXPCPQSDM] ----\u001b[0m\n",
      "\u001b[33mTook 7.6s, using 2512 tokens (in: 2081, out: 431) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mTo create a query that counts the number of orders where the delivery carrier contains 'DHL', we need to use a more flexible matching approach. We'll use a wildcard query to catch variations like 'DHL Express'. Here's the step-by-step plan:\n",
      "\n",
      "1. Use a wildcard query to match any carrier name containing 'DHL'.\n",
      "2. Use a count aggregation to count the number of matching documents.\n",
      "\n",
      "Now, let's construct the DSL query based on these requirements.\n",
      "</thinking>\n",
      "\n",
      "<json>\n",
      "{\n",
      "  \"size\": 0,\n",
      "  \"query\": {\n",
      "    \"wildcard\": {\n",
      "      \"delivery.carrier\": {\n",
      "        \"value\": \"*DHL*\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"aggs\": {\n",
      "    \"order_count\": {\n",
      "      \"value_count\": {\n",
      "        \"field\": \"order_id\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "</json>\n",
      "\n",
      "<thinking>\n",
      "Now that we have created the DSL query using a wildcard to match any carrier name containing 'DHL', let's execute it using the provided function to get the results.\u001b[0m\n",
      "\u001b[35mUsing tool: execute_dsl_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'dsl_query', 'type': 'string', 'value': '{\\n  \"size\": 0,\\n  \"query\": {\\n    \"wildcard\": {\\n      \"delivery.carrier\": {\\n        \"value\": \"*DHL*\"\\n      }\\n    }\\n  },\\n  \"aggs\": {\\n    \"order_count\": {\\n      \"value_count\": {\\n        \"field\": \"order_id\"\\n      }\\n    }\\n  }\\n}'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': {'took': 15, 'timed_out': False, '_shards': {'total': 0, 'successful': 0, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 1, 'relation': 'eq'}, 'max_score': None, 'hits': []}, 'aggregations': {'order_count': {'value': 1}}}}}...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.2 [using sub-agent name:dsl-query-agent-us-west-2-533, id:YNAHU2DUTR/WJXPCPQSDM] ----\u001b[0m\n",
      "\u001b[33mTook 2.9s, using 2693 tokens (in: 2626, out: 67) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mNow that we have the results, let's interpret them and provide an answer to the user's question.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "Based on the query results, there is 1 order where the delivery carrier contains 'DHL' or its variations (like 'DHL Express')....\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent dsl-query-agent-us-west-2-533 output text:\n",
      "Based on the query results, there is 1 order where the delivery carrier contains 'DHL' or its variations (like 'DHL Express')....\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:45:08,167 - __main__ - INFO - Supervisor agent response: There is 1 order that has been shipped by DHL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m---- Step 1.3 [using sub-agent name:dsl-query-agent-us-west-2-533, id:YNAHU2DUTR/WJXPCPQSDM] ----\u001b[0m\n",
      "\u001b[33mTook 2.8s, using 3652 tokens (in: 3549, out: 103) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mLet me send this information to the user.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "There is 1 order that has been shipped by DHL....\u001b[0m\n",
      "\u001b[33mAgent made a total of 4 LLM calls, using 12362 tokens (in: 11597, out: 765), and took 17.7 total seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 7. Invoke the Supervisor Agent with a sample query\n",
    "# -------------------------------------------------------------------------\n",
    "response = supervisor_agent.invoke(\n",
    "    input_text=\"How many orders have been shipped by DHL?\",\n",
    "    session_id=\"12346\",\n",
    "    enable_trace=True,\n",
    "    trace_level=\"core\",\n",
    "    session_state={\n",
    "        \"sessionAttributes\": {\n",
    "            \"userName\": \"Alice\"\n",
    "        }\n",
    "    }\n",
    "\n",
    ")\n",
    "logger.info(f\"Supervisor agent response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "invokeAgent API input parameters: input_text: How many orders have been shipped by DHL?, agent_id: QDMOLJO9TV, agent_alias_id: LXJCIDHVKO, session_id: 12345, session_state: {}, enable_trace: True, end_session: False, trace_level: all, multi_agent_names: {'CSKABIRKLE/V6WM6PSNHT': 'dsl-query-agent-us-west-2-533', 'SGSWZUVNOB/7KNLAUFPQC': 'query-fixer-agent-us-west-2-533', 'PM73T3Y9BR/GHH3JVSG1F': 'kb-response-agent-us-west-2-533', 'QDMOLJO9TV/LXJCIDHVKO': 'supervisor-agent-us-west-2-533'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if isinstance(response, dict):\n",
    "#     print(f\"Answer: {response['answer']}\")\n",
    "#     print(f\"Duration: {response['metadata']['duration_seconds']}s\")\n",
    "#     print(f\"Total tokens: {response['metadata']['total_tokens']}\")\n",
    "#     # Save trace data if needed\n",
    "#     with open('trace.json', 'w') as f:\n",
    "#         json.dump(response['trace'], f, indent=2)\n",
    "# else:\n",
    "#     print(response)  # Just print the answer for other trace_level values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invokeAgent API input parameters: input_text: What are the effects of Covid-19 on e-commerce?, agent_id: QDMOLJO9TV, agent_alias_id: LXJCIDHVKO, session_id: 1245, session_state: {}, enable_trace: True, end_session: False, trace_level: core, multi_agent_names: {'CSKABIRKLE/V6WM6PSNHT': 'dsl-query-agent-us-west-2-533', 'SGSWZUVNOB/7KNLAUFPQC': 'query-fixer-agent-us-west-2-533', 'PM73T3Y9BR/GHH3JVSG1F': 'kb-response-agent-us-west-2-533', 'QDMOLJO9TV/LXJCIDHVKO': 'supervisor-agent-us-west-2-533'}\n",
      "\u001b[33mTook 8.0s, using 2897 tokens.\u001b[0m\n",
      "\u001b[34mThis question requires analyzing shipping data patterns and trends related to Covid-19. I should route this to both the DSL query agent to get relevant data and then to the KB response agent for a comprehensive analysis.\n",
      "\n",
      "First, I'll request a query to analyze shipping patterns during the Covid-19 period (focusing on 2020-2021) and compare it with pre-Covid data.\u001b[0m\n",
      "\u001b[33mTook 11.0s, using 2118 tokens.\u001b[0m\n",
      "\u001b[34mTo analyze shipping patterns during the Covid-19 period (2020-2021) compared to pre-Covid (2019), we need to create a complex query that includes multiple aggregations. We'll use date ranges to separate the years and then perform various aggregations for each year. Let's break down the query construction:\n",
      "\n",
      "1. We'll use date ranges for 2019, 2020, and 2021 using the created_at field.\n",
      "2. For each year, we'll create sub-aggregations for:\n",
      "   a. Total number of orders (count)\n",
      "   b. Average shipping costs\n",
      "   c. Delivery status patterns\n",
      "   d. Common shipping carriers used\n",
      "\n",
      "Let's construct the Query DSL to achieve this analysis.\u001b[0m\n",
      "\u001b[33mTook 12.7s, using 3258 tokens.\u001b[0m\n",
      "\u001b[34mBased on the query results, it appears that there is no data available for the specified time periods (2019, 2020, and 2021). All the aggregations return empty results or zero values. This could mean that either:\n",
      "\n",
      "1. There are no shipping records in the database for these years.\n",
      "2. The data might be stored in a different format or under different field names.\n",
      "3. There might be an issue with the date range or the query itself.\n",
      "\n",
      "Given this situation, we should provide an answer to the user explaining the lack of data and suggest some possible next steps.\u001b[0m\n",
      "\u001b[33mTook 5.2s, using 3372 tokens.\u001b[0m\n",
      "\u001b[34mSince we received a response indicating data availability issues, I should modify our approach to focus on any available shipping data patterns that might reflect e-commerce trends, without specific date constraints.\u001b[0m\n",
      "\u001b[33mTook 10.0s, using 3766 tokens.\u001b[0m\n",
      "\u001b[34mTo analyze general shipping patterns without specifying a date range, we'll create a query that focuses on the overall statistics you've requested. We'll use aggregations to gather the required information. Here's how we'll structure the query:\n",
      "\n",
      "1. Total number of orders: We'll use a value_count aggregation on the order_id field.\n",
      "2. Average shipping costs: We'll use an avg aggregation on the delivery.shipping_cost field.\n",
      "3. Most common delivery carriers: We'll use a terms aggregation on the delivery.carrier field.\n",
      "4. Distribution of shipping status: We'll use a terms aggregation on the status field.\n",
      "5. Package values distribution: We'll use a histogram aggregation on the package.value field.\n",
      "\n",
      "Let's construct the Query DSL to achieve this analysis.\u001b[0m\n",
      "\u001b[33mTook 7.7s, using 4529 tokens.\u001b[0m\n",
      "\u001b[34mNow that we have the results from our query, let's analyze the data and prepare a concise summary for the user. We'll break down the information according to the requested categories.\u001b[0m\n",
      "\u001b[33mTook 6.1s, using 3821 tokens.\u001b[0m\n",
      "\u001b[34mNow that I have some data, albeit limited, I should ask the KB response agent to analyze this information and provide insights about e-commerce patterns.\u001b[0m\n",
      "\u001b[33mTook 13.1s, using 4021 tokens.\u001b[0m\n",
      "\u001b[33mTook 9.7s, using 4516 tokens.\u001b[0m\n",
      "\u001b[34mI should now synthesize this information and send a comprehensive response to the user.\u001b[0m\n",
      "\u001b[33mAgent made 9 LLM calls, used 32298 tokens, and took 83.5s total.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = supervisor_agent.invoke(\n",
    "    input_text=\"What are the effects of Covid-19 on e-commerce?\",\n",
    "    session_id=\"1245\",\n",
    "    enable_trace=True,\n",
    "    trace_level=\"core\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = supervisor_agent.invoke(\n",
    "#     input_text=\"How many orders have recipients in Spain and were last updated during customs clearance after January 16, 2024?\",\n",
    "#     session_id=\"1245\",\n",
    "#     enable_trace=True,\n",
    "#     trace_level=\"core\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = supervisor_agent.invoke(\n",
    "#     input_text=\"What are the temperature controlled packages delivered within 2 hours?\",\n",
    "#     session_id=\"1245\",\n",
    "#     enable_trace=True,\n",
    "#     trace_level=\"core\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:59:55,424 - __main__ - INFO - Deleting Supervisor Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: supervisor-agent-us-west-2-533, id: PTF2OIBXNT\n",
      "Deleting aliases for agent PTF2OIBXNT...\n",
      "Deleting alias SM0GTIQCW6 from agent PTF2OIBXNT\n",
      "Deleting alias TSTALIASID from agent PTF2OIBXNT\n",
      "Deleting agent: PTF2OIBXNT...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_supervisor-agent-us-west-2-533...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 19:00:07,254 - __main__ - INFO - Deleting DSL Query Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: dsl-query-agent-us-west-2-533, id: UZ0ZCHE67L\n",
      "Deleting aliases for agent UZ0ZCHE67L...\n",
      "Deleting alias AGKDHUAHJJ from agent UZ0ZCHE67L\n",
      "Deleting alias TSTALIASID from agent UZ0ZCHE67L\n",
      "Deleting agent: UZ0ZCHE67L...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_dsl-query-agent-us-west-2-533...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 19:00:19,581 - __main__ - INFO - Deleting Query Fixer Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: query-fixer-agent-us-west-2-533, id: BVZPDW8UGH\n",
      "Deleting aliases for agent BVZPDW8UGH...\n",
      "Deleting alias KCBBPUCEVM from agent BVZPDW8UGH\n",
      "Deleting alias TSTALIASID from agent BVZPDW8UGH\n",
      "Deleting agent: BVZPDW8UGH...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_query-fixer-agent-us-west-2-533...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 19:00:31,450 - __main__ - INFO - Deleting KB Response Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: kb-response-agent-us-west-2-533, id: CRL3Q8ZW0K\n",
      "Deleting aliases for agent CRL3Q8ZW0K...\n",
      "Deleting alias J6KX4IIFJQ from agent CRL3Q8ZW0K\n",
      "Deleting alias TSTALIASID from agent CRL3Q8ZW0K\n",
      "Deleting agent: CRL3Q8ZW0K...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_kb-response-agent-us-west-2-533...\n"
     ]
    }
   ],
   "source": [
    "# # -------------------------------------------------------------------------\n",
    "# # 8. Cleanup: Delete the created agents\n",
    "# # -------------------------------------------------------------------------\n",
    "# logger.info(\"Deleting Supervisor Agent...\")\n",
    "# agents_helper.delete_agent(supervisor_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting DSL Query Agent...\")\n",
    "# agents_helper.delete_agent(dsl_query_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting Query Fixer Agent...\")\n",
    "# agents_helper.delete_agent(query_fixer_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting KB Response Agent...\")\n",
    "# agents_helper.delete_agent(kb_rag_agent.name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
