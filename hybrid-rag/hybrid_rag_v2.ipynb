{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Collaboration - DSL query use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import subprocess\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configure Logging\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Bedrock Agent utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 16:04:11,907 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 version: 1.36.9\n"
     ]
    }
   ],
   "source": [
    "from src.utils.bedrock_agent import Agent, SupervisorAgent, agents_helper, region, account_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AWS clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 16:04:13,271 - __main__ - INFO - Region: us-west-2\n",
      "2025-02-07 16:04:13,272 - __main__ - INFO - Account ID: 533267284022\n",
      "2025-02-07 16:04:13,272 - __main__ - INFO - Agent Suffix: us-west-2-533\n"
     ]
    }
   ],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = session.region_name\n",
    "account_id_suffix = account_id[:3]\n",
    "agent_suffix = f\"{region}-{account_id_suffix}\"\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n",
    "iam_client = boto3.client('iam', region_name=region)\n",
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "\n",
    "logger.info(f\"Region: {region}\")\n",
    "logger.info(f\"Account ID: {account_id}\")\n",
    "logger.info(f\"Agent Suffix: {agent_suffix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iam_role(role_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates or retrieves an IAM Role with the necessary trust policy for Lambda.\n",
    "    Attaches AWSLambdaBasicExecutionRole, and adds inline policies for OpenSearch \n",
    "    and AOSS access.\n",
    "\n",
    "    :param role_name: Name of the IAM Role to create or retrieve.\n",
    "    :return: ARN of the created or retrieved IAM Role.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating or retrieving IAM Role: {role_name}\")\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        role = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    "        )\n",
    "        logger.info(f\"IAM Role {role_name} created.\")\n",
    "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "        logger.info(f\"IAM Role {role_name} already exists. Retrieving existing role.\")\n",
    "        role = iam_client.get_role(RoleName=role_name)\n",
    "\n",
    "    # Attach AWS Lambda execution policy\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "    )\n",
    "    logger.info(f\"Attached AWSLambdaBasicExecutionRole to {role_name}.\")\n",
    "\n",
    "    # Attach additional policies for OpenSearch access\n",
    "    opensearch_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"es:Describe*\",\n",
    "                    \"es:List*\",\n",
    "                    \"es:Get*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    opensearch_policy_name = f\"{role_name}-OpenSearchPolicy\"\n",
    "    try:\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=opensearch_policy_name,\n",
    "            PolicyDocument=json.dumps(opensearch_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Attached OpenSearch policy to IAM Role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to attach OpenSearch policy to IAM Role {role_name}: {str(e)}\")\n",
    "\n",
    "    # Attach the new policy for aoss:APICall\n",
    "    aoss_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"aoss:*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    aoss_policy_name = f\"{role_name}-AOSSPolicy\"\n",
    "    try:\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=aoss_policy_name,\n",
    "            PolicyDocument=json.dumps(aoss_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Attached AOSS policy to IAM Role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to attach AOSS policy to IAM Role {role_name}: {str(e)}\")\n",
    "\n",
    "    role_arn = role['Role']['Arn']\n",
    "\n",
    "    # Wait for IAM role to propagate\n",
    "    logger.info(\"Waiting 10 seconds for IAM role to propagate...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    return role_arn\n",
    "\n",
    "\n",
    "def create_lambda_package(source_file: str, zip_file_path: str, dependencies: list):\n",
    "    \"\"\"\n",
    "    Packages a Lambda function and its dependencies into a single ZIP file.\n",
    "\n",
    "    :param source_file: Path to the Lambda function source code.\n",
    "    :param zip_file_path: Path to the ZIP file that will be created.\n",
    "    :param dependencies: A list of Python packages required by the Lambda.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Packaging Lambda function from {source_file} with dependencies {dependencies}\")\n",
    "    package_dir = \"package\"\n",
    "\n",
    "    # Install dependencies to a local folder\n",
    "    if not os.path.exists(package_dir):\n",
    "        os.makedirs(package_dir)\n",
    "    logger.info(\"Installing dependencies locally...\")\n",
    "    subprocess.run(\n",
    "        f\"pip install {' '.join(dependencies)} -t {package_dir}\",\n",
    "        shell=True,\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    # Create ZIP file with dependencies and function\n",
    "    logger.info(f\"Creating Lambda deployment package: {zip_file_path}\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
    "        # Add dependencies\n",
    "        for root, _, files in os.walk(package_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, package_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add the Lambda function code\n",
    "        zipf.write(source_file, os.path.basename(source_file))\n",
    "\n",
    "    # Cleanup temporary package directory\n",
    "    logger.info(\"Cleaning up temporary package directory...\")\n",
    "    subprocess.run(f\"rm -rf {package_dir}\", shell=True)\n",
    "    logger.info(\"Lambda package created successfully.\")\n",
    "\n",
    "\n",
    "def create_lambda_function(function_name: str,\n",
    "                           role_arn: str,\n",
    "                           handler: str,\n",
    "                           runtime: str,\n",
    "                           zip_file_path: str,\n",
    "                           region_name: str = region) -> dict:\n",
    "    \"\"\"\n",
    "    Creates or updates an AWS Lambda function.\n",
    "\n",
    "    :param function_name: Name of the Lambda function to create or update.\n",
    "    :param role_arn: ARN of the IAM Role that Lambda will assume.\n",
    "    :param handler: The function handler (e.g., 'index.lambda_handler').\n",
    "    :param runtime: The Lambda runtime (e.g., 'python3.12').\n",
    "    :param zip_file_path: Path to the ZIP file containing the Lambda code.\n",
    "    :param region_name: AWS region where the Lambda will be created.\n",
    "    :return: The response from the create_function or update_function_code API call.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating/updating Lambda function: {function_name}\")\n",
    "    lambda_client = boto3.client('lambda', region_name=region_name)\n",
    "\n",
    "    with open(zip_file_path, 'rb') as f:\n",
    "        zip_content = f.read()\n",
    "\n",
    "    try:\n",
    "        response = lambda_client.create_function(\n",
    "            FunctionName=function_name,\n",
    "            Runtime=runtime,\n",
    "            Role=role_arn,\n",
    "            Handler=handler,\n",
    "            Code={'ZipFile': zip_content},\n",
    "            Description='Lambda function to execute DSL queries',\n",
    "            Timeout=15,\n",
    "            MemorySize=128,\n",
    "            Publish=True\n",
    "        )\n",
    "        logger.info(f\"Lambda function {function_name} created successfully.\")\n",
    "    except lambda_client.exceptions.ResourceConflictException:\n",
    "        logger.info(f\"Lambda function {function_name} already exists. Updating its code...\")\n",
    "        response = lambda_client.update_function_code(\n",
    "            FunctionName=function_name,\n",
    "            ZipFile=zip_content\n",
    "        )\n",
    "        logger.info(f\"Lambda function {function_name} updated successfully.\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def add_resource_based_policy(function_name: str,\n",
    "                              agent_ids: list,\n",
    "                              region_name: str,\n",
    "                              account_id: str):\n",
    "    \"\"\"\n",
    "    Adds a resource-based policy to the specified Lambda function to allow invocation\n",
    "    from one or more Bedrock agents.\n",
    "\n",
    "    :param function_name: Name of the Lambda function.\n",
    "    :param agent_ids: List of agent IDs permitted to invoke this Lambda.\n",
    "    :param region_name: AWS region.\n",
    "    :param account_id: AWS account ID.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Adding resource-based policy to Lambda function {function_name} for agents: {agent_ids}\")\n",
    "    statement_id_prefix = \"AllowExecutionFromBedrockAgent\"\n",
    "    policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": []\n",
    "    }\n",
    "\n",
    "    for agent_id in agent_ids:\n",
    "        sid = f\"{statement_id_prefix}_{agent_id}\"\n",
    "        policy_doc['Statement'].append({\n",
    "            \"Sid\": sid,\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"lambda:InvokeFunction\",\n",
    "            \"Resource\": f\"arn:aws:lambda:{region_name}:{account_id}:function:{function_name}\",\n",
    "            \"Condition\": {\n",
    "                \"ArnLike\": {\n",
    "                    \"AWS:SourceArn\": f\"arn:aws:bedrock:{region_name}:{account_id}:agent/{agent_id}\"\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Retrieve existing policy and remove any existing statements with the same prefix\n",
    "    try:\n",
    "        existing_policy = lambda_client.get_policy(FunctionName=function_name)\n",
    "        existing_policy_doc = json.loads(existing_policy['Policy'])\n",
    "        for stmt in existing_policy_doc['Statement']:\n",
    "            if stmt['Sid'].startswith(statement_id_prefix):\n",
    "                sid_to_remove = stmt['Sid']\n",
    "                logger.info(f\"Removing existing statement: {sid_to_remove}\")\n",
    "                lambda_client.remove_permission(\n",
    "                    FunctionName=function_name,\n",
    "                    StatementId=sid_to_remove\n",
    "                )\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"No existing policy found for Lambda function {function_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving/removing existing policy for {function_name}: {str(e)}\")\n",
    "\n",
    "    # Add new permissions\n",
    "    for stmt in policy_doc['Statement']:\n",
    "        sid_val = stmt['Sid']\n",
    "        try:\n",
    "            lambda_client.add_permission(\n",
    "                FunctionName=function_name,\n",
    "                StatementId=sid_val,\n",
    "                Action=stmt['Action'],\n",
    "                Principal=stmt['Principal']['Service'],\n",
    "                SourceArn=stmt['Condition']['ArnLike']['AWS:SourceArn']\n",
    "            )\n",
    "            logger.info(f\"Added permission for statement: {sid_val}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to add resource-based policy for {function_name}, statement {sid_val}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shipping Schema \n",
    "with open('schemas/ecom_shipping_schema.json', 'r') as file:\n",
    "    ecom_shipping_schema = json.load(file)\n",
    "ecom_shipping_schema_string = json.dumps(ecom_shipping_schema, indent=2)\n",
    "\n",
    "# Agent foundation model \n",
    "agent_foundation_model = [\n",
    "    \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "]\n",
    "\n",
    "# Force re-create default setting for Agent objects, but for now set to False\n",
    "Agent.set_force_recreate_default(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 16:04:13,296 - __main__ - INFO - Creating or retrieving IAM Role: LambdaExecutionRole-us-west-2-533\n",
      "2025-02-07 16:04:13,643 - __main__ - INFO - IAM Role LambdaExecutionRole-us-west-2-533 already exists. Retrieving existing role.\n",
      "2025-02-07 16:04:13,886 - __main__ - INFO - Attached AWSLambdaBasicExecutionRole to LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-07 16:04:14,024 - __main__ - INFO - Attached OpenSearch policy to IAM Role LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-07 16:04:14,158 - __main__ - INFO - Attached AOSS policy to IAM Role LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-07 16:04:14,160 - __main__ - INFO - Waiting 10 seconds for IAM role to propagate...\n",
      "2025-02-07 16:04:24,165 - __main__ - INFO - Packaging Lambda function from src/lambda/execute_dsl_query.py with dependencies ['opensearch-py', 'requests', 'urllib3']\n",
      "2025-02-07 16:04:24,169 - __main__ - INFO - Installing dependencies locally...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py\n",
      "  Using cached opensearch_py-2.8.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-dateutil (from opensearch-py)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2024.07.04 (from opensearch-py)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Events (from opensearch-py)\n",
      "  Using cached Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting six>=1.5 (from python-dateutil->opensearch-py)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached opensearch_py-2.8.0-py3-none-any.whl (353 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: Events, urllib3, six, idna, charset-normalizer, certifi, requests, python-dateutil, opensearch-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.16.1 requires botocore<1.35.89,>=1.35.74, but you have botocore 1.36.9 which is incompatible.\n",
      "datasets 2.21.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "awscli 1.34.8 requires botocore==1.35.8, but you have botocore 1.36.9 which is incompatible.\n",
      "awscli 1.34.8 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-02-07 16:04:25,274 - __main__ - INFO - Creating Lambda deployment package: dsl_query_function.zip\n",
      "2025-02-07 16:04:25,319 - __main__ - INFO - Cleaning up temporary package directory...\n",
      "2025-02-07 16:04:25,352 - __main__ - INFO - Lambda package created successfully.\n",
      "2025-02-07 16:04:25,353 - __main__ - INFO - Creating/updating Lambda function: execute-dsl-query-us-west-2-533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Events-0.5 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 opensearch-py-2.8.0 python-dateutil-2.9.0.post0 requests-2.32.3 six-1.17.0 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 16:04:30,626 - __main__ - INFO - Lambda function execute-dsl-query-us-west-2-533 already exists. Updating its code...\n",
      "2025-02-07 16:04:32,366 - __main__ - INFO - Lambda function execute-dsl-query-us-west-2-533 updated successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main execution flow:\n",
    "    1. Create an IAM Role for Lambda.\n",
    "    2. Create/Update two Lambda functions (execute-dsl-query, execute-modified-dsl-query).\n",
    "    3. Create DSL Query Agent & Query Fixer Agent referencing those Lambda functions.\n",
    "    4. Retrieve the newly created agent IDs.\n",
    "    5. Add resource-based policies to each Lambda function for those agent IDs.\n",
    "    6. Create the Supervisor Agent to orchestrate both DSL Query and Query Fixer agents.\n",
    "    7. Invoke the Supervisor Agent with a sample query.\n",
    "    8. Delete the agents (cleanup).\n",
    "\"\"\"\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Create (or retrieve) IAM Role for Lambda\n",
    "# -------------------------------------------------------------------------\n",
    "IAM_ROLE_NAME = f\"LambdaExecutionRole-{agent_suffix}\"\n",
    "role_arn = create_iam_role(IAM_ROLE_NAME)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Create the first Lambda (execute-dsl-query)\n",
    "# -------------------------------------------------------------------------\n",
    "DSL_QUERY_LAMBDA_NAME = f\"execute-dsl-query-{agent_suffix}\"\n",
    "DSL_QUERY_LAMBDA_PATH = \"src/lambda/execute_dsl_query.py\"\n",
    "DSL_QUERY_ZIP_PATH = \"dsl_query_function.zip\"\n",
    "\n",
    "if not os.path.exists(DSL_QUERY_LAMBDA_PATH):\n",
    "    logger.error(f\"Error: {DSL_QUERY_LAMBDA_PATH} does not exist.\")\n",
    "\n",
    "DEPENDENCIES = [\"opensearch-py\", \"requests\", \"urllib3\"]\n",
    "\n",
    "# Package & create the Lambda\n",
    "create_lambda_package(DSL_QUERY_LAMBDA_PATH, DSL_QUERY_ZIP_PATH, DEPENDENCIES)\n",
    "create_lambda_function(\n",
    "    function_name=DSL_QUERY_LAMBDA_NAME,\n",
    "    role_arn=role_arn,\n",
    "    handler=\"execute_dsl_query.lambda_handler\",\n",
    "    runtime=\"python3.12\",\n",
    "    zip_file_path=DSL_QUERY_ZIP_PATH\n",
    ")\n",
    "os.remove(DSL_QUERY_ZIP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 16:04:32,380 - __main__ - INFO - Packaging Lambda function from src/lambda/execute_modified_dsl_query.py with dependencies ['opensearch-py', 'requests', 'urllib3']\n",
      "2025-02-07 16:04:32,381 - __main__ - INFO - Installing dependencies locally...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py\n",
      "  Using cached opensearch_py-2.8.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-dateutil (from opensearch-py)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2024.07.04 (from opensearch-py)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Events (from opensearch-py)\n",
      "  Using cached Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting six>=1.5 (from python-dateutil->opensearch-py)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached opensearch_py-2.8.0-py3-none-any.whl (353 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: Events, urllib3, six, idna, charset-normalizer, certifi, requests, python-dateutil, opensearch-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.16.1 requires botocore<1.35.89,>=1.35.74, but you have botocore 1.36.9 which is incompatible.\n",
      "datasets 2.21.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "awscli 1.34.8 requires botocore==1.35.8, but you have botocore 1.36.9 which is incompatible.\n",
      "awscli 1.34.8 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-02-07 16:04:33,317 - __main__ - INFO - Creating Lambda deployment package: modified_query_function.zip\n",
      "2025-02-07 16:04:33,358 - __main__ - INFO - Cleaning up temporary package directory...\n",
      "2025-02-07 16:04:33,395 - __main__ - INFO - Lambda package created successfully.\n",
      "2025-02-07 16:04:33,396 - __main__ - INFO - Creating/updating Lambda function: execute-modified-dsl-query-us-west-2-533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Events-0.5 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 opensearch-py-2.8.0 python-dateutil-2.9.0.post0 requests-2.32.3 six-1.17.0 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 16:04:35,448 - __main__ - INFO - Lambda function execute-modified-dsl-query-us-west-2-533 already exists. Updating its code...\n",
      "2025-02-07 16:04:37,075 - __main__ - INFO - Lambda function execute-modified-dsl-query-us-west-2-533 updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2(b). Create the second Lambda (execute-modified-dsl-query)\n",
    "# -------------------------------------------------------------------------\n",
    "MODIFIED_QUERY_LAMBDA_NAME = f\"execute-modified-dsl-query-{agent_suffix}\"\n",
    "MODIFIED_QUERY_LAMBDA_PATH = \"src/lambda/execute_modified_dsl_query.py\"\n",
    "MODIFIED_QUERY_ZIP_PATH = \"modified_query_function.zip\"\n",
    "\n",
    "if not os.path.exists(MODIFIED_QUERY_LAMBDA_PATH):\n",
    "    logger.error(f\"Error: {MODIFIED_QUERY_LAMBDA_PATH} does not exist.\")\n",
    "\n",
    "create_lambda_package(MODIFIED_QUERY_LAMBDA_PATH, MODIFIED_QUERY_ZIP_PATH, DEPENDENCIES)\n",
    "create_lambda_function(\n",
    "    function_name=MODIFIED_QUERY_LAMBDA_NAME,\n",
    "    role_arn=role_arn,\n",
    "    handler=\"execute_modified_dsl_query.lambda_handler\",\n",
    "    runtime=\"python3.12\",\n",
    "    zip_file_path=MODIFIED_QUERY_ZIP_PATH\n",
    ")\n",
    "os.remove(MODIFIED_QUERY_ZIP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 16:04:37,090 - __main__ - INFO - Creating DSL Query Agent...\n",
      "2025-02-07 16:04:37,610 - __main__ - INFO - Creating Query Fixer Agent...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Create the DSL Query Agent & Query Fixer Agent\n",
    "#\n",
    "#    Important: reference the just-created Lambda ARNs in \"tool_code\"\n",
    "#    The actual ARN is \"arn:aws:lambda:<REGION>:<ACCOUNT>:function:<FUNCTION_NAME>\"\n",
    "# -------------------------------------------------------------------------\n",
    "dsl_query_lambda_arn = f\"arn:aws:lambda:{region}:{account_id}:function:{DSL_QUERY_LAMBDA_NAME}\"\n",
    "modified_query_lambda_arn = f\"arn:aws:lambda:{region}:{account_id}:function:{MODIFIED_QUERY_LAMBDA_NAME}\"\n",
    "\n",
    "logger.info(\"Creating DSL Query Agent...\")\n",
    "dsl_query_agent = Agent.direct_create(\n",
    "    name=f\"dsl-query-agent-{agent_suffix}\",\n",
    "    role=\"DSL Query Creator\",\n",
    "    goal=\"Create DSL queries for a given user query\",\n",
    "    instructions=f\"\"\"\n",
    "    You are an expert in generating Query DSL for Elasticsearch-style queries. Your task is to convert a \n",
    "    given natural language user question into a well-structured Query DSL.\n",
    "    \n",
    "    ## Instructions:\n",
    "    - Use the provided e-commerce shipping schema to construct the query.\n",
    "    - Encapsulate the output in <json>...</json> tags.\n",
    "    - Follow the syntax of the Query DSL strictly; do not introduce elements outside the provided schema.\n",
    "    \n",
    "    ## Query Construction Rules:\n",
    "    - **Keyword fields** (carrier, status, country): Use `term` for exact matches or `prefix`/`wildcard` for partial matches.\n",
    "    - **Text fields** (description, address): Use `match` queries to account for analyzed terms.\n",
    "    - **Nested fields** (tracking): Always use `nested` queries.\n",
    "    - **Date fields**: Use `range` queries with date math for filtering by date ranges.\n",
    "    - **Aggregations**: When counting occurrences, use a 'terms' aggregation on the relevant keyword field to capture the exact values present (e.g., 'delivery.carrier').\n",
    "    - Break down complex queries into smaller parts for accuracy.\n",
    "    - Think step-by-step before constructing the query.\n",
    "\n",
    "\n",
    "    ## Schema:\n",
    "    {ecom_shipping_schema_string}\n",
    "\n",
    "    ## Output Format:\n",
    "    - Return only the generated Query DSL within <json>...</json> tags.\n",
    "    - Do not include explanations, comments, or additional text.\n",
    "    \"\"\",\n",
    "    tool_code=dsl_query_lambda_arn,\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"execute_dsl_query\",\n",
    "            \"description\": \"Executes a given DSL query and returns the results\",\n",
    "            \"parameters\": {\n",
    "                \"dsl_query\": {\n",
    "                    \"description\": \"The DSL query to execute\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(\"Creating Query Fixer Agent...\")\n",
    "query_fixer_agent = Agent.direct_create(\n",
    "    name=f\"query-fixer-agent-{agent_suffix}\",\n",
    "    role=\"Query Repair Specialist\",\n",
    "    goal=\"Fix and optimize failed DSL queries\",\n",
    "    instructions=f\"\"\"\n",
    "    You are an expert query debugger and optimizer. Your tasks are:\n",
    "    1. Analyze failed DSL queries from the query generator\n",
    "    2. Diagnose errors using OpenSearch error messages\n",
    "    3. Apply targeted fixes while maintaining original intent\n",
    "    4. Optimize queries for better recall when results are empty\n",
    "    5. Extract exact terms from 'terms' aggregations for accurate reporting.\n",
    "    6. Identify alternative ways to answer queries when direct fields are missing.\n",
    "    7. Recognize schema gaps and propose workarounds or schema modifications.\n",
    "\n",
    "    ## Repair Strategies:\n",
    "    - SYNTAX ERRORS: Fix formatting issues in nested queries/aggregations\n",
    "    - FIELD ERRORS: Map invalid fields to valid schema equivalents\n",
    "    - ZERO HITS: Apply query relaxation techniques:\n",
    "        * Add wildcards to keyword searches\n",
    "        * Expand date ranges\n",
    "        * Reduce strictness of term matches\n",
    "        * Add synonym expansion\n",
    "    - Ensure queries include 'terms' aggregations to capture exact values\n",
    "\n",
    "    ## Optimization Rules:\n",
    "    - Maintain original query structure where possible\n",
    "    - Prefer query-time fixes over rearchitecting\n",
    "    - Document all modifications in revision notes\n",
    "    - Limit query relaxation to 3 iterations\n",
    "    - When results are found, check the 'terms' aggregation for the exact field values.\n",
    "    - Report the exact terms from the data (e.g., use \"DHL Express\" if that's the stored value).\n",
    "\n",
    "    ## Schema Gap Analysis & Alternative Solutions:\n",
    "    - Creative Field Mapping: If direct fields are missing, use existing fields to infer answers.\n",
    "      * Example: If \"delivery duration\" is not available, compute it using `delivered_time - out_for_delivery_time`.\n",
    "    - Schema Enhancement: Identify missing fields required for full query support.\n",
    "    - Derived Data Solutions:\n",
    "      * If exact data isn’t available but can be computed, create scripted fields using painless scripting.\n",
    "      * If no alternative exists, clearly state the required data and suggest schema modifications.\n",
    "\n",
    "\n",
    "\n",
    "    ## Schema:\n",
    "    {ecom_shipping_schema_string}\n",
    "\n",
    "    ## Output Format:\n",
    "    - Return modified query in <json> tags\n",
    "    - Include revision notes and exact terms from aggregations in <notes> tags \n",
    "    \"\"\",\n",
    "    tool_code=modified_query_lambda_arn,\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"retry_query\",\n",
    "            \"description\": \"Retries a modified version of the failed query\",\n",
    "            \"parameters\": {\n",
    "                \"modified_dsl_query\": {\n",
    "                    \"description\": \"The corrected DSL query\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True\n",
    "                },\n",
    "                \"revision_notes\": {\n",
    "                    \"description\": \"Description of modifications made\",\n",
    "                    \"type\": \"string\",\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "kb_rag_agent = Agent.direct_create(\n",
    "    name=f\"kb-response-agent-{agent_suffix}\",\n",
    "    role=\"Knowledge Base Content Analyzer\",\n",
    "    goal=\"Analyze retrieved document content and generate well-structured responses\",\n",
    "    instructions=\"\"\"\n",
    "    You analyze pre-retrieved document content and generate clear, accurate responses.\n",
    "\n",
    "    ## Response Rules:\n",
    "    - Synthesize information from provided passages\n",
    "    - Include relevant quotes with proper citations\n",
    "    - Use consistent citation format [doc_id:para_num]\n",
    "    - Maintain factual accuracy\n",
    "    - Flag any inconsistencies between sources\n",
    "\n",
    "    ## Output Format:\n",
    "    Response should be structured as:\n",
    "    1. Direct answer\n",
    "    2. Supporting evidence\n",
    "    3. Source citations\n",
    "    4. Confidence level (High/Medium/Low)\n",
    "\n",
    "    ## Quality Guidelines:\n",
    "    - Prefer direct quotes for key information\n",
    "    - Summarize when appropriate\n",
    "    - Note any information gaps\n",
    "    - Maintain neutral tone\n",
    "    \"\"\",\n",
    "    kb_descr=\"Use knowledge base to extract relevant information, analyze content across multiple documents, and generate accurate responses with proper citations. Focus on maintaining context and factual accuracy.\",\n",
    "    kb_id=\"5GADU65GNF\",\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 16:04:38,420 - __main__ - INFO - Retrieving DSL Query Agent ID...\n",
      "2025-02-07 16:04:38,514 - __main__ - INFO - DSL Query Agent ID: CSKABIRKLE\n",
      "2025-02-07 16:04:38,515 - __main__ - INFO - Retrieving Query Fixer Agent ID...\n",
      "2025-02-07 16:04:38,602 - __main__ - INFO - Query Fixer Agent ID: SGSWZUVNOB\n",
      "2025-02-07 16:04:38,603 - __main__ - INFO - Retrieving KB Response Agent ID...\n",
      "2025-02-07 16:04:38,688 - __main__ - INFO - KB Response Agent ID: PM73T3Y9BR\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Retrieve the newly created Agent IDs\n",
    "# -------------------------------------------------------------------------\n",
    "logger.info(\"Retrieving DSL Query Agent ID...\")\n",
    "dsl_query_agent_id = agents_helper.get_agent_id_by_name(dsl_query_agent.name)\n",
    "logger.info(f\"DSL Query Agent ID: {dsl_query_agent_id}\")\n",
    "\n",
    "logger.info(\"Retrieving Query Fixer Agent ID...\")\n",
    "query_fixer_agent_id = agents_helper.get_agent_id_by_name(query_fixer_agent.name)\n",
    "logger.info(f\"Query Fixer Agent ID: {query_fixer_agent_id}\")\n",
    "\n",
    "logger.info(\"Retrieving KB Response Agent ID...\")\n",
    "kb_rag_agent_id = agents_helper.get_agent_id_by_name(kb_rag_agent.name)\n",
    "logger.info(f\"KB Response Agent ID: {kb_rag_agent_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 16:04:38,700 - __main__ - INFO - Adding resource-based policy to Lambda function execute-dsl-query-us-west-2-533 for agents: ['CSKABIRKLE']\n",
      "2025-02-07 16:04:38,824 - __main__ - INFO - Removing existing statement: AllowExecutionFromBedrockAgent_CSKABIRKLE\n",
      "2025-02-07 16:04:38,979 - __main__ - INFO - Added permission for statement: AllowExecutionFromBedrockAgent_CSKABIRKLE\n",
      "2025-02-07 16:04:38,980 - __main__ - INFO - Adding resource-based policy to Lambda function execute-modified-dsl-query-us-west-2-533 for agents: ['SGSWZUVNOB']\n",
      "2025-02-07 16:04:39,025 - __main__ - INFO - Removing existing statement: AllowExecutionFromBedrockAgent_SGSWZUVNOB\n",
      "2025-02-07 16:04:39,179 - __main__ - INFO - Added permission for statement: AllowExecutionFromBedrockAgent_SGSWZUVNOB\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 5. Add resource-based policy to each Lambda so the Agents can invoke them\n",
    "# -------------------------------------------------------------------------\n",
    "add_resource_based_policy(DSL_QUERY_LAMBDA_NAME, [dsl_query_agent_id], region, account_id)\n",
    "add_resource_based_policy(MODIFIED_QUERY_LAMBDA_NAME, [query_fixer_agent_id], region, account_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_agent = SupervisorAgent.direct_create(\n",
    "    name=f\"supervisor-agent-{agent_suffix}\",\n",
    "    role=\"Query Pipeline Orchestrator\",\n",
    "    collaboration_type=\"SUPERVISOR\",\n",
    "    collaborator_objects=[dsl_query_agent, query_fixer_agent, kb_rag_agent],\n",
    "    collaborator_agents=[\n",
    "        {\n",
    "            \"agent\": dsl_query_agent.name,\n",
    "            \"instructions\": dedent(f\"\"\"\n",
    "                {dsl_query_agent.name} is responsible for generating the primary DSL query based on \n",
    "                the provided e-commerce shipping schema. Your task is to produce a precise Query DSL \n",
    "                encapsulated in <json>...</json> tags. Ensure the query strictly follows the schema \n",
    "                and DSL syntax without any additional commentary or explanations.\n",
    "            \"\"\").strip(),\n",
    "            \"relay_conversation_history\": \"TO_COLLABORATOR\"\n",
    "        },\n",
    "        {\n",
    "            \"agent\": query_fixer_agent.name,\n",
    "            \"instructions\": dedent(f\"\"\"\n",
    "                Engage {query_fixer_agent.name} when any of the following conditions occur:\n",
    "                1. The DSL query returns syntax or validation errors.\n",
    "                2. The DSL query execution returns zero hits.\n",
    "                3. The query requires optimization for improved recall.\n",
    "                4. Alternative query solutions are needed due to schema limitations.\n",
    "                \n",
    "                Responsibilities:\n",
    "                - Analyze error messages and the current query structure.\n",
    "                - Apply targeted fixes that preserve the original query intent.\n",
    "                - Implement query relaxation techniques (for example, adding wildcards, extending date ranges, or expanding term matches).\n",
    "                - Identify and map alternative fields if direct schema fields are missing.\n",
    "                - Suggest schema enhancements when appropriate.\n",
    "                - Document all modifications with clear revision notes and output exact terms from aggregations.\n",
    "                \n",
    "                Return the corrected DSL query within <json>...</json> tags and include any revision notes within <notes>...</notes> tags.\n",
    "            \"\"\"),\n",
    "            \"relay_conversation_history\": \"TO_COLLABORATOR\"\n",
    "        },\n",
    "        {\n",
    "            \"agent\": kb_rag_agent.name,\n",
    "            \"instructions\": dedent(f\"\"\"\n",
    "                Engage {kb_rag_agent.name} to answer user questions that require analyzing the document content retrieved from executed queries.\n",
    "                When search results are available, your task is to:\n",
    "                \n",
    "                1. Synthesize and validate the information from the provided passages.\n",
    "                2. Generate a final response that includes a direct answer and supporting evidence with relevant quotes and citations.\n",
    "                \n",
    "                Your output must be clear, well-structured, and factually accurate to support decision-making.\n",
    "            \"\"\"),\n",
    "            \"relay_conversation_history\": \"TO_COLLABORATOR\"\n",
    "        }\n",
    "    ],\n",
    "    instructions=dedent(f\"\"\"\n",
    "        High-Level Overview:\n",
    "        The supervisor agent routes user queries to the appropriate agent based on the type of answer required:\n",
    "          - Structured Data Retrieval: If the query requires retrieving structured information from the e-commerce shipping data, it is routed to {dsl_query_agent.name}. If the DSL query returns errors or zero hits, the query is immediately routed to the Query Fixer Agent for reattempts.\n",
    "          - Document Content Analysis: If the query requires a synthesized, in-depth answer derived from analyzing executed query results, it is routed to {kb_rag_agent.name}.\n",
    "        \n",
    "        Detailed Instructions:\n",
    "        \n",
    "        Route A: Structured Data Retrieval (DSL Query Agent + Query Fixer Agent)\n",
    "        1. Initial Query Analysis:\n",
    "           - Receive the user's natural language query.\n",
    "           - Determine if the query requires structured data retrieval from the e-commerce shipping data.\n",
    "           - Validate the query against the provided schema:\n",
    "             {ecom_shipping_schema_string}\n",
    "           - If the query qualifies, route it to {dsl_query_agent.name}.\n",
    "        \n",
    "        2. DSL Query Execution:\n",
    "           - {dsl_query_agent.name} generates a Query DSL that is encapsulated in <json>...</json> tags and follows the provided schema.\n",
    "        \n",
    "        3. Error Handling & Retry:\n",
    "           - Monitor the query execution results:\n",
    "             a. If the DSL query returns syntax or validation errors, or if the result is zero hits, capture the error context.\n",
    "             b. Immediately route the query, along with diagnostic details, to {query_fixer_agent.name}.\n",
    "             c. {query_fixer_agent.name} applies targeted fixes and query relaxation techniques, then returns a modified DSL query.\n",
    "             d. Validate the modified query; allow up to 3 retry attempts if necessary.\n",
    "        \n",
    "        4. Evaluation & Final Approval (for structured data queries):\n",
    "           - Confirm that the final DSL query adheres to best practices (for example, proper nested queries, correct field types and mappings).\n",
    "           - Maintain an audit trail of all query versions and modifications.\n",
    "           - Generate an execution summary including:\n",
    "             - Query versions attempted.\n",
    "             - Reasons for modifications.\n",
    "             - Performance metrics.\n",
    "        \n",
    "        General Aggregation Guidance:\n",
    "           - If an aggregation returns an unexpectedly inflated count, verify whether the aggregation is counting nested or repeated values.\n",
    "           - To accurately count unique items, use a cardinality aggregation on a unique field identifier rather than aggregating on fields that might contain duplicate entries.\n",
    "        \n",
    "        Route B: Document Content Analysis (KB Response Agent)\n",
    "        1. Initial Query Analysis:\n",
    "           - Receive the user's natural language query.\n",
    "           - Determine if the query requires synthesizing and analyzing document content from executed queries.\n",
    "           - If so, route the query to {kb_rag_agent.name}.\n",
    "        \n",
    "        2. KB Response Generation:\n",
    "           - {kb_rag_agent.name} synthesizes and validates the information from the provided passages.\n",
    "           - Generate a final response that includes:\n",
    "             a. A direct answer.\n",
    "             b. Supporting evidence with relevant quotes and citations.\n",
    "             c. A confidence level.\n",
    "        \n",
    "        3. Final Response:\n",
    "           - Deliver the final, well-structured answer to the user.\n",
    "    \"\"\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect attributes of the SupervisorAgent object\n",
    "# print(dir(supervisor_agent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 16:04:39,524 - __main__ - INFO - Retrieving Supervisor Agent ID...\n",
      "2025-02-07 16:04:39,606 - __main__ - INFO - Supervisor Agent ID: QDMOLJO9TV\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the Supervisor Agent ID\n",
    "logger.info(\"Retrieving Supervisor Agent ID...\")\n",
    "supervisor_agent_id = agents_helper.get_agent_id_by_name(supervisor_agent.name)\n",
    "logger.info(f\"Supervisor Agent ID: {supervisor_agent_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the agent based on the ID\n",
    "# supervisor_agentV2 = agents_helper.get_agent_by_id(\"KGTOVCVLKI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invokeAgent API input parameters: input_text: How many orders have been shipped by DHL?, agent_id: QDMOLJO9TV, agent_alias_id: LXJCIDHVKO, session_id: 12345, session_state: {}, enable_trace: True, end_session: False, trace_level: all, multi_agent_names: {'CSKABIRKLE/V6WM6PSNHT': 'dsl-query-agent-us-west-2-533', 'SGSWZUVNOB/7KNLAUFPQC': 'query-fixer-agent-us-west-2-533', 'PM73T3Y9BR/GHH3JVSG1F': 'kb-response-agent-us-west-2-533', 'QDMOLJO9TV/LXJCIDHVKO': 'supervisor-agent-us-west-2-533'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'BufferedReader' instances",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 7. Invoke the Supervisor Agent with a sample query\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msupervisor_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow many orders have been shipped by DHL?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m12345\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_trace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrace_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupervisor agent response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Code/agentic-rag-research/agentic-rag-research/hybrid-rag/src/utils/bedrock_agent.py:825\u001b[0m, in \u001b[0;36mSupervisorAgent.invoke\u001b[0;34m(self, input_text, session_id, enable_trace, trace_level, session_state, multi_agent_names)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_agent_names \u001b[38;5;241m==\u001b[39m {}:\n\u001b[1;32m    824\u001b[0m     multi_agent_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_agent_names\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43magents_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupervisor_agent_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_alias_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupervisor_agent_alias_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_trace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrace_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrace_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_agent_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_agent_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Code/agentic-rag-research/agentic-rag-research/hybrid-rag/src/utils/bedrock_agent_helper.py:1378\u001b[0m, in \u001b[0;36mAgentsForAmazonBedrock.invoke\u001b[0;34m(self, input_text, agent_id, agent_alias_id, session_id, session_state, enable_trace, end_session, trace_level, multi_agent_names)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;66;03m####################################################################\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;66;03m# --------- New Code to Dump the Agent Trace as JSON File --------- #\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m####################################################################\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# When trace is enabled and trace_level is 'all', dump the entire\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;66;03m# raw response (including event stream) to a JSON file named \"trace.json\".\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable_trace \u001b[38;5;129;01mand\u001b[39;00m trace_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1378\u001b[0m     agent_resp_for_json \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_agent_resp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;66;03m# Convert any binary fields (like 'chunk'->'bytes' or 'files'->'bytes') to base64\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m agent_resp_for_json:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/copy.py:151\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    149\u001b[0m reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reductor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[43mreductor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'BufferedReader' instances"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 7. Invoke the Supervisor Agent with a sample query\n",
    "# -------------------------------------------------------------------------\n",
    "response = supervisor_agent.invoke(\n",
    "    input_text=\"How many orders have been shipped by DHL?\",\n",
    "    session_id=\"12345\",\n",
    "    enable_trace=True,\n",
    "    trace_level=\"all\"\n",
    ")\n",
    "logger.info(f\"Supervisor agent response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "invokeAgent API input parameters: input_text: How many orders have been shipped by DHL?, agent_id: QDMOLJO9TV, agent_alias_id: LXJCIDHVKO, session_id: 12345, session_state: {}, enable_trace: True, end_session: False, trace_level: all, multi_agent_names: {'CSKABIRKLE/V6WM6PSNHT': 'dsl-query-agent-us-west-2-533', 'SGSWZUVNOB/7KNLAUFPQC': 'query-fixer-agent-us-west-2-533', 'PM73T3Y9BR/GHH3JVSG1F': 'kb-response-agent-us-west-2-533', 'QDMOLJO9TV/LXJCIDHVKO': 'supervisor-agent-us-west-2-533'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SupervisorAgent' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m supervisor_agent_id \u001b[38;5;241m=\u001b[39m \u001b[43msupervisor_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SupervisorAgent' object has no attribute 'id'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if isinstance(response, dict):\n",
    "#     print(f\"Answer: {response['answer']}\")\n",
    "#     print(f\"Duration: {response['metadata']['duration_seconds']}s\")\n",
    "#     print(f\"Total tokens: {response['metadata']['total_tokens']}\")\n",
    "#     # Save trace data if needed\n",
    "#     with open('trace.json', 'w') as f:\n",
    "#         json.dump(response['trace'], f, indent=2)\n",
    "# else:\n",
    "#     print(response)  # Just print the answer for other trace_level values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = supervisor_agent.invoke(\n",
    "#     input_text=\"What are the effects of Covid-19 on e-commerce?\",\n",
    "#     session_id=\"1245\",\n",
    "#     enable_trace=True,\n",
    "#     trace_level=\"core\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = supervisor_agent.invoke(\n",
    "#     input_text=\"How many orders have recipients in Spain and were last updated during customs clearance after January 16, 2024?\",\n",
    "#     session_id=\"1245\",\n",
    "#     enable_trace=True,\n",
    "#     trace_level=\"core\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = supervisor_agent.invoke(\n",
    "#     input_text=\"What are the temperature controlled packages delivered within 2 hours?\",\n",
    "#     session_id=\"1245\",\n",
    "#     enable_trace=True,\n",
    "#     trace_level=\"core\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 11:40:14,603 - __main__ - INFO - Deleting Supervisor Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: supervisor-agent-us-west-2-533, id: SAQKI8DRDC\n",
      "Deleting aliases for agent SAQKI8DRDC...\n",
      "Deleting alias TSTALIASID from agent SAQKI8DRDC\n",
      "Deleting alias ZFSRCRHPRK from agent SAQKI8DRDC\n",
      "Deleting agent: SAQKI8DRDC...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_supervisor-agent-us-west-2-533...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 11:40:26,536 - __main__ - INFO - Deleting DSL Query Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: dsl-query-agent-us-west-2-533, id: X0A6ULQXO6\n",
      "Deleting aliases for agent X0A6ULQXO6...\n",
      "Deleting alias QH6TZZURIC from agent X0A6ULQXO6\n",
      "Deleting alias TSTALIASID from agent X0A6ULQXO6\n",
      "Deleting agent: X0A6ULQXO6...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_dsl-query-agent-us-west-2-533...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 11:40:38,335 - __main__ - INFO - Deleting Query Fixer Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: query-fixer-agent-us-west-2-533, id: B2LZE9MR7N\n",
      "Deleting aliases for agent B2LZE9MR7N...\n",
      "Deleting alias JHPIRWL1FP from agent B2LZE9MR7N\n",
      "Deleting alias TSTALIASID from agent B2LZE9MR7N\n",
      "Deleting agent: B2LZE9MR7N...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_query-fixer-agent-us-west-2-533...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 11:40:50,168 - __main__ - INFO - Deleting KB Response Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: kb-response-agent-us-west-2-533, id: TP0CKSXEJR\n",
      "Deleting aliases for agent TP0CKSXEJR...\n",
      "Deleting alias 7N0IKHH5UT from agent TP0CKSXEJR\n",
      "Deleting alias TSTALIASID from agent TP0CKSXEJR\n",
      "Deleting agent: TP0CKSXEJR...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_kb-response-agent-us-west-2-533...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 11:41:02,010 - __main__ - INFO - All agents deleted. Script completed.\n"
     ]
    }
   ],
   "source": [
    "# # -------------------------------------------------------------------------\n",
    "# # 8. Cleanup: Delete the created agents\n",
    "# # -------------------------------------------------------------------------\n",
    "# logger.info(\"Deleting Supervisor Agent...\")\n",
    "# agents_helper.delete_agent(supervisor_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting DSL Query Agent...\")\n",
    "# agents_helper.delete_agent(dsl_query_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting Query Fixer Agent...\")\n",
    "# agents_helper.delete_agent(query_fixer_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting KB Response Agent...\")\n",
    "# agents_helper.delete_agent(kb_rag_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"All agents deleted. Script completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
