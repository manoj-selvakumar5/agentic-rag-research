{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Collaboration - DSL query use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import subprocess\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configure Logging\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Bedrock Agent utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 08:59:43,516 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 version: 1.36.9\n"
     ]
    }
   ],
   "source": [
    "from src.utils.bedrock_agent import Agent, SupervisorAgent, agents_helper, region, account_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AWS clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 08:59:44,888 - __main__ - INFO - Region: us-west-2\n",
      "2025-02-02 08:59:44,888 - __main__ - INFO - Account ID: 533267284022\n",
      "2025-02-02 08:59:44,888 - __main__ - INFO - Agent Suffix: us-west-2-533\n"
     ]
    }
   ],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = session.region_name\n",
    "account_id_suffix = account_id[:3]\n",
    "agent_suffix = f\"{region}-{account_id_suffix}\"\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n",
    "iam_client = boto3.client('iam', region_name=region)\n",
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "\n",
    "logger.info(f\"Region: {region}\")\n",
    "logger.info(f\"Account ID: {account_id}\")\n",
    "logger.info(f\"Agent Suffix: {agent_suffix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iam_role(role_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates or retrieves an IAM Role with the necessary trust policy for Lambda.\n",
    "    Attaches AWSLambdaBasicExecutionRole, and adds inline policies for OpenSearch \n",
    "    and AOSS access.\n",
    "\n",
    "    :param role_name: Name of the IAM Role to create or retrieve.\n",
    "    :return: ARN of the created or retrieved IAM Role.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating or retrieving IAM Role: {role_name}\")\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        role = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    "        )\n",
    "        logger.info(f\"IAM Role {role_name} created.\")\n",
    "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "        logger.info(f\"IAM Role {role_name} already exists. Retrieving existing role.\")\n",
    "        role = iam_client.get_role(RoleName=role_name)\n",
    "\n",
    "    # Attach AWS Lambda execution policy\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "    )\n",
    "    logger.info(f\"Attached AWSLambdaBasicExecutionRole to {role_name}.\")\n",
    "\n",
    "    # Attach additional policies for OpenSearch access\n",
    "    opensearch_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"es:Describe*\",\n",
    "                    \"es:List*\",\n",
    "                    \"es:Get*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    opensearch_policy_name = f\"{role_name}-OpenSearchPolicy\"\n",
    "    try:\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=opensearch_policy_name,\n",
    "            PolicyDocument=json.dumps(opensearch_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Attached OpenSearch policy to IAM Role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to attach OpenSearch policy to IAM Role {role_name}: {str(e)}\")\n",
    "\n",
    "    # Attach the new policy for aoss:APICall\n",
    "    aoss_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"aoss:*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    aoss_policy_name = f\"{role_name}-AOSSPolicy\"\n",
    "    try:\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=aoss_policy_name,\n",
    "            PolicyDocument=json.dumps(aoss_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Attached AOSS policy to IAM Role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to attach AOSS policy to IAM Role {role_name}: {str(e)}\")\n",
    "\n",
    "    role_arn = role['Role']['Arn']\n",
    "\n",
    "    # Wait for IAM role to propagate\n",
    "    logger.info(\"Waiting 10 seconds for IAM role to propagate...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    return role_arn\n",
    "\n",
    "\n",
    "def create_lambda_package(source_file: str, zip_file_path: str, dependencies: list):\n",
    "    \"\"\"\n",
    "    Packages a Lambda function and its dependencies into a single ZIP file.\n",
    "\n",
    "    :param source_file: Path to the Lambda function source code.\n",
    "    :param zip_file_path: Path to the ZIP file that will be created.\n",
    "    :param dependencies: A list of Python packages required by the Lambda.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Packaging Lambda function from {source_file} with dependencies {dependencies}\")\n",
    "    package_dir = \"package\"\n",
    "\n",
    "    # Install dependencies to a local folder\n",
    "    if not os.path.exists(package_dir):\n",
    "        os.makedirs(package_dir)\n",
    "    logger.info(\"Installing dependencies locally...\")\n",
    "    subprocess.run(\n",
    "        f\"pip install {' '.join(dependencies)} -t {package_dir}\",\n",
    "        shell=True,\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    # Create ZIP file with dependencies and function\n",
    "    logger.info(f\"Creating Lambda deployment package: {zip_file_path}\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
    "        # Add dependencies\n",
    "        for root, _, files in os.walk(package_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, package_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add the Lambda function code\n",
    "        zipf.write(source_file, os.path.basename(source_file))\n",
    "\n",
    "    # Cleanup temporary package directory\n",
    "    logger.info(\"Cleaning up temporary package directory...\")\n",
    "    subprocess.run(f\"rm -rf {package_dir}\", shell=True)\n",
    "    logger.info(\"Lambda package created successfully.\")\n",
    "\n",
    "\n",
    "def create_lambda_function(function_name: str,\n",
    "                           role_arn: str,\n",
    "                           handler: str,\n",
    "                           runtime: str,\n",
    "                           zip_file_path: str,\n",
    "                           region_name: str = region) -> dict:\n",
    "    \"\"\"\n",
    "    Creates or updates an AWS Lambda function.\n",
    "\n",
    "    :param function_name: Name of the Lambda function to create or update.\n",
    "    :param role_arn: ARN of the IAM Role that Lambda will assume.\n",
    "    :param handler: The function handler (e.g., 'index.lambda_handler').\n",
    "    :param runtime: The Lambda runtime (e.g., 'python3.12').\n",
    "    :param zip_file_path: Path to the ZIP file containing the Lambda code.\n",
    "    :param region_name: AWS region where the Lambda will be created.\n",
    "    :return: The response from the create_function or update_function_code API call.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating/updating Lambda function: {function_name}\")\n",
    "    lambda_client = boto3.client('lambda', region_name=region_name)\n",
    "\n",
    "    with open(zip_file_path, 'rb') as f:\n",
    "        zip_content = f.read()\n",
    "\n",
    "    try:\n",
    "        response = lambda_client.create_function(\n",
    "            FunctionName=function_name,\n",
    "            Runtime=runtime,\n",
    "            Role=role_arn,\n",
    "            Handler=handler,\n",
    "            Code={'ZipFile': zip_content},\n",
    "            Description='Lambda function to execute DSL queries',\n",
    "            Timeout=15,\n",
    "            MemorySize=128,\n",
    "            Publish=True\n",
    "        )\n",
    "        logger.info(f\"Lambda function {function_name} created successfully.\")\n",
    "    except lambda_client.exceptions.ResourceConflictException:\n",
    "        logger.info(f\"Lambda function {function_name} already exists. Updating its code...\")\n",
    "        response = lambda_client.update_function_code(\n",
    "            FunctionName=function_name,\n",
    "            ZipFile=zip_content\n",
    "        )\n",
    "        logger.info(f\"Lambda function {function_name} updated successfully.\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def add_resource_based_policy(function_name: str,\n",
    "                              agent_ids: list,\n",
    "                              region_name: str,\n",
    "                              account_id: str):\n",
    "    \"\"\"\n",
    "    Adds a resource-based policy to the specified Lambda function to allow invocation\n",
    "    from one or more Bedrock agents.\n",
    "\n",
    "    :param function_name: Name of the Lambda function.\n",
    "    :param agent_ids: List of agent IDs permitted to invoke this Lambda.\n",
    "    :param region_name: AWS region.\n",
    "    :param account_id: AWS account ID.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Adding resource-based policy to Lambda function {function_name} for agents: {agent_ids}\")\n",
    "    statement_id_prefix = \"AllowExecutionFromBedrockAgent\"\n",
    "    policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": []\n",
    "    }\n",
    "\n",
    "    for agent_id in agent_ids:\n",
    "        sid = f\"{statement_id_prefix}_{agent_id}\"\n",
    "        policy_doc['Statement'].append({\n",
    "            \"Sid\": sid,\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"lambda:InvokeFunction\",\n",
    "            \"Resource\": f\"arn:aws:lambda:{region_name}:{account_id}:function:{function_name}\",\n",
    "            \"Condition\": {\n",
    "                \"ArnLike\": {\n",
    "                    \"AWS:SourceArn\": f\"arn:aws:bedrock:{region_name}:{account_id}:agent/{agent_id}\"\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Retrieve existing policy and remove any existing statements with the same prefix\n",
    "    try:\n",
    "        existing_policy = lambda_client.get_policy(FunctionName=function_name)\n",
    "        existing_policy_doc = json.loads(existing_policy['Policy'])\n",
    "        for stmt in existing_policy_doc['Statement']:\n",
    "            if stmt['Sid'].startswith(statement_id_prefix):\n",
    "                sid_to_remove = stmt['Sid']\n",
    "                logger.info(f\"Removing existing statement: {sid_to_remove}\")\n",
    "                lambda_client.remove_permission(\n",
    "                    FunctionName=function_name,\n",
    "                    StatementId=sid_to_remove\n",
    "                )\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"No existing policy found for Lambda function {function_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving/removing existing policy for {function_name}: {str(e)}\")\n",
    "\n",
    "    # Add new permissions\n",
    "    for stmt in policy_doc['Statement']:\n",
    "        sid_val = stmt['Sid']\n",
    "        try:\n",
    "            lambda_client.add_permission(\n",
    "                FunctionName=function_name,\n",
    "                StatementId=sid_val,\n",
    "                Action=stmt['Action'],\n",
    "                Principal=stmt['Principal']['Service'],\n",
    "                SourceArn=stmt['Condition']['ArnLike']['AWS:SourceArn']\n",
    "            )\n",
    "            logger.info(f\"Added permission for statement: {sid_val}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to add resource-based policy for {function_name}, statement {sid_val}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shipping Schema \n",
    "with open('schemas/ecom_shipping_schema.json', 'r') as file:\n",
    "    ecom_shipping_schema = json.load(file)\n",
    "ecom_shipping_schema_string = json.dumps(ecom_shipping_schema, indent=2)\n",
    "\n",
    "# Agent foundation model \n",
    "agent_foundation_model = [\n",
    "    \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "]\n",
    "\n",
    "# Force re-create default setting for Agent objects\n",
    "Agent.set_force_recreate_default(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 08:59:44,910 - __main__ - INFO - Creating or retrieving IAM Role: LambdaExecutionRole-us-west-2-533\n",
      "2025-02-02 08:59:45,238 - __main__ - INFO - IAM Role LambdaExecutionRole-us-west-2-533 already exists. Retrieving existing role.\n",
      "2025-02-02 08:59:45,472 - __main__ - INFO - Attached AWSLambdaBasicExecutionRole to LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-02 08:59:45,588 - __main__ - INFO - Attached OpenSearch policy to IAM Role LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-02 08:59:45,718 - __main__ - INFO - Attached AOSS policy to IAM Role LambdaExecutionRole-us-west-2-533.\n",
      "2025-02-02 08:59:45,719 - __main__ - INFO - Waiting 10 seconds for IAM role to propagate...\n",
      "2025-02-02 08:59:55,725 - __main__ - INFO - Packaging Lambda function from src/lambda/execute_dsl_query.py with dependencies ['opensearch-py', 'requests', 'urllib3']\n",
      "2025-02-02 08:59:55,727 - __main__ - INFO - Installing dependencies locally...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py\n",
      "  Using cached opensearch_py-2.8.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-dateutil (from opensearch-py)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2024.07.04 (from opensearch-py)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Events (from opensearch-py)\n",
      "  Using cached Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting six>=1.5 (from python-dateutil->opensearch-py)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached opensearch_py-2.8.0-py3-none-any.whl (353 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: Events, urllib3, six, idna, charset-normalizer, certifi, requests, python-dateutil, opensearch-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.16.1 requires botocore<1.35.89,>=1.35.74, but you have botocore 1.36.9 which is incompatible.\n",
      "datasets 2.21.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "awscli 1.34.8 requires botocore==1.35.8, but you have botocore 1.36.9 which is incompatible.\n",
      "awscli 1.34.8 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-02-02 08:59:56,796 - __main__ - INFO - Creating Lambda deployment package: dsl_query_function.zip\n",
      "2025-02-02 08:59:56,839 - __main__ - INFO - Cleaning up temporary package directory...\n",
      "2025-02-02 08:59:56,871 - __main__ - INFO - Lambda package created successfully.\n",
      "2025-02-02 08:59:56,872 - __main__ - INFO - Creating/updating Lambda function: execute-dsl-query-us-west-2-533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Events-0.5 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 opensearch-py-2.8.0 python-dateutil-2.9.0.post0 requests-2.32.3 six-1.17.0 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 08:59:58,961 - __main__ - INFO - Lambda function execute-dsl-query-us-west-2-533 created successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main execution flow:\n",
    "    1. Create an IAM Role for Lambda.\n",
    "    2. Create/Update two Lambda functions (execute-dsl-query, execute-modified-dsl-query).\n",
    "    3. Create DSL Query Agent & Query Fixer Agent referencing those Lambda functions.\n",
    "    4. Retrieve the newly created agent IDs.\n",
    "    5. Add resource-based policies to each Lambda function for those agent IDs.\n",
    "    6. Create the Supervisor Agent to orchestrate both DSL Query and Query Fixer agents.\n",
    "    7. Invoke the Supervisor Agent with a sample query.\n",
    "    8. Delete the agents (cleanup).\n",
    "\"\"\"\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Create (or retrieve) IAM Role for Lambda\n",
    "# -------------------------------------------------------------------------\n",
    "IAM_ROLE_NAME = f\"LambdaExecutionRole-{agent_suffix}\"\n",
    "role_arn = create_iam_role(IAM_ROLE_NAME)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Create the first Lambda (execute-dsl-query)\n",
    "# -------------------------------------------------------------------------\n",
    "DSL_QUERY_LAMBDA_NAME = f\"execute-dsl-query-{agent_suffix}\"\n",
    "DSL_QUERY_LAMBDA_PATH = \"src/lambda/execute_dsl_query.py\"\n",
    "DSL_QUERY_ZIP_PATH = \"dsl_query_function.zip\"\n",
    "\n",
    "if not os.path.exists(DSL_QUERY_LAMBDA_PATH):\n",
    "    logger.error(f\"Error: {DSL_QUERY_LAMBDA_PATH} does not exist.\")\n",
    "\n",
    "DEPENDENCIES = [\"opensearch-py\", \"requests\", \"urllib3\"]\n",
    "\n",
    "# Package & create the Lambda\n",
    "create_lambda_package(DSL_QUERY_LAMBDA_PATH, DSL_QUERY_ZIP_PATH, DEPENDENCIES)\n",
    "create_lambda_function(\n",
    "    function_name=DSL_QUERY_LAMBDA_NAME,\n",
    "    role_arn=role_arn,\n",
    "    handler=\"execute_dsl_query.lambda_handler\",\n",
    "    runtime=\"python3.12\",\n",
    "    zip_file_path=DSL_QUERY_ZIP_PATH\n",
    ")\n",
    "os.remove(DSL_QUERY_ZIP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 08:59:58,972 - __main__ - INFO - Packaging Lambda function from src/lambda/execute_modified_dsl_query.py with dependencies ['opensearch-py', 'requests', 'urllib3']\n",
      "2025-02-02 08:59:58,973 - __main__ - INFO - Installing dependencies locally...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py\n",
      "  Using cached opensearch_py-2.8.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-dateutil (from opensearch-py)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2024.07.04 (from opensearch-py)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Events (from opensearch-py)\n",
      "  Using cached Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting six>=1.5 (from python-dateutil->opensearch-py)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached opensearch_py-2.8.0-py3-none-any.whl (353 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: Events, urllib3, six, idna, charset-normalizer, certifi, requests, python-dateutil, opensearch-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.16.1 requires botocore<1.35.89,>=1.35.74, but you have botocore 1.36.9 which is incompatible.\n",
      "datasets 2.21.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "awscli 1.34.8 requires botocore==1.35.8, but you have botocore 1.36.9 which is incompatible.\n",
      "awscli 1.34.8 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-02-02 08:59:59,925 - __main__ - INFO - Creating Lambda deployment package: modified_query_function.zip\n",
      "2025-02-02 08:59:59,968 - __main__ - INFO - Cleaning up temporary package directory...\n",
      "2025-02-02 09:00:00,002 - __main__ - INFO - Lambda package created successfully.\n",
      "2025-02-02 09:00:00,003 - __main__ - INFO - Creating/updating Lambda function: execute-modified-dsl-query-us-west-2-533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Events-0.5 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 opensearch-py-2.8.0 python-dateutil-2.9.0.post0 requests-2.32.3 six-1.17.0 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 09:00:02,067 - __main__ - INFO - Lambda function execute-modified-dsl-query-us-west-2-533 created successfully.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2(b). Create the second Lambda (execute-modified-dsl-query)\n",
    "# -------------------------------------------------------------------------\n",
    "MODIFIED_QUERY_LAMBDA_NAME = f\"execute-modified-dsl-query-{agent_suffix}\"\n",
    "MODIFIED_QUERY_LAMBDA_PATH = \"src/lambda/execute_modified_dsl_query.py\"\n",
    "MODIFIED_QUERY_ZIP_PATH = \"modified_query_function.zip\"\n",
    "\n",
    "if not os.path.exists(MODIFIED_QUERY_LAMBDA_PATH):\n",
    "    logger.error(f\"Error: {MODIFIED_QUERY_LAMBDA_PATH} does not exist.\")\n",
    "\n",
    "create_lambda_package(MODIFIED_QUERY_LAMBDA_PATH, MODIFIED_QUERY_ZIP_PATH, DEPENDENCIES)\n",
    "create_lambda_function(\n",
    "    function_name=MODIFIED_QUERY_LAMBDA_NAME,\n",
    "    role_arn=role_arn,\n",
    "    handler=\"execute_modified_dsl_query.lambda_handler\",\n",
    "    runtime=\"python3.12\",\n",
    "    zip_file_path=MODIFIED_QUERY_ZIP_PATH\n",
    ")\n",
    "os.remove(MODIFIED_QUERY_ZIP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 10:00:03,425 - __main__ - INFO - Creating DSL Query Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting existing agent and corresponding lambda for: dsl-query-agent-us-west-2-533...\n",
      "Agent dsl-query-agent-us-west-2-533 not found\n",
      "Creating agent dsl-query-agent-us-west-2-533...\n",
      "Created agent, id: QAAT9MMOLL, alias id: TSTALIASID\n",
      "\n",
      "Adding action group with Lambda: arn:aws:lambda:us-west-2:533267284022:function:execute-dsl-query-us-west-2-533...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id QAAT9MMOLL current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id QAAT9MMOLL current status: PREPARED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 10:00:31,310 - __main__ - INFO - Creating Query Fixer Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: Agent: dsl-query-agent-us-west-2-533, id: QAAT9MMOLL, alias id: HZUJPGFETV\n",
      "\n",
      "\n",
      "Deleting existing agent and corresponding lambda for: query-fixer-agent-us-west-2-533...\n",
      "Agent query-fixer-agent-us-west-2-533 not found\n",
      "Creating agent query-fixer-agent-us-west-2-533...\n",
      "Created agent, id: B4QGLWZPHF, alias id: TSTALIASID\n",
      "\n",
      "Adding action group with Lambda: arn:aws:lambda:us-west-2:533267284022:function:execute-modified-dsl-query-us-west-2-533...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id B4QGLWZPHF current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id B4QGLWZPHF current status: PREPARED\n",
      "DONE: Agent: query-fixer-agent-us-west-2-533, id: B4QGLWZPHF, alias id: DDFVRPIMJC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Create the DSL Query Agent & Query Fixer Agent\n",
    "#\n",
    "#    Important: reference the just-created Lambda ARNs in \"tool_code\"\n",
    "#    The actual ARN is \"arn:aws:lambda:<REGION>:<ACCOUNT>:function:<FUNCTION_NAME>\"\n",
    "# -------------------------------------------------------------------------\n",
    "dsl_query_lambda_arn = f\"arn:aws:lambda:{region}:{account_id}:function:{DSL_QUERY_LAMBDA_NAME}\"\n",
    "modified_query_lambda_arn = f\"arn:aws:lambda:{region}:{account_id}:function:{MODIFIED_QUERY_LAMBDA_NAME}\"\n",
    "\n",
    "logger.info(\"Creating DSL Query Agent...\")\n",
    "dsl_query_agent = Agent.direct_create(\n",
    "    name=f\"dsl-query-agent-{agent_suffix}\",\n",
    "    role=\"DSL Query Creator\",\n",
    "    goal=\"Create DSL queries for a given user query\",\n",
    "    instructions=f\"\"\"\n",
    "    You are an expert in generating Query DSL for Elasticsearch-style queries. Your task is to convert a \n",
    "    given natural language user question into a well-structured Query DSL.\n",
    "    \n",
    "    ## Instructions:\n",
    "    - Use the provided e-commerce shipping schema to construct the query.\n",
    "    - Encapsulate the output in <json>...</json> tags.\n",
    "    - Follow the syntax of the Query DSL strictly; do not introduce elements outside the provided schema.\n",
    "    \n",
    "    ## Query Construction Rules:\n",
    "    - **Keyword fields** (carrier, status, country): Use `term` for exact matches or `prefix`/`wildcard` for partial matches.\n",
    "    - **Text fields** (description, address): Use `match` queries to account for analyzed terms.\n",
    "    - **Nested fields** (tracking): Always use `nested` queries.\n",
    "    - **Date fields**: Use `range` queries with date math for filtering by date ranges.\n",
    "    - **Aggregations**: When counting occurrences, use a 'terms' aggregation on the relevant keyword field to capture the exact values present (e.g., 'delivery.carrier').\n",
    "    - Break down complex queries into smaller parts for accuracy.\n",
    "    - Think step-by-step before constructing the query.\n",
    "\n",
    "\n",
    "    ## Schema:\n",
    "    {ecom_shipping_schema_string}\n",
    "\n",
    "    ## Output Format:\n",
    "    - Return only the generated Query DSL within <json>...</json> tags.\n",
    "    - Do not include explanations, comments, or additional text.\n",
    "    \"\"\",\n",
    "    tool_code=dsl_query_lambda_arn,\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"execute_dsl_query\",\n",
    "            \"description\": \"Executes a given DSL query and returns the results\",\n",
    "            \"parameters\": {\n",
    "                \"dsl_query\": {\n",
    "                    \"description\": \"The DSL query to execute\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(\"Creating Query Fixer Agent...\")\n",
    "query_fixer_agent = Agent.direct_create(\n",
    "    name=f\"query-fixer-agent-{agent_suffix}\",\n",
    "    role=\"Query Repair Specialist\",\n",
    "    goal=\"Fix and optimize failed DSL queries\",\n",
    "    instructions=f\"\"\"\n",
    "    You are an expert query debugger and optimizer. Your tasks are:\n",
    "    1. Analyze failed DSL queries from the query generator\n",
    "    2. Diagnose errors using OpenSearch error messages\n",
    "    3. Apply targeted fixes while maintaining original intent\n",
    "    4. Optimize queries for better recall when results are empty\n",
    "    5. Extract exact terms from 'terms' aggregations for accurate reporting.\n",
    "    6. Identify alternative ways to answer queries when direct fields are missing.\n",
    "    7. Recognize schema gaps and propose workarounds or schema modifications.\n",
    "\n",
    "    ## Repair Strategies:\n",
    "    - SYNTAX ERRORS: Fix formatting issues in nested queries/aggregations\n",
    "    - FIELD ERRORS: Map invalid fields to valid schema equivalents\n",
    "    - ZERO HITS: Apply query relaxation techniques:\n",
    "        * Add wildcards to keyword searches\n",
    "        * Expand date ranges\n",
    "        * Reduce strictness of term matches\n",
    "        * Add synonym expansion\n",
    "    - Ensure queries include 'terms' aggregations to capture exact values\n",
    "\n",
    "    ## Optimization Rules:\n",
    "    - Maintain original query structure where possible\n",
    "    - Prefer query-time fixes over rearchitecting\n",
    "    - Document all modifications in revision notes\n",
    "    - Limit query relaxation to 3 iterations\n",
    "    - When results are found, check the 'terms' aggregation for the exact field values.\n",
    "    - Report the exact terms from the data (e.g., use \"DHL Express\" if that's the stored value).\n",
    "\n",
    "    ## Schema Gap Analysis & Alternative Solutions:\n",
    "    - Creative Field Mapping: If direct fields are missing, use existing fields to infer answers.\n",
    "      * Example: If \"delivery duration\" is not available, compute it using `delivered_time - out_for_delivery_time`.\n",
    "    - Schema Enhancement: Identify missing fields required for full query support.\n",
    "    - Derived Data Solutions:\n",
    "      * If exact data isn’t available but can be computed, create scripted fields using painless scripting.\n",
    "      * If no alternative exists, clearly state the required data and suggest schema modifications.\n",
    "\n",
    "\n",
    "\n",
    "    ## Schema:\n",
    "    {ecom_shipping_schema_string}\n",
    "\n",
    "    ## Output Format:\n",
    "    - Return modified query in <json> tags\n",
    "    - Include revision notes and exact terms from aggregations in <notes> tags \n",
    "    \"\"\",\n",
    "    tool_code=modified_query_lambda_arn,\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"retry_query\",\n",
    "            \"description\": \"Retries a modified version of the failed query\",\n",
    "            \"parameters\": {\n",
    "                \"modified_dsl_query\": {\n",
    "                    \"description\": \"The corrected DSL query\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True\n",
    "                },\n",
    "                \"revision_notes\": {\n",
    "                    \"description\": \"Description of modifications made\",\n",
    "                    \"type\": \"string\",\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 10:00:58,941 - __main__ - INFO - Retrieving DSL Query Agent ID...\n",
      "2025-02-02 10:00:59,017 - __main__ - INFO - DSL Query Agent ID: QAAT9MMOLL\n",
      "2025-02-02 10:00:59,017 - __main__ - INFO - Retrieving Query Fixer Agent ID...\n",
      "2025-02-02 10:00:59,094 - __main__ - INFO - Query Fixer Agent ID: B4QGLWZPHF\n",
      "2025-02-02 10:00:59,095 - __main__ - INFO - Adding resource-based policy to Lambda function execute-dsl-query-us-west-2-533 for agents: ['QAAT9MMOLL']\n",
      "2025-02-02 10:00:59,142 - __main__ - INFO - Removing existing statement: AllowExecutionFromBedrockAgent_X5ISUSR3EE\n",
      "2025-02-02 10:00:59,336 - __main__ - INFO - Added permission for statement: AllowExecutionFromBedrockAgent_QAAT9MMOLL\n",
      "2025-02-02 10:00:59,336 - __main__ - INFO - Adding resource-based policy to Lambda function execute-modified-dsl-query-us-west-2-533 for agents: ['B4QGLWZPHF']\n",
      "2025-02-02 10:00:59,384 - __main__ - INFO - Removing existing statement: AllowExecutionFromBedrockAgent_ECGRBY6PO0\n",
      "2025-02-02 10:00:59,567 - __main__ - INFO - Added permission for statement: AllowExecutionFromBedrockAgent_B4QGLWZPHF\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Retrieve the newly created Agent IDs\n",
    "# -------------------------------------------------------------------------\n",
    "logger.info(\"Retrieving DSL Query Agent ID...\")\n",
    "dsl_query_agent_id = agents_helper.get_agent_id_by_name(dsl_query_agent.name)\n",
    "logger.info(f\"DSL Query Agent ID: {dsl_query_agent_id}\")\n",
    "\n",
    "logger.info(\"Retrieving Query Fixer Agent ID...\")\n",
    "query_fixer_agent_id = agents_helper.get_agent_id_by_name(query_fixer_agent.name)\n",
    "logger.info(f\"Query Fixer Agent ID: {query_fixer_agent_id}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Add resource-based policy to each Lambda so the Agents can invoke them\n",
    "# -------------------------------------------------------------------------\n",
    "add_resource_based_policy(DSL_QUERY_LAMBDA_NAME, [dsl_query_agent_id], region, account_id)\n",
    "add_resource_based_policy(MODIFIED_QUERY_LAMBDA_NAME, [query_fixer_agent_id], region, account_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent supervisor-agent-us-west-2-533 not found\n",
      "\n",
      "Created supervisor, id: 0JATH64VCJ, alias id: TSTALIASID\n",
      "\n",
      "  associating sub-agents / collaborators to supervisor...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id 0JATH64VCJ current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id 0JATH64VCJ current status: PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id 0JATH64VCJ current status: PREPARED\n",
      "DONE: Agent: supervisor-agent-us-west-2-533, id: 0JATH64VCJ, alias id: ZU6C3G8PYS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supervisor_agent = SupervisorAgent.direct_create(\n",
    "    name=f\"supervisor-agent-{agent_suffix}\",\n",
    "    role=\"Query Pipeline Orchestrator\",\n",
    "    collaboration_type=\"SUPERVISOR\",\n",
    "    collaborator_objects=[dsl_query_agent, query_fixer_agent],\n",
    "    collaborator_agents=[\n",
    "        {\n",
    "            \"agent\": dsl_query_agent.name,\n",
    "            \"instructions\": \"Primary DSL query generation using the schema\",\n",
    "            \"relay_conversation_history\": \"DISABLED\"\n",
    "        },\n",
    "        {\n",
    "            \"agent\": query_fixer_agent.name,\n",
    "            \"instructions\": dedent(\"\"\"\n",
    "                Engage when:\n",
    "                1. DSL query returns errors (parsing/validation)\n",
    "                2. Search results are empty (zero hits)\n",
    "                3. Query needs optimization for better recall\n",
    "                4. Query requires alternative solutions due to schema limitations\n",
    "\n",
    "                Responsibilities:\n",
    "                - Analyze error messages and query structure\n",
    "                - Apply targeted fixes while preserving intent\n",
    "                - Implement query relaxation strategies\n",
    "                - Identify alternative field mappings when direct fields are missing\n",
    "                - Suggest schema enhancements where necessary\n",
    "                - Document modifications made\n",
    "            \"\"\"),\n",
    "            \"relay_conversation_history\": \"TO_COLLABORATOR\"\n",
    "        }\n",
    "    ],\n",
    "    instructions=dedent(f\"\"\"\n",
    "        ## Query Orchestration & Validation Workflow:\n",
    "\n",
    "        1. Initial Query Generation:\n",
    "        - Receive natural language query from user.\n",
    "        - Route to DSL Query Agent for initial construction.\n",
    "        - Validate query structure against schema:\n",
    "            {ecom_shipping_schema_string}\n",
    "\n",
    "        2. Error Handling & Retry:\n",
    "        - Monitor for query execution errors.\n",
    "        - If errors or zero results occur:\n",
    "            a. Capture error context and original query.\n",
    "            b. Route to Query Fixer Agent with full diagnostics.\n",
    "            c. Validate the fixer’s modified query.\n",
    "            d. Approve max 3 retry attempts.\n",
    "\n",
    "        3. Schema Limitations & Alternative Solutions:\n",
    "        - Missing Fields: Do not immediately reject queries due to missing fields.\n",
    "        - Workarounds: Task Query Fixer Agent with finding suitable field substitutions.\n",
    "        - Alternative Data Sources: Allow inferred results based on related fields.\n",
    "        - Scripted Fields: Use painless scripting where data derivation is possible.\n",
    "        - Schema Recommendations: Where workarounds are insufficient, propose schema enhancements.\n",
    "\n",
    "        4. Evaluation Criteria for Alternatives:\n",
    "        - Confidence Level: Assess the reliability of the workaround.\n",
    "        - Data Freshness: Ensure substitute fields are properly maintained.\n",
    "        - Approximation Warning: Clearly flag estimated or inferred results.\n",
    "\n",
    "        5. Quality Assurance:\n",
    "        - Ensure the final query meets best practices:\n",
    "            - Proper use of nested queries.\n",
    "            - Correct field types and mappings.\n",
    "            - Appropriate query strictness level.\n",
    "        - Maintain an audit trail of all query versions.\n",
    "        - Provide users with clear and simplified error explanations.\n",
    "\n",
    "        6. Final Approval:\n",
    "        - Sign off on valid queries.\n",
    "        - Block invalid queries with detailed feedback.\n",
    "        - Generate an execution summary including:\n",
    "            - Query versions attempted.\n",
    "            - Modification reasons.\n",
    "            - Performance metrics.\n",
    "\n",
    "        ## Schema-Based Response Formatting:\n",
    "        - If exact data is unavailable, provide structured fallback results. Example:\n",
    "          \"While we don’t have a direct ‘temperature control’ field, we found:\"\n",
    "            - 42 packages with 'REFRIGERATE' in the description.\n",
    "            - 18 shipments using cold chain logistics providers.\n",
    "            - (Data derived from: description field + carrier analysis.)\n",
    "        - Schema Recommendation: \"Consider adding 'temperature_requirements' to tracking data.\"\n",
    "    \"\"\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invokeAgent API request ID: 8ab9fd22-2712-4392-8e62-d5b8c4d60df9\n",
      "invokeAgent API session ID: 12345\n",
      "  agent id: BZW5ZYSZB8, agent alias id: 6KZEM29VOE\n",
      "\u001b[32m---- Step 1 ----\u001b[0m\n",
      "\u001b[33mTook 3.6s, using 2833 tokens (in: 2678, out: 155) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI need to help generate a DSL query to find orders shipped by DHL carrier. Let me first send this request to the DSL query agent to generate the appropriate query.\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'dsl-query-agent-us-west-2-533 [X5ISUSR3EE/QQR20YGSSC]' passing input text:\u001b[0m\n",
      "\u001b[35mPlease generate a DSL query to count shipping orders where delivery.carrier starts with \"DHL\" to capture all DHL variations.\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.1 [using sub-agent name:dsl-query-agent-us-west-2-533, id:X5ISUSR3EE/QQR20YGSSC] ----\u001b[0m\n",
      "\u001b[33mTook 9.7s, using 3221 tokens (in: 2701, out: 520) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mTo answer this question, I need to create a DSL query that counts the number of shipping orders where the carrier starts with \"DHL\". This will capture all variations of DHL carriers. I'll use the delivery.carrier field from the schema, and create a prefix query to match carriers starting with \"DHL\". I'll also use a value_count aggregation to count the number of matching documents.\n",
      "</thinking>\n",
      "\n",
      "<json>\n",
      "{\n",
      "  \"size\": 0,\n",
      "  \"query\": {\n",
      "    \"prefix\": {\n",
      "      \"delivery.carrier\": \"DHL\"\n",
      "    }\n",
      "  },\n",
      "  \"aggs\": {\n",
      "    \"dhl_variations_order_count\": {\n",
      "      \"value_count\": {\n",
      "        \"field\": \"order_id\"\n",
      "      }\n",
      "    },\n",
      "    \"dhl_carriers\": {\n",
      "      \"terms\": {\n",
      "        \"field\": \"delivery.carrier\",\n",
      "        \"include\": \"DHL.*\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "</json>\n",
      "\n",
      "<thinking>\n",
      "I've created a DSL query that uses a prefix query to match all carriers starting with \"DHL\". I've also added two aggregations:\n",
      "1. To count the total number of orders with DHL variations\n",
      "2. To show the different DHL carrier variations present in the data\n",
      "\n",
      "Now, I'll execute this query using the provided function to get the actual count and variations of DHL carriers.\u001b[0m\n",
      "\u001b[35mUsing tool: execute_dsl_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'dsl_query', 'type': 'string', 'value': '{\\n  \"size\": 0,\\n  \"query\": {\\n    \"prefix\": {\\n      \"delivery.carrier\": \"DHL\"\\n    }\\n  },\\n  \"aggs\": {\\n    \"dhl_variations_order_count\": {\\n      \"value_count\": {\\n        \"field\": \"order_id\"\\n      }\\n    },\\n    \"dhl_carriers\": {\\n      \"terms\": {\\n        \"field\": \"delivery.carrier\",\\n        \"include\": \"DHL.*\"\\n      }\\n    }\\n  }\\n}'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': {'took': 24, 'timed_out': False, '_shards': {'total': 0, 'successful': 0, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 1, 'relation': 'eq'}, 'max_score': None, 'hits': []}, 'aggregations': {'dhl_variations_order_count': {'value': 1}, 'dhl_carriers': {'doc_count_error_upp...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.2 [using sub-agent name:dsl-query-agent-us-west-2-533, id:X5ISUSR3EE/QQR20YGSSC] ----\u001b[0m\n",
      "\u001b[33mTook 5.6s, using 3527 tokens (in: 3403, out: 124) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI have received the results from executing the DSL query. Let's analyze the results:\n",
      "\n",
      "1. The total count of shipping orders where the carrier starts with \"DHL\" is 1.\n",
      "2. There is only one variation of DHL carrier present in the data: \"DHL Express\".\n",
      "\n",
      "Now, I'll provide a concise answer to the user's question.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "The count of shipping orders where the carrier starts with \"DHL\" is 1. The only DHL variation present in the data is \"DHL Express\"....\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent dsl-query-agent-us-west-2-533 output text:\n",
      "The count of shipping orders where the carrier starts with \"DHL\" is 1. The only DHL variation present in the data is \"DHL Express\"....\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 09:22:48,579 - __main__ - INFO - Supervisor agent response: There is 1 order that has been shipped by DHL (specifically DHL Express).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m---- Step 2 ----\u001b[0m\n",
      "\u001b[33mTook 3.8s, using 3005 tokens (in: 2877, out: 128) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI have received a clear response showing there is 1 DHL shipment in the system. Let me communicate this to the user.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "There is 1 order that has been shipped by DHL (specifically DHL Express)....\u001b[0m\n",
      "\u001b[33mAgent made a total of 4 LLM calls, using 12586 tokens (in: 11659, out: 927), and took 23.0 total seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 7. Invoke the Supervisor Agent with a sample query\n",
    "# -------------------------------------------------------------------------\n",
    "response = supervisor_agent.invoke(\n",
    "    input_text=\"How many orders have been shipped by DHL?\",\n",
    "    session_id=\"12345\",\n",
    "    enable_trace=True,\n",
    "    trace_level=\"core\"\n",
    ")\n",
    "logger.info(f\"Supervisor agent response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invokeAgent API request ID: f0536129-7d2f-4ee1-9757-244214752911\n",
      "invokeAgent API session ID: 1245\n",
      "  agent id: BZW5ZYSZB8, agent alias id: 6KZEM29VOE\n",
      "\u001b[32m---- Step 1 ----\u001b[0m\n",
      "\u001b[33mTook 7.1s, using 2026 tokens (in: 1814, out: 212) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI need to create a query that will:\n",
      "1. Filter for recipients in Spain\n",
      "2. Look at tracking status updates for customs clearance\n",
      "3. Filter for updates after January 16, 2024\n",
      "\n",
      "Let me send this request to the DSL query agent first.\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'dsl-query-agent-us-west-2-533 [X5ISUSR3EE/QQR20YGSSC]' passing input text:\u001b[0m\n",
      "\u001b[35mCreate a query to find shipping orders where:\n",
      "1. recipient.country is \"Spain\"\n",
      "2. tracking array contains an entry with status \"customs_clearance\"\n",
      "3. tracking.timestamp is after \"2024-01-16\"\n",
      "Use nested query for the tracking conditions.\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.1 [using sub-agent name:dsl-query-agent-us-west-2-533, id:X5ISUSR3EE/QQR20YGSSC] ----\u001b[0m\n",
      "\u001b[33mTook 11.8s, using 2146 tokens (in: 1529, out: 617) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mTo create this query, we need to combine multiple conditions using a bool query. We'll use a term query for the recipient's country, and a nested query for the tracking conditions. Let's break it down step by step:\n",
      "\n",
      "1. Match recipient.country to \"Spain\" using a term query\n",
      "2. Use a nested query for the tracking array\n",
      "   a. Match the status \"customs_clearance\"\n",
      "   b. Use a range query for the timestamp to be after \"2024-01-16\"\n",
      "\n",
      "Now, let's construct the Query DSL based on these requirements.\n",
      "</thinking>\n",
      "\n",
      "<json>\n",
      "{\n",
      "  \"query\": {\n",
      "    \"bool\": {\n",
      "      \"must\": [\n",
      "        {\n",
      "          \"term\": {\n",
      "            \"recipient.country\": \"Spain\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"nested\": {\n",
      "            \"path\": \"tracking\",\n",
      "            \"query\": {\n",
      "              \"bool\": {\n",
      "                \"must\": [\n",
      "                  {\n",
      "                    \"term\": {\n",
      "                      \"tracking.status\": \"customs_clearance\"\n",
      "                    }\n",
      "                  },\n",
      "                  {\n",
      "                    \"range\": {\n",
      "                      \"tracking.timestamp\": {\n",
      "                        \"gt\": \"2024-01-16\"\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "</json>\n",
      "\n",
      "<thinking>\n",
      "Now that we have created the Query DSL, let's execute it using the provided function to get the results.\u001b[0m\n",
      "\u001b[35mUsing tool: execute_dsl_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'dsl_query', 'type': 'string', 'value': '{\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"term\": {\\n            \"recipient.country\": \"Spain\"\\n          }\\n        },\\n        {\\n          \"nested\": {\\n            \"path\": \"tracking\",\\n            \"query\": {\\n              \"bool\": {\\n                \"must\": [\\n                  {\\n                    \"term\": {\\n                      \"tracking.status\": \"customs_clearance\"\\n                    }\\n                  },\\n                  {\\n                    \"range\": {\\n                      \"tracking.timestamp\": {\\n                        \"gt\": \"2024-01-16\"\\n                      }\\n                    }\\n                  }\\n                ]\\n              }\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  }\\n}'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': {'took': 33, 'timed_out': False, '_shards': {'total': 0, 'successful': 0, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 1, 'relation': 'eq'}, 'max_score': 2.2685113, 'hits': [{'_index': 'ecom_shipping_index', '_id': 'hJHlsJQBCfSRmBN2WEfZ', '_score': 2.2685113, '_source': ...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.2 [using sub-agent name:dsl-query-agent-us-west-2-533, id:X5ISUSR3EE/QQR20YGSSC] ----\u001b[0m\n",
      "\u001b[33mTook 7.7s, using 2919 tokens (in: 2669, out: 250) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mThe query has been executed successfully, and we have received the results. Let's analyze the response and provide a concise answer to the user.\n",
      "\n",
      "The query returned one matching result, which meets all the specified criteria:\n",
      "1. The recipient's country is Spain.\n",
      "2. The tracking array contains an entry with the status \"customs_clearance\".\n",
      "3. There is a tracking entry with a timestamp after \"2024-01-16\" (specifically, on 2024-01-17).\n",
      "\n",
      "Let's summarize this information for the user.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "The query successfully found one shipping order that matches your criteria. This order:\n",
      "- Is being shipped to Spain\n",
      "- Has a tracking entry with \"customs_clearance\" status\n",
      "- Has a tracking update after January 16, 2024 (specifically on January 17, 2024)\n",
      "\n",
      "The order details include:\n",
      "- Order ID: ORD7891...\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent dsl-query-agent-us-west-2-533 output text:\n",
      "The query successfully found one shipping order that matches your criteria. This order:\n",
      "- Is being shipped to Spain\n",
      "- Has a tracking entry with \"customs_clearance\" status\n",
      "- Has a tracking update after January 16, 2024 (specifically on January 17, 2024)\n",
      "\n",
      "The order details include:\n",
      "- Order ID: ORD7891...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 2 ----\u001b[0m\n",
      "\u001b[33mTook 3.6s, using 2289 tokens (in: 2151, out: 138) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mThe DSL query agent has successfully executed the query and found one matching order. I'll communicate these results to the user.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "There is 1 order that matches your criteria. This order was in customs clearance after January 16, 2024, with a recipient in Spain....\u001b[0m\n",
      "\u001b[33mAgent made a total of 4 LLM calls, using 9380 tokens (in: 8163, out: 1217), and took 30.2 total seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = supervisor_agent.invoke(\n",
    "    input_text=\"How many orders have recipients in Spain and were last updated during customs clearance after January 16, 2024?\",\n",
    "    session_id=\"1245\",\n",
    "    enable_trace=True,\n",
    "    trace_level=\"core\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invokeAgent API request ID: 9f58c37b-8f99-4f46-a4fb-7fe5633630e2\n",
      "invokeAgent API session ID: 1245\n",
      "  agent id: 0JATH64VCJ, agent alias id: ZU6C3G8PYS\n",
      "\u001b[32m---- Step 1 ----\u001b[0m\n",
      "\u001b[33mTook 8.9s, using 2405 tokens (in: 2137, out: 268) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI need to analyze this query and break it down:\n",
      "1. Looking for temperature controlled packages\n",
      "2. Delivery time constraint of 2 hours\n",
      "3. Looking at the schema, there's no direct temperature control field\n",
      "4. Need to check if we can derive this information from existing fields\n",
      "\n",
      "Let me:\n",
      "1. First ask the DSL query agent to help construct a query that can work with available fields\n",
      "2. Then have the query fixer agent help optimize it if needed\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'dsl-query-agent-us-west-2-533 [QAAT9MMOLL/HZUJPGFETV]' passing input text:\u001b[0m\n",
      "\u001b[35mCreate a query to find packages that might be temperature controlled and delivered within 2 hours. Consider:\n",
      "1. Package description field might contain temperature-related keywords\n",
      "2. Use tracking data to determine delivery duration\n",
      "3. Look for relevant carrier information\n",
      "Note: We need to infer tem\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.1 [using sub-agent name:dsl-query-agent-us-west-2-533, id:QAAT9MMOLL/HZUJPGFETV] ----\u001b[0m\n",
      "\u001b[33mTook 16.6s, using 2518 tokens (in: 1541, out: 977) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mTo create a query that finds packages that might be temperature controlled and delivered within 2 hours, we need to consider multiple aspects of the shipping schema. Let's break down the requirements and construct the query step by step:\n",
      "\n",
      "1. For temperature-controlled packages, we'll search the package description field for relevant keywords.\n",
      "2. To determine delivery duration, we'll use the nested tracking data.\n",
      "3. We'll include carrier information in our query.\n",
      "\n",
      "Let's construct the Query DSL to address these requirements:\n",
      "</thinking>\n",
      "\n",
      "<json>\n",
      "{\n",
      "  \"query\": {\n",
      "    \"bool\": {\n",
      "      \"must\": [\n",
      "        {\n",
      "          \"match\": {\n",
      "            \"package.description\": {\n",
      "              \"query\": \"temperature controlled refrigerated frozen cold chain\",\n",
      "              \"operator\": \"or\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"nested\": {\n",
      "            \"path\": \"tracking\",\n",
      "            \"query\": {\n",
      "              \"bool\": {\n",
      "                \"must\": [\n",
      "                  {\n",
      "                    \"term\": {\n",
      "                      \"tracking.status\": \"delivered\"\n",
      "                    }\n",
      "                  },\n",
      "                  {\n",
      "                    \"range\": {\n",
      "                      \"tracking.timestamp\": {\n",
      "                        \"lte\": \"now\"\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"nested\": {\n",
      "            \"path\": \"tracking\",\n",
      "            \"query\": {\n",
      "              \"bool\": {\n",
      "                \"must\": [\n",
      "                  {\n",
      "                    \"term\": {\n",
      "                      \"tracking.status\": \"in_transit\"\n",
      "                    }\n",
      "                  },\n",
      "                  {\n",
      "                    \"range\": {\n",
      "                      \"tracking.timestamp\": {\n",
      "                        \"gte\": \"now-2h\"\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"aggs\": {\n",
      "    \"carriers\": {\n",
      "      \"terms\": {\n",
      "        \"field\": \"delivery.carrier\",\n",
      "        \"size\": 10\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "</json>\n",
      "\n",
      "<thinking>\n",
      "Now that we have constructed the Query DSL, let's execute it using the provided function to get the results.\u001b[0m\n",
      "\u001b[35mUsing tool: execute_dsl_query with these inputs:\u001b[0m\n",
      "\u001b[35m[{'name': 'dsl_query', 'type': 'string', 'value': '{\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"match\": {\\n            \"package.description\": {\\n              \"query\": \"temperature controlled refrigerated frozen cold chain\",\\n              \"operator\": \"or\"\\n            }\\n          }\\n        },\\n        {\\n          \"nested\": {\\n            \"path\": \"tracking\",\\n            \"query\": {\\n              \"bool\": {\\n                \"must\": [\\n                  {\\n                    \"term\": {\\n                      \"tracking.status\": \"delivered\"\\n                    }\\n                  },\\n                  {\\n                    \"range\": {\\n                      \"tracking.timestamp\": {\\n                        \"lte\": \"now\"\\n                      }\\n                    }\\n                  }\\n                ]\\n              }\\n            }\\n          }\\n        },\\n        {\\n          \"nested\": {\\n            \"path\": \"tracking\",\\n            \"query\": {\\n              \"bool\": {\\n                \"must\": [\\n                  {\\n                    \"term\": {\\n                      \"tracking.status\": \"in_transit\"\\n                    }\\n                  },\\n                  {\\n                    \"range\": {\\n                      \"tracking.timestamp\": {\\n                        \"gte\": \"now-2h\"\\n                      }\\n                    }\\n                  }\\n                ]\\n              }\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  },\\n  \"aggs\": {\\n    \"carriers\": {\\n      \"terms\": {\\n        \"field\": \"delivery.carrier\",\\n        \"size\": 10\\n      }\\n    }\\n  }\\n}'}]\n",
      "\u001b[0m\n",
      "\u001b[35m--tool outputs:\n",
      "{'TEXT': {'body': {'took': 28, 'timed_out': False, '_shards': {'total': 0, 'successful': 0, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}, 'aggregations': {'carriers': {'doc_count_error_upper_bound': 0, 'sum_other_doc_count': 0, 'buckets...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 1.2 [using sub-agent name:dsl-query-agent-us-west-2-533, id:QAAT9MMOLL/HZUJPGFETV] ----\u001b[0m\n",
      "\u001b[33mTook 12.2s, using 2995 tokens (in: 2655, out: 340) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mBased on the query results, it appears that there are no packages matching our criteria. Let's analyze the results:\n",
      "\n",
      "1. The query returned 0 hits, which means no packages were found that match all our conditions.\n",
      "2. The aggregation for carriers is empty, indicating that no carriers were found for the matching packages (since there were no matches).\n",
      "\n",
      "This could be due to several reasons:\n",
      "1. There might not be any packages in the system that are potentially temperature-controlled (based on the description keywords we used).\n",
      "2. There might not be any packages that were delivered within the last 2 hours.\n",
      "3. The combination of our criteria (temperature-controlled description + recent delivery) might be too restrictive.\n",
      "\n",
      "Let's provide a summary of these findings to the user.\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "Based on the query results, we couldn't find any packages that match all the specified criteria. This means:\n",
      "\n",
      "1. No packages were found with descriptions suggesting temperature control (using keywords like \"temperature controlled\", \"refrigerated\", \"frozen\", or \"cold chain\").\n",
      "2. No packages were deli...\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent dsl-query-agent-us-west-2-533 output text:\n",
      "Based on the query results, we couldn't find any packages that match all the specified criteria. This means:\n",
      "\n",
      "1. No packages were found with descriptions suggesting temperature control (using keywords like \"temperature controlled\", \"refrigerated\", \"frozen\", or \"cold chain\").\n",
      "2. No packages were deli...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 2 ----\u001b[0m\n",
      "\u001b[33mTook 5.6s, using 2772 tokens (in: 2569, out: 203) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mBased on the results, I should:\n",
      "1. Contact the query fixer agent to help optimize the query and suggest alternative approaches\n",
      "2. Then provide a comprehensive response to the user about the limitations and possible alternatives\u001b[0m\n",
      "\u001b[35mUsing sub-agent collaborator: 'query-fixer-agent-us-west-2-533 [B4QGLWZPHF/DDFVRPIMJC]' passing input text:\u001b[0m\n",
      "\u001b[35mNeed help optimizing a query for temperature-controlled packages delivered within 2 hours. Current limitations:\n",
      "1. No direct temperature control field in schema\n",
      "2. Zero results from initial query\n",
      "Please suggest:\n",
      "1. Alternative field combinations\n",
      "2. Query relaxation strategies\n",
      "3. Possible schema enha\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 2.1 [using sub-agent name:query-fixer-agent-us-west-2-533, id:B4QGLWZPHF/DDFVRPIMJC] ----\u001b[0m\n",
      "\u001b[33mTook 9.8s, using 2102 tokens (in: 1725, out: 377) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mI understand that you need help optimizing a query for temperature-controlled packages delivered within 2 hours. Let's analyze the current limitations and provide suggestions based on the available schema and optimization strategies.\n",
      "\n",
      "1. No direct temperature control field in schema:\n",
      "   This is a significant limitation as we don't have a specific field to filter temperature-controlled packages. We'll need to find alternative ways to infer this information.\n",
      "\n",
      "2. Zero results from initial query:\n",
      "   This suggests that our query might be too restrictive. We'll need to apply query relaxation techniques to broaden the search.\n",
      "\n",
      "Let's address your requests one by one:\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "1. Alternative field combinations:\n",
      "   - We can use the \"package.description\" field to search for keywords related to temperature control, such as \"cold\", \"frozen\", \"refrigerated\", or \"temperature-controlled\".\n",
      "   - For delivery time, we can use the \"tracking\" nested field to calculate the time differ...\u001b[0m\n",
      "\u001b[35m\n",
      "----sub-agent query-fixer-agent-us-west-2-533 output text:\n",
      "1. Alternative field combinations:\n",
      "   - We can use the \"package.description\" field to search for keywords related to temperature control, such as \"cold\", \"frozen\", \"refrigerated\", or \"temperature-controlled\".\n",
      "   - For delivery time, we can use the \"tracking\" nested field to calculate the time differ...\n",
      "\u001b[0m\n",
      "\u001b[32m---- Step 3 ----\u001b[0m\n",
      "\u001b[33mTook 6.2s, using 3237 tokens (in: 3009, out: 228) to complete prior action, observe, orchestrate.\u001b[0m\n",
      "\u001b[34mNow I should provide a clear response to the user explaining the situation and limitations\u001b[0m\n",
      "\u001b[36mFinal response:\n",
      "I was unable to find any temperature-controlled packages delivered within 2 hours. This is because:\n",
      "\n",
      "1. The system doesn't have a direct way to identify temperature-controlled packages\n",
      "2. We attempted to search using package descriptions and carrier information\n",
      "3. No packages matching these criteria...\u001b[0m\n",
      "\u001b[33mAgent made a total of 6 LLM calls, using 16029 tokens (in: 13636, out: 2393), and took 59.8 total seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = supervisor_agent.invoke(\n",
    "    input_text=\"What are the temperature controlled packages delivered within 2 hours?\",\n",
    "    session_id=\"1245\",\n",
    "    enable_trace=True,\n",
    "    trace_level=\"core\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 09:58:00,880 - __main__ - INFO - Deleting Supervisor Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: supervisor-agent-us-west-2-533, id: BZW5ZYSZB8\n",
      "Deleting aliases for agent BZW5ZYSZB8...\n",
      "Deleting alias 6KZEM29VOE from agent BZW5ZYSZB8\n",
      "Deleting alias TSTALIASID from agent BZW5ZYSZB8\n",
      "Deleting agent: BZW5ZYSZB8...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_supervisor-agent-us-west-2-533...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 09:58:12,365 - __main__ - INFO - Deleting DSL Query Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: dsl-query-agent-us-west-2-533, id: X5ISUSR3EE\n",
      "Deleting aliases for agent X5ISUSR3EE...\n",
      "Deleting alias QQR20YGSSC from agent X5ISUSR3EE\n",
      "Deleting alias TSTALIASID from agent X5ISUSR3EE\n",
      "Deleting agent: X5ISUSR3EE...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_dsl-query-agent-us-west-2-533...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 09:58:24,120 - __main__ - INFO - Deleting Query Fixer Agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: query-fixer-agent-us-west-2-533, id: ECGRBY6PO0\n",
      "Deleting aliases for agent ECGRBY6PO0...\n",
      "Deleting alias EQYZTTYOKR from agent ECGRBY6PO0\n",
      "Deleting alias TSTALIASID from agent ECGRBY6PO0\n",
      "Deleting agent: ECGRBY6PO0...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_query-fixer-agent-us-west-2-533...\n"
     ]
    }
   ],
   "source": [
    "# # -------------------------------------------------------------------------\n",
    "# # 8. Cleanup: Delete the created agents\n",
    "# # -------------------------------------------------------------------------\n",
    "# logger.info(\"Deleting Supervisor Agent...\")\n",
    "# agents_helper.delete_agent(supervisor_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting DSL Query Agent...\")\n",
    "# agents_helper.delete_agent(dsl_query_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"Deleting Query Fixer Agent...\")\n",
    "# agents_helper.delete_agent(query_fixer_agent.name, verbose=True)\n",
    "\n",
    "# logger.info(\"All agents deleted. Script completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
