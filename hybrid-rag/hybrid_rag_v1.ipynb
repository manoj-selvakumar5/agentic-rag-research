{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.bedrock_agent import Agent, SupervisorAgent, Task, region, account_id, agents_helper\n",
    "from textwrap import dedent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import subprocess\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = session.region_name\n",
    "account_id_suffix = account_id[:3]\n",
    "agent_suffix = f\"{region}-{account_id_suffix}\"\n",
    "\n",
    "s3_client = boto3.client('s3', region)\n",
    "bedrock_client = boto3.client('bedrock-runtime', region)\n",
    "iam_client = boto3.client('iam', region)\n",
    "lambda_client = boto3.client('lambda', region)\n",
    "\n",
    "\n",
    "# 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "# 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "# 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "\n",
    "agent_foundation_model = [\n",
    "    'anthropic.claude-3-5-sonnet-20241022-v2:0'\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-west-2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-west-2-533'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl_query_agent_name =  f\"dsl-query-agent-{agent_suffix}\"\n",
    "# dsl_query_agent_role_name = f'AmazonBedrockExecutionRoleForAgents_{dsl_query_agent_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent.set_force_recreate_default(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Schemas formatted for OSS Index\n",
    "with open('schemas/ecom_shipping_schema.json', 'r') as file:\n",
    "    ecom_shipping_schema = json.load(file)\n",
    "\n",
    "ecom_shipping_schema_string= json.dumps(ecom_shipping_schema, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"shipping\": {\\n    \"properties\": {\\n      \"order_id\": {\\n        \"type\": \"keyword\"\\n      },\\n      \"tracking_number\": {\\n        \"type\": \"keyword\"\\n      },\\n      \"status\": {\\n        \"type\": \"keyword\"\\n      },\\n      \"created_at\": {\\n        \"type\": \"date\"\\n      },\\n      \"sender\": {\\n        \"properties\": {\\n          \"name\": {\\n            \"type\": \"text\"\\n          },\\n          \"email\": {\\n            \"type\": \"keyword\"\\n          },\\n          \"address\": {\\n            \"type\": \"text\"\\n          },\\n          \"city\": {\\n            \"type\": \"keyword\"\\n          },\\n          \"country\": {\\n            \"type\": \"keyword\"\\n          }\\n        }\\n      },\\n      \"recipient\": {\\n        \"properties\": {\\n          \"name\": {\\n            \"type\": \"text\"\\n          },\\n          \"email\": {\\n            \"type\": \"keyword\"\\n          },\\n          \"address\": {\\n            \"type\": \"text\"\\n          },\\n          \"city\": {\\n            \"type\": \"keyword\"\\n          },\\n          \"country\": {\\n            \"type\": \"keyword\"\\n          }\\n        }\\n      },\\n      \"package\": {\\n        \"properties\": {\\n          \"weight\": {\\n            \"type\": \"float\"\\n          },\\n          \"description\": {\\n            \"type\": \"text\"\\n          },\\n          \"value\": {\\n            \"type\": \"float\"\\n          }\\n        }\\n      },\\n      \"delivery\": {\\n        \"properties\": {\\n          \"carrier\": {\\n            \"type\": \"keyword\"\\n          },\\n          \"estimated_date\": {\\n            \"type\": \"date\"\\n          },\\n          \"shipping_cost\": {\\n            \"type\": \"float\"\\n          }\\n        }\\n      },\\n      \"tracking\": {\\n        \"type\": \"nested\",\\n        \"properties\": {\\n          \"timestamp\": {\\n            \"type\": \"date\"\\n          },\\n          \"status\": {\\n            \"type\": \"keyword\"\\n          },\\n          \"location\": {\\n            \"type\": \"keyword\"\\n          }\\n        }\\n      }\\n    }\\n  }\\n}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecom_shipping_schema_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting existing agent and corresponding lambda for: dsl-query-agent-us-west-2-533...\n",
      "Found target agent, name: dsl-query-agent-us-west-2-533, id: WJYFKTH1IX\n",
      "Deleting aliases for agent WJYFKTH1IX...\n",
      "Deleting alias 55BQIJMVGI from agent WJYFKTH1IX\n",
      "Deleting alias TSTALIASID from agent WJYFKTH1IX\n",
      "Deleting agent: WJYFKTH1IX...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_dsl-query-agent-us-west-2-533...\n",
      "Creating agent dsl-query-agent-us-west-2-533...\n",
      "Created agent, id: AK3UI7V9RL, alias id: TSTALIASID\n",
      "\n",
      "Adding action group with Lambda: arn:aws:lambda:us-west-2:533267284022:function:execute-dsl-query-us-west-2-533...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id AK3UI7V9RL current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id AK3UI7V9RL current status: PREPARED\n",
      "DONE: Agent: dsl-query-agent-us-west-2-533, id: AK3UI7V9RL, alias id: BDZDCS2YJE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dsl_query_agent = Agent.direct_create(\n",
    "    name=f\"dsl-query-agent-{agent_suffix}\",\n",
    "    role=\"DSL Query Creator\",\n",
    "    goal=\"Create DSL queries for a given user query\",\n",
    "    instructions=f\"\"\"\n",
    "    You are an expert in generating Query DSL for Elasticsearch-style queries. Your task is to convert a given natural language user question into a well-structured Query DSL.\n",
    "    \n",
    "    ## Instructions:\n",
    "    - Use the provided e-commerce shipping schema to construct the query.\n",
    "    - Encapsulate the output in <json>...</json> tags.\n",
    "    - Follow the syntax of the Query DSL strictly; do not introduce elements outside the provided schema.\n",
    "    \n",
    "    ## Query Construction Rules:\n",
    "    - **Keyword fields** (e.g., carrier, status, country): Use `term` for exact matches or `prefix`/`wildcard` for partial matches.\n",
    "    - **Text fields** (e.g., description, address): Use `match` queries to account for analyzed terms.\n",
    "    - **Nested fields** (e.g., tracking): Always use `nested` queries.\n",
    "    - **Date fields**: Use `range` queries with date math for filtering by date ranges.\n",
    "    - Break down complex queries into smaller parts for accuracy.\n",
    "    - Think step-by-step before constructing the query.\n",
    "\n",
    "    ## Schema:\n",
    "    {ecom_shipping_schema_string}\n",
    "\n",
    "    ## Output Format:\n",
    "    - Return only the generated Query DSL within <json>...</json> tags.\n",
    "    - Do not include explanations, comments, or additional text.\n",
    "    \"\"\",\n",
    "    tool_code=f\"arn:aws:lambda:{region}:{account_id}:function:execute-dsl-query-us-west-2-533\",\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"execute_dsl_query\",\n",
    "            \"description\": \"Executes a given DSL query and returns the results\",\n",
    "            \"parameters\": {\n",
    "                \"dsl_query\": {\n",
    "                    \"description\": \"The DSL query to execute\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent ID for DSL Query Agent: AK3UI7V9RL\n",
      "Agent ID for Query Fixer Agent: 3T9YA9W3WR\n"
     ]
    }
   ],
   "source": [
    "dsl_query_agent_id = agents_helper.get_agent_id_by_name(f\"dsl-query-agent-{agent_suffix}\")\n",
    "print(f\"Agent ID for DSL Query Agent: {dsl_query_agent_id}\")\n",
    "\n",
    "query_fixer_agent_id = agents_helper.get_agent_id_by_name(f\"query-fixer-agent-{agent_suffix}\")\n",
    "print(f\"Agent ID for Query Fixer Agent: {query_fixer_agent_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function and role names\n",
    "LAMBDA_FUNCTION_NAME = f\"execute-dsl-query-{agent_suffix}\"\n",
    "IAM_ROLE_NAME = f\"LambdaExecutionRole-{agent_suffix}\"\n",
    "ZIP_FILE_NAME = \"function.zip\"\n",
    "LAMBDA_FILE_PATH = \"src/lambda/execute_dsl_query.py\"\n",
    "DEPENDENCIES = [\"opensearch-py\", \"requests\", \"urllib3\"]  # Required packages\n",
    "\n",
    "def create_iam_role(role_name):\n",
    "    \"\"\"Creates an IAM Role with necessary trust policy for Lambda.\"\"\"\n",
    "    \n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        role = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    "        )\n",
    "        print(f\"IAM Role {role_name} created.\")\n",
    "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "        print(f\"IAM Role {role_name} already exists.\")\n",
    "        role = iam_client.get_role(RoleName=role_name)\n",
    "\n",
    "    # Attach AWS Lambda execution policy\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Attach additional policies for OpenSearch access\n",
    "    opensearch_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"es:Describe*\",\n",
    "                    \"es:List*\",\n",
    "                    \"es:Get*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    opensearch_policy_name = f\"{role_name}-OpenSearchPolicy\"\n",
    "    try:\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=opensearch_policy_name,\n",
    "            PolicyDocument=json.dumps(opensearch_policy_document)\n",
    "        )\n",
    "        print(f\"Attached OpenSearch policy to IAM Role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to attach OpenSearch policy to IAM Role {role_name}: {e}\")\n",
    "\n",
    "\n",
    "    # Attach the new policy for aoss:APICall\n",
    "    aoss_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"aoss:*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    aoss_policy_name = f\"{role_name}-AOSSPolicy\"\n",
    "    try:\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=aoss_policy_name,\n",
    "            PolicyDocument=json.dumps(aoss_policy_document)\n",
    "        )\n",
    "        print(f\"Attached AOSS policy to IAM Role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to attach AOSS policy to IAM Role {role_name}: {e}\")\n",
    "\n",
    "\n",
    "    role_arn = role['Role']['Arn']\n",
    "\n",
    "    # Wait for IAM role to propagate\n",
    "    print(\"Waiting for IAM role to be usable...\")\n",
    "    time.sleep(10)  # Delay to allow propagation\n",
    "\n",
    "    return role_arn\n",
    "\n",
    "\n",
    "def create_zip_file(source_file, zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
    "        zipf.write(source_file, os.path.basename(source_file))\n",
    "\n",
    "def create_lambda_package(source_file, zip_file_path):\n",
    "    \"\"\"Packages the Lambda function and its dependencies into a ZIP file.\"\"\"\n",
    "    package_dir = \"package\"\n",
    "    \n",
    "    # Install dependencies\n",
    "    if not os.path.exists(package_dir):\n",
    "        os.makedirs(package_dir)\n",
    "    print(\"Installing dependencies...\")\n",
    "    subprocess.run(f\"pip install {' '.join(DEPENDENCIES)} -t {package_dir}\", shell=True, check=True)\n",
    "\n",
    "    # Create ZIP file with dependencies and function\n",
    "    print(\"Creating Lambda deployment package...\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
    "        for root, _, files in os.walk(package_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zipf.write(file_path, os.path.relpath(file_path, package_dir))\n",
    "\n",
    "        zipf.write(source_file, os.path.basename(source_file))\n",
    "\n",
    "    # Cleanup temporary package directory\n",
    "    subprocess.run(f\"rm -rf {package_dir}\", shell=True)\n",
    "    print(\"Lambda package created successfully.\")\n",
    "\n",
    "\n",
    "def create_lambda_function(function_name, role_arn, handler, runtime, zip_file_path, region_name=region):\n",
    "    # Initialize a session using Amazon Lambda\n",
    "    lambda_client = boto3.client('lambda', region_name=region_name)\n",
    "\n",
    "    # Read the zip file content\n",
    "    with open(zip_file_path, 'rb') as f:\n",
    "        zip_content = f.read()\n",
    "\n",
    "    # Create the Lambda function\n",
    "    try:\n",
    "        response = lambda_client.create_function(\n",
    "            FunctionName=function_name,\n",
    "            Runtime=runtime,\n",
    "            Role=role_arn,\n",
    "            Handler=handler,\n",
    "            Code={'ZipFile': zip_content},\n",
    "            Description='Lambda function to execute DSL queries',\n",
    "            Timeout=15,\n",
    "            MemorySize=128,\n",
    "            Publish=True\n",
    "        )\n",
    "        print(f\"Lambda function {function_name} created successfully.\")\n",
    "    except lambda_client.exceptions.ResourceConflictException:\n",
    "        print(f\"Lambda function {function_name} already exists. Updating the function...\")\n",
    "\n",
    "        response = lambda_client.update_function_code(\n",
    "            FunctionName=function_name,\n",
    "            ZipFile=zip_content\n",
    "        )\n",
    "        print(f\"Lambda function {function_name} updated successfully.\")\n",
    "\n",
    "    return response\n",
    "\n",
    "def add_resource_based_policy(function_name, agent_ids, region, account_id):\n",
    "    statement_id = \"AllowExecutionFromBedrockAgent\"\n",
    "    policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": []\n",
    "    }\n",
    "\n",
    "    for agent_id in agent_ids:\n",
    "        policy['Statement'].append({\n",
    "            \"Sid\": f\"{statement_id}_{agent_id}\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"lambda:InvokeFunction\",\n",
    "            \"Resource\": f\"arn:aws:lambda:{region}:{account_id}:function:{function_name}\",\n",
    "            \"Condition\": {\n",
    "                \"ArnLike\": {\n",
    "                    \"AWS:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:agent/{agent_id}\"\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Check if the statement ID already exists and remove it if necessary\n",
    "    try:\n",
    "        existing_policy = lambda_client.get_policy(FunctionName=function_name)\n",
    "        policy_document = json.loads(existing_policy['Policy'])\n",
    "        for statement in policy_document['Statement']:\n",
    "            if statement['Sid'].startswith(statement_id):\n",
    "                lambda_client.remove_permission(\n",
    "                    FunctionName=function_name,\n",
    "                    StatementId=statement['Sid']\n",
    "                )\n",
    "                print(f\"Removed existing statement ID {statement['Sid']} from Lambda function {function_name}.\")\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        print(f\"No existing policy found for Lambda function {function_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking existing policy for Lambda function {function_name}: {e}\")\n",
    "\n",
    "    # Add the new permissions\n",
    "    try:\n",
    "        for statement in policy['Statement']:\n",
    "            response = lambda_client.add_permission(\n",
    "                FunctionName=function_name,\n",
    "                StatementId=statement['Sid'],\n",
    "                Action=statement['Action'],\n",
    "                Principal=statement['Principal']['Service'],\n",
    "                SourceArn=statement['Condition']['ArnLike']['AWS:SourceArn']\n",
    "            )\n",
    "        print(f\"Resource-based policies added to Lambda function {function_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to add resource-based policies to Lambda function {function_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM Role LambdaExecutionRole-us-west-2-533 already exists.\n",
      "Attached OpenSearch policy to IAM Role LambdaExecutionRole-us-west-2-533.\n",
      "Attached AOSS policy to IAM Role LambdaExecutionRole-us-west-2-533.\n",
      "Waiting for IAM role to be usable...\n",
      "Installing dependencies...\n",
      "Collecting opensearch-py\n",
      "  Using cached opensearch_py-2.8.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-dateutil (from opensearch-py)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2024.07.04 (from opensearch-py)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Events (from opensearch-py)\n",
      "  Using cached Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting six>=1.5 (from python-dateutil->opensearch-py)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached opensearch_py-2.8.0-py3-none-any.whl (353 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: Events, urllib3, six, idna, charset-normalizer, certifi, requests, python-dateutil, opensearch-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.16.1 requires botocore<1.35.89,>=1.35.74, but you have botocore 1.36.9 which is incompatible.\n",
      "datasets 2.21.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "awscli 1.34.8 requires botocore==1.35.8, but you have botocore 1.36.9 which is incompatible.\n",
      "awscli 1.34.8 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Events-0.5 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 opensearch-py-2.8.0 python-dateutil-2.9.0.post0 requests-2.32.3 six-1.17.0 urllib3-2.3.0\n",
      "Creating Lambda deployment package...\n",
      "Lambda package created successfully.\n",
      "Lambda function execute-dsl-query-us-west-2-533 already exists. Updating the function...\n",
      "Lambda function execute-dsl-query-us-west-2-533 updated successfully.\n",
      "Removed existing statement ID AllowExecutionFromBedrockAgent from Lambda function execute-dsl-query-us-west-2-533.\n",
      "Resource-based policies added to Lambda function execute-dsl-query-us-west-2-533.\n",
      "{'ResponseMetadata': {'RequestId': 'd58f1423-0de0-4202-9a2e-2bdf231c6ade', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 02 Feb 2025 02:54:30 GMT', 'content-type': 'application/json', 'content-length': '1449', 'connection': 'keep-alive', 'x-amzn-requestid': 'd58f1423-0de0-4202-9a2e-2bdf231c6ade'}, 'RetryAttempts': 0}, 'FunctionName': 'execute-dsl-query-us-west-2-533', 'FunctionArn': 'arn:aws:lambda:us-west-2:533267284022:function:execute-dsl-query-us-west-2-533', 'Runtime': 'python3.12', 'Role': 'arn:aws:iam::533267284022:role/LambdaExecutionRole-us-west-2-533', 'Handler': 'execute_dsl_query.lambda_handler', 'CodeSize': 8105773, 'Description': 'Lambda function to execute DSL queries', 'Timeout': 15, 'MemorySize': 128, 'LastModified': '2025-02-02T02:54:29.000+0000', 'CodeSha256': 'JixmmkDHR0Zj9pqs8EY5cpqRy2jl9dqWeU1y9KJpktI=', 'Version': '$LATEST', 'TracingConfig': {'Mode': 'PassThrough'}, 'RevisionId': 'd4dd1a92-77a2-4ec8-a440-dbf6b91c1fa7', 'State': 'Active', 'LastUpdateStatus': 'InProgress', 'LastUpdateStatusReason': 'The function is being created.', 'LastUpdateStatusReasonCode': 'Creating', 'PackageType': 'Zip', 'Architectures': ['x86_64'], 'EphemeralStorage': {'Size': 512}, 'SnapStart': {'ApplyOn': 'None', 'OptimizationStatus': 'Off'}, 'RuntimeVersionConfig': {'RuntimeVersionArn': 'arn:aws:lambda:us-west-2::runtime:7515e00d6763496e7a147ffa395ef5b0f0c1ffd6064130abb5ecde5a6d630e86'}, 'LoggingConfig': {'LogFormat': 'Text', 'LogGroup': '/aws/lambda/execute-dsl-query-us-west-2-533'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Create IAM Role for Lambda\n",
    "role_arn = create_iam_role(IAM_ROLE_NAME)\n",
    "\n",
    "# Step 2: Zip the existing lambda function file\n",
    "if not os.path.exists(LAMBDA_FILE_PATH):\n",
    "    print(f\"Error: {LAMBDA_FILE_PATH} does not exist. Create the file first.\")\n",
    "    exit(1)\n",
    "\n",
    "# Step 3: Package Lambda function and dependencies\n",
    "create_lambda_package(LAMBDA_FILE_PATH, ZIP_FILE_NAME)\n",
    "\n",
    "# Step 4: Create or update the Lambda function\n",
    "response = create_lambda_function(\n",
    "    function_name=LAMBDA_FUNCTION_NAME,\n",
    "    role_arn=role_arn,\n",
    "    handler=\"execute_dsl_query.lambda_handler\",\n",
    "    runtime=\"python3.12\",\n",
    "    zip_file_path=ZIP_FILE_NAME\n",
    ")\n",
    "\n",
    "# Step 4: Add resource-based policy to the Lambda function\n",
    "add_resource_based_policy(LAMBDA_FUNCTION_NAME, [dsl_query_agent_id,query_fixer_agent_id], region, account_id)\n",
    "\n",
    "# add_resource_based_policy(LAMBDA_FUNCTION_NAME, query_fixer_agent_id, region, account_id)\n",
    "\n",
    "\n",
    "# Cleanup: Remove temporary ZIP file\n",
    "os.remove(ZIP_FILE_NAME)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: supervisor-agent-us-west-2-533, id: DOHAROQZQX\n",
      "Deleting aliases for agent DOHAROQZQX...\n",
      "Deleting alias TSTALIASID from agent DOHAROQZQX\n",
      "Deleting agent: DOHAROQZQX...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_supervisor-agent-us-west-2-533...\n",
      "\n",
      "Created supervisor, id: K20DRR08HG, alias id: TSTALIASID\n",
      "\n",
      "  associating sub-agents / collaborators to supervisor...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id K20DRR08HG current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id K20DRR08HG current status: PREPARED\n",
      "DONE: Agent: supervisor-agent-us-west-2-533, id: K20DRR08HG, alias id: HKHEMURSE1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# supervisor_agent = SupervisorAgent.direct_create(\n",
    "#     name=f\"supervisor-agent-{agent_suffix}\",\n",
    "#     role=\"Supervisor who reviews and approves the generated DSL queries\",\n",
    "#     collaboration_type=\"SUPERVISOR\",  # or \"ORCHESTRATION\"\n",
    "#     collaborator_objects=[dsl_query_agent],\n",
    "#     collaborator_agents=[\n",
    "#         {\n",
    "#             \"agent\": dsl_query_agent.name,\n",
    "#             \"instructions\": \"Use for generating DSL queries\",\n",
    "#             \"relay_conversation_history\": \"DISABLED\"\n",
    "#         }\n",
    "#     ],\n",
    "#     instructions=f\"\"\"\n",
    "#     You are a supervisor responsible for reviewing and approving the generated DSL queries. Your task is to ensure that the generated queries are accurate and follow the provided schema.\n",
    "    \n",
    "#     ## Instructions:\n",
    "#     - Review the generated Query DSL for accuracy and completeness.\n",
    "#     - Ensure that the generated query adheres to the provided e-commerce shipping schema.\n",
    "#     - Approve the query if it meets the requirements.\n",
    "#     - Provide feedback for any necessary changes.\n",
    "#     \"\"\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting existing agent and corresponding lambda for: query-fixer-agent-us-west-2-533...\n",
      "Agent query-fixer-agent-us-west-2-533 not found\n",
      "Creating agent query-fixer-agent-us-west-2-533...\n",
      "Created agent, id: 3T9YA9W3WR, alias id: TSTALIASID\n",
      "\n",
      "Adding action group with Lambda: arn:aws:lambda:us-west-2:533267284022:function:execute-dsl-query-us-west-2-533...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id 3T9YA9W3WR current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status VERSIONING\n",
      "Agent id 3T9YA9W3WR current status: PREPARED\n",
      "DONE: Agent: query-fixer-agent-us-west-2-533, id: 3T9YA9W3WR, alias id: JZUFH46JTG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the collaborator agent\n",
    "query_fixer_agent = Agent.direct_create(\n",
    "    name=f\"query-fixer-agent-{agent_suffix}\",\n",
    "    role=\"Query Repair Specialist\",\n",
    "    goal=\"Fix and optimize failed DSL queries\",\n",
    "    instructions=f\"\"\"\n",
    "    You are an expert query debugger and optimizer. Your tasks are:\n",
    "    1. Analyze failed DSL queries from the query generator\n",
    "    2. Diagnose errors using OpenSearch error messages\n",
    "    3. Apply targeted fixes while maintaining original intent\n",
    "    4. Optimize queries for better recall when results are empty\n",
    "    \n",
    "    ## Repair Strategies:\n",
    "    - SYNTAX ERRORS: Fix formatting issues in nested queries/aggregations\n",
    "    - FIELD ERRORS: Map invalid fields to valid schema equivalents\n",
    "    - ZERO HITS: Apply query relaxation techniques:\n",
    "      * Add wildcards to keyword searches\n",
    "      * Expand date ranges\n",
    "      * Reduce strictness of term matches\n",
    "      * Add synonym expansion\n",
    "    \n",
    "    ## Optimization Rules:\n",
    "    - Maintain original query structure where possible\n",
    "    - Prefer query-time fixes over rearchitecting\n",
    "    - Document all modifications in revision notes\n",
    "    - Limit query relaxation to 3 iterations\n",
    "    \n",
    "    ## Schema:\n",
    "    {ecom_shipping_schema_string}\n",
    "    \n",
    "    ## Output Format:\n",
    "    - Return modified query in <json> tags\n",
    "    - Include revision notes in <notes> tags\n",
    "    \"\"\",\n",
    "    tool_code=f\"arn:aws:lambda:{region}:{account_id}:function:execute-dsl-query-us-west-2-533\",\n",
    "    tool_defs=[\n",
    "        {\n",
    "            \"name\": \"retry_query\",\n",
    "            \"description\": \"Retries a modified version of the failed query\",\n",
    "            \"parameters\": {\n",
    "                \"modified_dsl_query\": {\n",
    "                    \"description\": \"The corrected DSL query\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"required\": True\n",
    "                },\n",
    "                \"revision_notes\": {\n",
    "                    \"description\": \"Description of modifications made\",\n",
    "                    \"type\": \"string\",\n",
    "                },\n",
    "            }   \n",
    "        }   \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target agent, name: supervisor-agent-us-west-2-533, id: KUFAC13FPK\n",
      "Deleting aliases for agent KUFAC13FPK...\n",
      "Deleting alias TSTALIASID from agent KUFAC13FPK\n",
      "Deleting agent: KUFAC13FPK...\n",
      "Deleting IAM role: AmazonBedrockExecutionRoleForAgents_supervisor-agent-us-west-2-533...\n",
      "\n",
      "Created supervisor, id: DNEKJQWQE2, alias id: TSTALIASID\n",
      "\n",
      "  associating sub-agents / collaborators to supervisor...\n",
      "Waiting for agent status to change. Current status CREATING\n",
      "Agent id DNEKJQWQE2 current status: NOT_PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id DNEKJQWQE2 current status: PREPARED\n",
      "Waiting for agent status to change. Current status PREPARING\n",
      "Agent id DNEKJQWQE2 current status: PREPARED\n",
      "DONE: Agent: supervisor-agent-us-west-2-533, id: DNEKJQWQE2, alias id: PDJUMLOENC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "supervisor_agent = SupervisorAgent.direct_create(\n",
    "    name=f\"supervisor-agent-{agent_suffix}\",\n",
    "    role=\"Query Pipeline Orchestrator\",\n",
    "    collaboration_type=\"SUPERVISOR\",\n",
    "    collaborator_objects=[dsl_query_agent, query_fixer_agent],\n",
    "    collaborator_agents=[\n",
    "        {\n",
    "            \"agent\": dsl_query_agent.name,\n",
    "            \"instructions\": \"Primary DSL query generation using the schema\",\n",
    "            \"relay_conversation_history\": \"DISABLED\"\n",
    "        },\n",
    "        {\n",
    "            \"agent\": query_fixer_agent.name,\n",
    "            \"instructions\": dedent(\"\"\"\n",
    "                Engage when:\n",
    "                1. DSL query returns errors (parsing/validation)\n",
    "                2. Search results are empty (zero hits)\n",
    "                3. Query needs optimization for better recall\n",
    "                \n",
    "                Responsibilities:\n",
    "                - Analyze error messages and query structure\n",
    "                - Apply targeted fixes while preserving intent\n",
    "                - Implement query relaxation strategies\n",
    "                - Document modifications made\n",
    "                \"\"\"),\n",
    "            \"relay_conversation_history\": \"TO_COLLABORATOR\"\n",
    "        }\n",
    "    ],\n",
    "    instructions=dedent(f\"\"\"\n",
    "    Orchestrate the end-to-end query generation and validation workflow:\n",
    "    \n",
    "    1. Initial Query Generation:\n",
    "    - Receive natural language query from user\n",
    "    - Route to DSL Query Agent for initial construction\n",
    "    - Validate query structure against schema:\n",
    "      {ecom_shipping_schema_string}\n",
    "    \n",
    "    2. Error Handling & Retry:\n",
    "    - Monitor for query execution errors\n",
    "    - On error/zero results:\n",
    "      a. Capture error context and original query\n",
    "      b. Route to Query Fixer Agent with full diagnostics\n",
    "      c. Validate fixer's modified query\n",
    "      d. Approve max 3 retry attempts\n",
    "    \n",
    "    3. Quality Assurance:\n",
    "    - Ensure final query meets quality standards:\n",
    "      - Proper use of nested queries\n",
    "      - Correct field types and mappings\n",
    "      - Appropriate query strictness level\n",
    "    - Maintain audit trail of all query versions\n",
    "    - Provide user with cleaned error explanations\n",
    "    \n",
    "    4. Final Approval:\n",
    "    - Sign off on valid queries\n",
    "    - Block invalid queries with detailed feedback\n",
    "    - Generate execution summary with:\n",
    "      - Query versions attempted\n",
    "      - Modification reasons\n",
    "      - Performance metrics\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
